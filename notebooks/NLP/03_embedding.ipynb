{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859e5d60-b877-4826-8c66-0282f99f9373",
   "metadata": {},
   "source": [
    "# Les embeddings, ou comment synthétiser l’information textuelle\n",
    "\n",
    "Lino Galiana  \n",
    "2025-12-26\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-warning callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Warning\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Ce chapitre va évoluer prochainement.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples présents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/03_embedding.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-pytorch?autoLaunch=true&name=«03_embedding»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«NLP%2003_embedding»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-pytorch?autoLaunch=true&name=«03_embedding»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«NLP%2003_embedding»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/NLP/03_embedding.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Cette page approfondit certains aspects présentés dans la\n",
    "[partie introductive](../../content/NLP/02_exoclean.qmd).\n",
    "Nous allons avancer dans notre compréhension des enjeux du NLP grâce à la\n",
    "modélisation du langage.\n",
    "\n",
    "Nous partirons de la conclusion, évoquée à la fin du précédent chapitre, que les approches fréquentistes présentent plusieurs\n",
    "inconvénients : représentation du langage sur des régularités statistiques indépendantes des proximités entre des mots ou phrases, difficulté à prendre en compte le contexte.\n",
    "\n",
    "L’objectif de ce chapitre est d’évoquer le premier point. Il s’agira d’une introduction au sujet des *embeddings*, ces représentations du langage qui sont au fondement des modèles de langage actuels utilisés par des outils entrés dans notre quotidien (`DeepL`, `ChatGPT`…).\n",
    "\n",
    "## 1.1 Données utilisées\n",
    "\n",
    "Nous allons continuer notre exploration de la littérature\n",
    "avec, à nouveau, les trois auteurs anglophones :\n",
    "\n",
    "-   Edgar Allan Poe, (EAP) ;\n",
    "-   HP Lovecraft (HPL) ;\n",
    "-   Mary Wollstonecraft Shelley (MWS).\n",
    "\n",
    "Les données sont disponibles sur un CSV mis à disposition sur [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv). L’URL pour les récupérer directement est\n",
    "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
    "\n",
    "Pour rentrer dans le sujet des *embeddings*, nous allons faire de la modélisation du langage en essayant de prédire les auteurs ayant écrit tel ou tel texte. On parle de modèle de langage pour désigner la représentation d’un texte ou d’une langue sous la forme d’une distribution de probabilités de termes (généralement les mots).\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Sources d’inspiration\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Ce chapitre s’inspire de plusieurs ressources disponibles en ligne:\n",
    "\n",
    "-   Un [premier *notebook* sur `Kaggle`](https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras)\n",
    "    et un [deuxième](https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook) ;\n",
    "-   Un [dépôt `Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg) ;\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "## 1.2 Packages à installer\n",
    "\n",
    "Comme dans la [partie précédente](../../content/NLP/02_exoclean.qmd), il faut télécharger des librairies\n",
    "spécialiséees pour le NLP, ainsi que certaines de leurs dépendances. Ce TD utilisera plusieurs librairies dont certaines dépendent de `PyTorch` qui est une librairie volumineuse.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-important callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "PyTorch sur le SSPCloud\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "**La prochaine remarque ne concerne que les utilisateurs.trices du `SSPCloud`.**\n",
    "\n",
    "Les services `Python` standards sur le `SSPCloud` (les services `vscode-python` et `jupyter-python`) ne proposent pas `PyTorch` préinstallé. Cette librairie est en effet assez volumineuse (de l’ordre de 600Mo) et nécessite un certain nombre de configurations *ad hoc* pour fonctionner de manière transparente quelle que soit la configuration logicielle derrière. Pour des raisons de frugalité écologique, cet environnement *boosté* n’est pas proposé par défaut. Néanmoins, si besoin, un tel environnement où `Pytorch` est pré à l’emploi est disponible.\n",
    "\n",
    "Pour cela, il suffit de démarrer un service `vscode-pytorch` ou `jupyter-pytorch`. Si vous avez utilisé l’un des boutons disponibles ci-dessus, c’est ce service standardisé qui a automatiquement été mis à disposition pour vous.\n",
    "\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db30011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas spacy transformers scikit-learn langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef75e2b-69cc-4066-8235-de2451880f6b",
   "metadata": {},
   "source": [
    "Ensuite, comme nous allons utiliser la librairie `SpaCy` avec un corpus de textes\n",
    "en Anglais, il convient de télécharger le modèle NLP pour l’Anglais. Pour cela,\n",
    "on peut se référer à [la documentation de `SpaCy`](https://spacy.io/usage/models),\n",
    "extrêmement bien faite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756f903-ff76-4e61-a280-72b509243d36",
   "metadata": {},
   "source": [
    "# 2. Préparation des données\n",
    "\n",
    "Nous allons à nouveau utiliser le jeu de données `spooky` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
    "spooky_df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7799b5-7f13-4594-af3f-c8f981273a79",
   "metadata": {},
   "source": [
    "Le jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f45ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spooky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97aa13b-0685-4aa9-8c02-7aeb7be650a5",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing\n",
    "\n",
    "Comme nous l’avons évoqué dans le chapitre précédent, la première étape de tout travail sur données textuelles est souvent celle du *preprocessing*, qui inclut notamment les étapes de tokenization et de nettoyage du texte.\n",
    "\n",
    "On se contentera ici d’un *preprocessing* minimaliste : suppression de la ponctuation et des *stop words* (pour la visualisation et les méthodes de vectorisation basées sur des comptages).\n",
    "\n",
    "Pour initialiser le processus de nettoyage,\n",
    "on va utiliser le corpus `en_core_web_sm` de `Spacy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac77a9-87c7-463a-a4b5-392a597b9e46",
   "metadata": {},
   "source": [
    "On va utiliser un `pipe` `spacy` qui permet d’automatiser, et de paralléliser,\n",
    "un certain nombre d’opérations. Les *pipes* sont l’équivalent, en NLP, de\n",
    "nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s’agit donc d’un outil\n",
    "très approprié pour industrialiser un certain nombre d’opérations de\n",
    "*preprocessing* :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import spacy\n",
    "\n",
    "def clean_docs(\n",
    "    texts: List[str],\n",
    "    remove_stopwords: bool = False,\n",
    "    n_process: int = 4,\n",
    "    remove_punctuation: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans a list of text documents by tokenizing, optionally removing stopwords, and optionally removing punctuation.\n",
    "\n",
    "    Parameters:\n",
    "        texts (List[str]): List of text documents to clean.\n",
    "        remove_stopwords (bool): Whether to remove stopwords. Default is False.\n",
    "        n_process (int): Number of processes to use for processing. Default is 4.\n",
    "        remove_punctuation (bool): Whether to remove punctuation. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of cleaned text documents.\n",
    "    \"\"\"\n",
    "    # Load spacy's nlp model\n",
    "    docs = nlp.pipe(\n",
    "        texts,\n",
    "        n_process=n_process,\n",
    "        disable=['parser', 'ner', 'lemmatizer', 'textcat']\n",
    "    )\n",
    "\n",
    "    # Pre-load stopwords for faster checking\n",
    "    stopwords = set(nlp.Defaults.stop_words)\n",
    "\n",
    "    # Process documents\n",
    "    docs_cleaned = (\n",
    "        ' '.join(\n",
    "            tok.text.lower().strip()\n",
    "            for tok in doc\n",
    "            if (not remove_punctuation or not tok.is_punct) and\n",
    "               (not remove_stopwords or tok.text.lower() not in stopwords)\n",
    "        )\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "    return list(docs_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f583663-ac8f-49a9-9196-9328f635f249",
   "metadata": {},
   "source": [
    "On applique la fonction `clean_docs` à notre colonne `pandas`.\n",
    "Les `pandas.Series` étant itérables, elles se comportent comme des listes et\n",
    "fonctionnent ainsi très bien avec notre `pipe` `spacy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c13d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_df['text_clean'] = clean_docs(spooky_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e16c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spooky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6b979-17a2-42d0-b053-8d2ce08070d2",
   "metadata": {},
   "source": [
    "## 2.2 Encodage de la variable à prédire\n",
    "\n",
    "On réalise un simple encodage de la variable à prédire :\n",
    "il y a trois catégories (auteurs), représentées par des entiers 0, 1 et 2.\n",
    "Pour cela, on utilise le `LabelEncoder` de `Scikit` déjà présenté\n",
    "dans la [partie modélisation](../../content/modelisation/0_preprocessing.qmd). On va utiliser la méthode\n",
    "`fit_transform` qui permet, en un tour de main, d’appliquer à la fois\n",
    "l’entraînement (`fit`), à savoir la création d’une correspondance entre valeurs\n",
    "numériques et *labels*, et l’appliquer (`transform`) à la même colonne.\n",
    "\n",
    "On peut vérifier les classes de notre `LabelEncoder` :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f7f91-546e-4a40-bb99-bc5755250e85",
   "metadata": {},
   "source": [
    "## 2.3 Construction des bases d’entraînement et de test\n",
    "\n",
    "On met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).\n",
    "Cela permettra d’évaluer nos différents modèles toute à la fin de manière très rigoureuse,\n",
    "puisque ces données n’auront jamais utilisées pendant l’entraînement.\n",
    "\n",
    "Notre échantillon initial n’est pas équilibré (*balanced*) : on retrouve plus d’oeuvres de\n",
    "certains auteurs que d’autres. Afin d’obtenir un modèle qui soit évalué au mieux, nous allons donc stratifier notre échantillon de manière à obtenir une répartition similaire d’auteurs dans nos\n",
    "ensembles d’entraînement et de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51582819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = spooky_df[\"author\"]\n",
    "X = spooky_df['text_clean']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de1aac-0960-4e20-a5e9-fb776a2c083f",
   "metadata": {},
   "source": [
    "Aperçu du premier élément de `X_train` :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0883a-ef88-433d-be56-443d83ff9369",
   "metadata": {},
   "source": [
    "# 3. Vectorisation par l’approche *bag of words*\n",
    "\n",
    "La représentation de nos textes sous forme de sac de mot permet de vectoriser notre corpus et ainsi d’avoir une représentation numérique de chaque texte. On peut à partir de là effectuer plusieurs types de tâches de modélisation.\n",
    "\n",
    "Définissons notre représentation vectorielle par TF-IDF grâce à `Scikit`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb486c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "])\n",
    "pipeline_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d2efe-f3df-4373-8aef-f923236db986",
   "metadata": {},
   "source": [
    "Entraînons d’ores et déjà notre modèle à vectoriser le texte à partir de la méthode TF-IDF. Pour le moment il n’est pas encore question de faire de l’évaluation, faisons donc un entraînement sur l’ensemble de notre base et pas seulement sur `X_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-2');</script></body>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_tfidf.fit(spooky_df['text_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2a66a-df5f-4e3f-9aed-587b25e3ee73",
   "metadata": {},
   "source": [
    "## 3.1 Trouver le texte le plus similaire\n",
    "\n",
    "En premier lieu, on peut chercher le texte le plus proche, au sens de TF-IDF, d’une phrase donnée. Prenons cet exemple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df424e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He was afraid by Frankenstein monster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45b33f-a8ed-4937-94b4-91ddd3f59a85",
   "metadata": {},
   "source": [
    "Comment retrouver le texte le plus proche de celui-ci ? Il faudrait transformer notre texte dans cette même représentation vectorielle et rapprocher ensuite celui-ci des autres textes représentés sous cette même forme.\n",
    "\n",
    "Cela revient à effectuer une tâche de recherche d’information, cas d’usage classique du NLP, mis en oeuvre par exemple par les moteurs de recherche. Le terme Frankenstein étant assez discrminant, nous devrions, grâce à TF-IDF, retrouver des similarités entre notre texte et d’autres textes écrits par Mary Shelley.\n",
    "\n",
    "Une métrique régulièrement utilisée pour comparer des vecteurs est la similarité cosinus. Il s’agit d’ailleurs d’une mesure centrale dans l’approche moderne du NLP. Celle-ci a plus de sens avec des vecteurs dense, que nous verrons prochainement, qu’avec des vecteurs comportant beaucoup de 0 comme le sont les vecteurs *sparse* des mesures *bag-of-words*. Néanmoins c’est déjà un exercice intéressant pour comprendre la similarité entre deux vecteurs.\n",
    "\n",
    "Si chaque dimension d’un vecteur correspond à une direction, l’idée derrière la similarité cosinus est de mesurer l’angle entre deux vecteurs. L’angle sera réduit si les vecteurs sont proches.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png)\n",
    "\n",
    "### 3.1.1 Avec `Scikit-Learn`\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercice 1: recherche de similarité avec TF-IDF\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  Utiliser la méthode `transform` pour vectoriser tout notre corpus d’entraînement.\n",
    "\n",
    "2.  En supposant que votre jeu d’entraînement vectorisé s’appelle `X_train_tfidf`, vous pouvez le transformer en *DataFrame* avec la commande suivante:\n",
    "\n",
    "``` python\n",
    "X_train_tfidf = pd.DataFrame(\n",
    "    X_train_tfidf.todense(), columns=pipeline_tfidf.get_feature_names_out()\n",
    ")\n",
    "```\n",
    "\n",
    "1.  Utiliser la méthode `cosine_similarity` de `Scikit` pour calculer la similarité cosinus entre notre texte vectorisé et l’ensemble du corpus d’entraînement grâce au code suivant:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    X_train_tfidf,\n",
    "    pipeline_tfidf.transform([text])\n",
    ").flatten()\n",
    "\n",
    "top_4_indices = np.argsort(cosine_similarities)[-4:][::-1]  # Tri décroissant\n",
    "top_4_similarities = cosine_similarities[top_4_indices]\n",
    "```\n",
    "\n",
    "1.  Retrouver les documents concernés. Êtes-vous satisfait du résultat ? Comprenez-vous ce qu’il s’est passé ?\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "A l’issue de l’exercice, les 4 textes les plus similaires sont:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca668d-1eaa-46c8-84fa-a48598f19bf1",
   "metadata": {},
   "source": [
    "### 3.1.2 Avec `Langchain`\n",
    "\n",
    "Cette approche de calcul de similarité textuelle est assez laborieuse avec `Scikit`. Avec le développement continu d’applications `Python` utilisant des modèles de langage, un écosystème très complet s’est développé pour pouvoir faire ces tâches en quelques lignes de code avec `Python`.\n",
    "\n",
    "Parmi les outils les plus précieux, nous trouvons [`Langchain`](https://www.langchain.com/), un écosystème `Python` haut-niveau permettant de construire des chaînes de production utilisant des données textuelles.\n",
    "\n",
    "Nous allons ici procéder en 2 étapes:\n",
    "\n",
    "-   Créer un *retriever*, c’est-à-dire vectoriser avec TF-IDF notre corpus (les textes de nos trois auteurs) et les stocker sous forme de base de données vectorielle.\n",
    "-   Vectoriser à la volée notre texte de recherche (l’objet `text` créé précédemment) et rechercher sa contrepartie la plus proche dans la base de données vectorielle.\n",
    "\n",
    "La vectorisation de notre corpus se fait très simplement grâce à `Langchain`\n",
    "puisque le `TfidfVectorizer` de `Scikit` est encapsulé dans un module *ad hoc* de `Langchain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(spooky_df, page_content_column=\"text_clean\")\n",
    "\n",
    "retriever = TFIDFRetriever.from_documents(\n",
    "    loader.load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75936da8-bc44-4f31-9019-71c43eaac1db",
   "metadata": {},
   "source": [
    "Cet objet `retriever` est un point d’entrée sur notre corpus. `Langchain` présente l’intérêt de fournir plusieurs points d’entrées standardisés, forts utiles dans les projets de NLP puisqu’il suffit de changer les vectoriseurs en entrée sans avoir à changer leur usage en fin de chaîne.\n",
    "\n",
    "La méthode `invoke` permet de rechercher les vecteurs les plus similaires à notre texte de recherche:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8d3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'id12587', 'text': 'Listen to me, Frankenstein.', 'author': 'MWS', 'author_encoded': 2}, page_content='listen to me frankenstein'),\n",
       " Document(metadata={'id': 'id09284', 'text': 'I screamed aloud that I was not afraid; that I never could be afraid; and others screamed with me for solace.', 'author': 'HPL', 'author_encoded': 1}, page_content='i screamed aloud that i was not afraid that i never could be afraid and others screamed with me for solace'),\n",
       " Document(metadata={'id': 'id09797', 'text': 'It seemed to be a sort of monster, or symbol representing a monster, of a form which only a diseased fancy could conceive.', 'author': 'HPL', 'author_encoded': 1}, page_content='it seemed to be a sort of monster or symbol representing a monster of a form which only a diseased fancy could conceive'),\n",
       " Document(metadata={'id': 'id10816', 'text': 'And, as I have implied, it was not of the dead man himself that I became afraid.', 'author': 'HPL', 'author_encoded': 1}, page_content='and as i have implied it was not of the dead man himself that i became afraid')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb884bb-3e10-45ed-8a2f-fbee5c64e487",
   "metadata": {},
   "source": [
    "La sortie est un objet `Langchain`, ce qui n’est pas pratique pour nous dans notre situation. On se ramène à un *DataFrame*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for best_echoes in retriever.invoke(text):\n",
    "    documents += [{**best_echoes.metadata, **{\"text_clean\": best_echoes.page_content}}]\n",
    "\n",
    "documents = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe907f-953e-48b3-84c9-f71e40df7c7e",
   "metadata": {},
   "source": [
    "On peut ajouter à ce *DataFrame* la colonne de score:\n",
    "\n",
    "On retrouve bien les mêmes documents:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ef41a-e226-4f32-bc98-986da79a41e6",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "La métrique BM25\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "BM25 est un modèle de récupération d’informations basé sur la pertinence probabiliste, au même titre que TF-IDF. BM25 est souvent utilisée dans les moteurs de recherche pour classer les documents par rapport à une requête.\n",
    "\n",
    "BM25 repose sur une combinaison de la fréquence des termes (TF), la fréquence inverse des documents (IDF), et une normalisation basée sur la longueur des documents. Autrement dit, il s’agit de tenir compte d’améliorer TF-IDF tout en normalisant les mesures en fonction de la taille des *strings* afin de ne pas surpondérer les grands documents.\n",
    "\n",
    "BM25 est donc particulièrement performant dans des environnements où les documents varient en longueur et en contenu. C’est pour cette raison que des moteurs de recherche comme `Elasticsearch` en ont fait une pierre angulaire du mécanisme de recherche.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Pourquoi ne sont-ils pas tous pertinents ? On peut anticiper plusieurs raisons à cela.\n",
    "\n",
    "La première hypothèse vient du fait qu’on entraîne notre vectoriseur sur un corpus biaisé. Certes Frankestein est un terme rare mais il est beaucoup plus fréquent dans notre corpus que dans la langue anglaise. L’*inverse document frequency* est donc biaisée en défaveur de ce terme: son apparition devrait être un signe beaucoup plus fort que le texte recherché correspond à Mary Shelley. Si cela peut améliorer un peu la pertinence des résultats renvoyés, ce n’est néanmoins pas là que le bât blesse.\n",
    "\n",
    "L’approche fréquentiste suppose que les termes sont aussi dissemblables les uns que les autres. Une phrase où apparaît le terme *“créature”* ne bénéficiera pas d’un score positif si on recherche *“monstre”*. De plus, là encore, nous avons pris notre corpus comme un sac où les mots sont indépendants: on n’a pas plus de chance de tirer *“Frankenstein”* après *“docteur”*. Ces limites vont nous amener vers le sujet des *embeddings*. Néanmoins, si l’approche fréquentiste est un peu *old school*, elle n’est néanmoins pas inutile et représente souvent une *“tough to beat baseline”*. Dans les domaines de l’extraction d’information avec des textes courts, où chaque terme est porteur d’un signal fort, cette approche est souvent judicieuse.\n",
    "\n",
    "## 3.2 Trouver l’auteur le plus proche: une introduction au classifieur naif Bayes\n",
    "\n",
    "Avant d’explorer les *embeddings*, nous pouvons essayer d’avoir un cas d’usage un petit peu différent dans notre cadre probabiliste. Supposons qu’on désire maintenant faire de la prédiction d’auteur. Si l’intuition précédente est vraie - certains mots sont plus probables dans les textes de certains auteurs - cela veut dire qu’on peut entraîner un algorithme de classification automatique à prédire un auteur à partir d’un texte.\n",
    "\n",
    "La méthode la plus naturelle pour se lancer dans cette approche est d’utiliser le classifieur naif de Bayes. Ce dernier est parfaitement adapté à l’approche fréquentiste que nous avons adoptée jusqu’à présent puisqu’il exploite les probabilités d’occurrence de mots par auteur.\n",
    "\n",
    "Le classifieur naif de Bayes consiste à appliquer une règle de décision, à savoir sélectionner la classe la plus probable sachant la structure observée du document, c’est-à-dire les mots apparaissant dans celui-ci.\n",
    "\n",
    "Autrement dit, on sélectionne la classe $\\widehat{c}$ qui est la plus probable, sachant les termes dans le document $d$.\n",
    "\n",
    "<span id=\"eq-definition-bayes\">$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(c|d) =  \\arg \\max_{c \\in \\mathcal{C}} \\frac{ \\mathbb{P}(d|c)\\mathbb{P}(c)}{\\mathbb{P}(d)}\n",
    " \\qquad(3.1)$$</span>\n",
    "\n",
    "Comme ceci est classique en estimation bayésienne, on peut se passer de certains termes constants, à savoir $\\mathbb{P}(d)$. La définition de la classe estimée peut ainsi être reformulée de cette manière:\n",
    "\n",
    "<span id=\"eq-rewriting-bayes\">$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(d|c)\\mathbb{P}(c)\n",
    " \\qquad(3.2)$$</span>\n",
    "\n",
    "L’hypothèse du sac de mot intervient à ce niveau. Un document $d$ est une collection de mots $w_i$ dont l’ordre n’a pas d’intérêt. Autrement dit, on peut se contenter de faire un modèle sur les mots, sans faire intervenir des probabilités conditionnelles sur l’ordre d’occurrence.\n",
    "La seconde hypothèse forte est l’hypothèse naive à laquelle la méthode doit son nom: la probabilité de tirer un mot ne dépend que de la catégorie $c$ d’appartenance du document. Autrement dit, on peut considérer qu’un document est une suite de tirage indépendants de mots dont la probabilité ne dépend que de l’auteur.\n",
    "\n",
    "Comme cela est expliqué dans la boite dédiée, en faisant ces hypothèses, on peut réécrire ce classifieur sous la forme\n",
    "\n",
    "$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(c)\\prod_{w \\in \\mathcal{W}}{\\mathbb{P}(w|c)}\n",
    "$$\n",
    "\n",
    "avec $\\mathcal{W}$ l’ensemble des mots dans le corpus (notre vocabulaire).\n",
    "\n",
    "Empiriquement, nous sommes dans une tâche d’apprentissage supervisé où le *label* est la classe du document et les *features* sont nos mots vectorisés. Empiriquement, les probabilités sont estimées à partir du dénombrement des mots dans le corpus et des types de documents dans le corpus.\n",
    "\n",
    "Il est bien-sûr possible de calculer toutes ces grandeurs à la main mais `Scikit` permet d’implémenter un estimateur naif de Bayes après avoir vectorisé son corpus comme le montre le prochain exercice. Cela peut néanmoins poser un problème pratique: en principe, le corpus de test ne doit pas comporter de nouveaux mots car ces “nouvelles” dimensions n’étaient pas présentes dans le corpus d’entraînement. En pratique, la solution la plus simple est souvent celle choisie: ces mots sont ignorés.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercice 2: le classifieur naif de Bayes\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  En repartant de l’exemple précédent, définir un *pipeline* qui vectorise chaque document (utiliser `CountVectorizer` plutôt que `TFIDFVectorizer`) et effectue une prédiction grâce à un modèle naif de Bayes.\n",
    "2.  Entraîner ce modèle, faire une prédiction sur le jeu de test.\n",
    "3.  Evaluer la performance de votre modèle\n",
    "4.  Faire une prédiction sur la phrase que nous avons utilisée tout à l’heure dans la variable `text`. Obtenez-vous ce qui était attendu ?\n",
    "5.  Regarder les probabilités obtenues (méthode `predict_proba`).\n",
    "\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d6c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-3');</script></body>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b468a8-d444-4509-a3a7-686387c5acba",
   "metadata": {},
   "source": [
    "On obtient une précision satisfaisante:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374e61b-ae0c-48cc-b343-f579434fb7b6",
   "metadata": {},
   "source": [
    "Les performances décomposées sont les suivantes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffed3a-a8ae-4e80-80b6-6ca236a9583e",
   "metadata": {},
   "source": [
    "Sans surprise, on obtient bien la prédiction de Mary Shelley:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c302e8-64cd-4f37-8a0e-d88a7b711f74",
   "metadata": {},
   "source": [
    "Finalement, si on regarde les probabilités estimées (question 5), on se rend compte que la prédiction est très certaine:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0a2ea-44ae-4f72-8425-33f1dbc9694a",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Comprendre la logique du classifieur naif de Bayes\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Supposons que nous sommes dans un problème de classification avec des classes \\$(c_1,…,c_K) (ensemble noté $\\mathcal{C}$). Nous plaçant dans le cadre de pensée du sac de mot, nous pouvons ne pas nous préoccuper des positions des mots dans les documents, qui complexifieraient beaucoup l’écriture de nos équations.\n",
    "\n",
    "L’équation <a href=\"#eq-rewriting-bayes\" class=\"quarto-xref\">Équation 3.2</a> peut être réécrite\n",
    "\n",
    "$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(w_1, ..., w_n|c)\\mathbb{P}(c)\n",
    "$$\n",
    "\n",
    "Dans le monde bayésien, on nomme $\\mathbb{P}(w_1, ..., w_n|c)$ la vraisemblance (*likelihood*) et $\\mathbb{P}(c)$ l’*a priori* (*prior*).\n",
    "\n",
    "L’hypothèse Bayes naive permet de traiter un document comme une suite de tirages aléatoires dont les probabilités ne dépendent que de la catégorie. Dans ce cas, le tirage d’une phrase est une suite de tirages de mots et la probabilité composée est donc\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(w_1, ..., w_n|c) = \\prod_{i=1}^n \\mathbb{P}(w_i|c)\n",
    "$$\n",
    "\n",
    "Par exemple, en simplifiant en deux classes, si les probabilités sont celles du <a href=\"#tbl-fake-proba\" class=\"quarto-xref\">Table 3.1</a>, la phrase *“afraid by Doctor Frankenstein”* aura un peu moins de 1% de chance (0.8%) d’être écrite si l’autrice est Mary Shelley mais sera encore moins vraisemblable chez Lovecraft (0.006%) car si *“afraid”* est très probable chez lui, Frankenstein est un événement rare qui rend peu vraisemblable cette composition de mots.\n",
    "\n",
    "| Mot ($w_i$)  | Probabilité chez Mary Shelley | Probabilité chez Lovecraft |\n",
    "|--------------|-------------------------------|----------------------------|\n",
    "| Afraid       | 0.1                           | 0.6                        |\n",
    "| By           | 0.2                           | 0.2                        |\n",
    "| Doctor       | 0.2                           | 0.05                       |\n",
    "| Frankenstein | 0.2                           | 0.01                       |\n",
    "\n",
    "Table 3.1: Exemple fictif de probabilités de tirage\n",
    "\n",
    "En composant ces différentes équations, on obtient\n",
    "\n",
    "$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(c)\\prod_{w \\in \\mathcal{W}}{\\mathbb{P}(w|c)}\n",
    "$$\n",
    "\n",
    "La contrepartie empirique de $\\mathbb{P}(c)$ est assez évidente: la fréquence observée de chaque catégorie (les auteurs) dans notre corpus. Autrement dit,\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbb{P}(c)} = \\frac{n_c}{n_{doc}}\n",
    "$$\n",
    "\n",
    "Quelle est la contrepartie empirique de $\\mathbb{P}(w_i|c)$ ? C’est la fréquence d’apparition du mot en question chez l’auteur. Pour le calculer, il suffit de compter le nombre de fois qu’il apparaît chez l’auteur et de diviser par le nombre de mots de l’auteur.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "# 4. Le modèle Word2Vec, une représentation plus synthétique\n",
    "\n",
    "## 4.1 Vers une représentation plus synthétique du langage\n",
    "\n",
    "La représentation vectorielle issue de l’approche *bag of words* n’est pas très synthétique ni stable et surtout est assez frustre.\n",
    "\n",
    "Si on a un petit corpus, on va avoir des problèmes à extrapoler puisque de nouveaux textes ont toutes les chances d’apporter de nouveaux mots, qui sont de nouvelles dimensions de *features* qui n’étaient pas présentes dans le corpus d’entraînement, ce qui conceptuellement est un problème puisque les algorithmes de *machine learning* n’ont pas vocation à prédire sur des caractéristiques sur lesquelles ils n’ont pas été entraîné[1].\n",
    "\n",
    "A l’inverse, plus on a de texte dans un corpus, plus notre représentation vectorielle sera importante. Par exemple, si votre sac de mot a vu tout le vocabulaire français, soit 60 000 mots selon l’[Académie Française](https://www.dictionnaire-academie.fr/article/QDL056) (les estimations étant de 200 000 pour la langue anglaise), cela fait des vecteurs de taille conséquente. Cependant, la diversité des textes est, en pratique, bien moindre: l’usage courant du Français nécessite plutôt autour de 3000 mots et la plupart des textes, notamment s’ils sont courts, n’utilisent pas un vocabulaire si complet. Ceci implique donc des vecteurs très peu denses, avec beaucoup de 0.\n",
    "\n",
    "La vectorisation selon cette approche est donc peu efficace; le signal est peu compressé. Des représentations denses, c’est-à-dire de dimension plus faible mais portant toutes une information, semblent plus adéquate pour pouvoir généraliser notre modélisation du langage.\n",
    "L’algorithme qui a rendu célèbre cette approche est le modèle `Word2Vec`, en quelques sortes le premier ancêtre commun des LLM modernes. La représentation vectorielle de `Word2Vec` est assez synthétique: la dimension de ces *embeddings* est entre 100 et 300.\n",
    "\n",
    "## 4.2 Des relations sémantique entre les termes\n",
    "\n",
    "Cette représentation dense va représenter une solution à une limite de l’approche *bag of words* que nous avons évoquée à de multiples reprises. Chacune de ces dimensions va représenter un facteur latent,\n",
    "c’est à dire une variable inobservée,\n",
    "de la même manière que les composantes principales produites par une ACP. Ces dimensions latentes peuvent être interprétées comme des dimensions “fondamentales” du langage\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://jalammar.github.io/images/word2vec/word2vec.png\" alt=\"Illustration du principe de la représentation de Word2Vec (source: Jay Alammar)\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration du principe de la représentation de Word2Vec (source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\">Jay Alammar</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "Par exemple, un humain sait qu’un document contenant le mot *“Roi”*\n",
    "et un autre document contenant le mot *“Reine”* ont beaucoup de chance\n",
    "d’aborder des sujets semblables. Un modèle `Word2Vec` bien entraîné va capter\n",
    "qu’il existe un facteur latent de type *“royauté”*\n",
    "et la similarité entre les vecteurs associés aux deux mots sera forte.\n",
    "\n",
    "La magie va même plus loin : le modèle captera aussi qu’il existe un\n",
    "facteur latent de type *“genre”*,\n",
    "et va permettre de construire un espace sémantique dans lequel les\n",
    "relations arithmétiques entre vecteurs ont du sens. Par exemple,\n",
    "\n",
    "$$\n",
    "\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\n",
    "$$\n",
    "\n",
    "ou, pour reprendre, l’exemple issu du papier originel `Word2Vec` (Mikolov 2013),\n",
    "\n",
    "$$\n",
    "\\text{Paris} - \\text{France} + \\text{Italy} ≈ \\text{Rome}\n",
    "$$\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://ssphub.netlify.app/post/embedding/word_embedding.png\" alt=\"Illustration du plongement lexical. Source : Post de blog Word Embedding : Basics\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration du plongement lexical. Source : Post de blog <a href=\"https://medium.com/@hari4om/word-embedding-d816f643140\">Word Embedding : Basics</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Un autre “miracle” de cette approche est qu’on obtient une forme de transfert entre les langues. Les relations sémantiques pouvant être similaires entre les langues, pour de nombreux mots usuels, on peut voir translater certaines langues les unes avec les autres si elles ont un socle commun (par exemple les langues occidentales). Ce concept est le point de départ des traducteurs automatiques et des IA multilingues\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://engineering.fb.com/wp-content/uploads/2018/01/GJ_9lgFMnVaR0ZYAAAAAAABV9MkQbj0JAAAC.gif\" alt=\"Exemple de translation entre deux représentations vectorielles. Source: Meta\" />\n",
    "<figcaption aria-hidden=\"true\">Exemple de translation entre deux représentations vectorielles. Source: <a href=\"https://engineering.fb.com/2018/01/24/ml-applications/under-the-hood-multilingual-embeddings/\">Meta</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "## 4.3 Comment ces modèles sont-ils entraînés ?\n",
    "\n",
    "Ces modèles sont entraînés à partir d’une tâche de prédiction résolue par un réseau de neurones simple, généralement avec une approche par renforcement.\n",
    "\n",
    "L’idée fondamentale est que la signification d’un mot se comprend en regardant les mots qui apparaissent fréquemment dans son voisinage. Pour un mot donné, on va donc essayer de prédire les mots qui apparaissent dans une fenêtre autour du mot cible.\n",
    "\n",
    "En répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,\n",
    "on obtient finalement des *embeddings* pour chaque mot du vocabulaire,\n",
    "qui présentent les propriétés discutées précédemment. L’ensemble des articles `Wikipedia` est un des corpus de prédilection des personnes ayant construit des plongements\n",
    "lexicaux. Il comporte en effet des phrases complètes, contrairement à des informations issues de commentaires de réseaux sociaux,\n",
    "et propose des rapprochements intéressants entre des personnes, des lieux, etc.\n",
    "\n",
    "Le contexte d’un mot est défini par une fenêtre de taille fixe autour de ce mot. La taille de la fenêtre est un paramètre de la construction de l’*embedding*. Le corpus fournit un grand ensemble d’exemples mots-contexte, qui peuvent servir à entraîner un réseau de neurones.\n",
    "\n",
    "Plus précisément, il existe deux approches, dont nous ne développerons pas les détails :\n",
    "\n",
    "-   *Continuous bag of words* (CBOW), où le modèle est entraîné à prédire un mot à partir de son contexte ;\n",
    "-   *Skip-gram*, où le modèle tente de prédire le contexte à partir d’un seul mot.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://ssphub.netlify.app/post/embedding/CBOW_Skipgram_training.png\" alt=\"Illustration de la différence entre les approches CBOW et Skip-gram\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration de la différence entre les approches CBOW et Skip-gram</figcaption>\n",
    "</figure>\n",
    "\n",
    "## 4.4 Modèles liés\n",
    "\n",
    "Plusieurs modèles ont une filiation directe avec le modèle `Word2Vec` quoiqu’ils s’en distinguent par la nature de l’architecture utilisée.\n",
    "\n",
    "C’est le cas, par exemple, du modèle modèle [`GloVe`](https://nlp.stanford.edu/projects/glove/), développé en 2014 à Stanford,\n",
    "qui ne repose pas sur des réseaux de neurones mais sur la construction d’une grande matrice de co-occurrences de mots. Pour chaque mot, il s’agit de calculer les fréquences d’apparition des autres mots dans une fenêtre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factorisée par une décomposition en valeurs singulières.\n",
    "\n",
    "Le modèle [`FastText`](https://fasttext.cc/), développé en 2016 par une équipe de `Facebook`, fonctionne de façon similaire à `Word2Vec` mais se distingue particulièrement sur deux points :\n",
    "\n",
    "-   En plus des mots eux-mêmes, le modèle apprend des représentations pour les n-grams de caractères (sous-séquences de caractères de taille $n$, par exemple *« tar »*, *« art »* et *« rte »* sont les trigrammes du mot *« tarte »*), ce qui le rend notamment robuste aux variations d’orthographe ;\n",
    "-   Le modèle a été optimisé pour que son entraînement soit particulièrement rapide.\n",
    "\n",
    "Le modèle [`FastText`](https://fasttext.cc/) est particulièrement performant pour les problématiques de classification automatique. L’Insee l’utilise par exemple pour plusieurs modèles de classification de libellés textuels dans des nomenclatures.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://ssphub.netlify.app/post/embedding/fasttext.png\" alt=\"Illustration du modèle fastText\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration du modèle fastText</figcaption>\n",
    "</figure>\n",
    "\n",
    "Voici un exemple sur un projet de classification automatisée des professions dans la typologie\n",
    "des nomenclatures d’activités (les PCS) à partir d’un modèle entraîné par la librairie `Fasttext` :\n",
    "\n",
    "[1] This remark may seem surprising while generative AIs occupy an important place in our usage. Nevertheless, we must keep in mind that while you ask new questions to AIs, you ask them in terms they know: natural language in a language present in their training corpus, digital images that are therefore interpretable by a machine, etc. In other words, your *prompt* is not, in itself, unknown to the AI, it can interpret it even if its content is new and original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a3af7-3f13-4449-b4b7-b7984705a737",
   "metadata": {},
   "source": [
    "Ces modèles sont héritiers de `Word2Vec` dans le sens où ils reprennent une représentation vectorielle dense de faible dimension de documents textuels. `Word2Vec` reste un modèle héritier de la logique sac de mot. La représentation d’une phrase ou d’un document est une forme de moyenne des représentations des mots qui les composent.\n",
    "\n",
    "Depuis 2013, plusieurs révolutions ont amené à enrichir les modèles de langage pour aller au-delà d’une représentation par mot de ceux-ci. Des architectures beaucoup plus complexes pour représenter non seulement les mots sous forme d’*embeddings* mais aussi les phrases et les documents sont aujourd’hui à l’oeuvre et peuvent être reliées à la révolution des architectures *transformers*.\n",
    "\n",
    "# 5. Les *transformers*: une représentation plus riche du langage\n",
    "\n",
    "Si le modèle `Word2Vec` est entraîné de manière contextuelle, sa vocation est de donner une représentation vectorielle d’un mot de manière absolue, indépendamment du contexte. Par exemple, le terme *“banc”* aura exactement la même représentation vectorielle qu’il se trouve dans la phrase *“Elle court vers le banc de sable”* ou “Il t’attend sur un banc au parc”\\_. C’est une limite majeure de ce type d’approche et on se doute bien de l’importance du contexte pour l’interprétation du langage.\n",
    "\n",
    "L’objectif des architectures *transformers* est de permettre des représentations vectorielles contextuelles. Autrement dit, un mot aura plusieurs représentations vectorielles, selon son contexte d’occurrence. Ces modèles s’appuient sur le mécanisme d’attention (Vaswani 2017). Avant cette approche, lorsqu’un modèle apprenait à vectoriser un texte et qu’il arrivait au énième mot, la seule mémoire qu’il gardait était celle du mot précédent. Par récurrence, cela signifiait qu’il gardait une mémoire des mots précédents mais celle-ci tendait à se dissiper. Par conséquent, pour un mot arrivant loin dans la phrase, il était probable que le contexte de début de phrase était oublié. Autrement dit, dans la phrase *“à la plage, il allait explorer le banc”*, il était fort probable qu’arrivé au mot *“banc”*, le modèle ait oublié le début de phrase qui avait pourtant de l’importance pour l’interprétation.\n",
    "\n",
    "L’objectif du mécanisme d’attention est de créer une mémoire interne au modèle permettant, pour tout mot d’un texte, de pouvoir garder trace des autres mots. Bien-sûr tous ne sont pas pertinents pour interpréter le texte mais cela évite d’oublier ceux qui sont importants. L’innovation principale des dernières années en NLP a été de parvenir à créer des mécanismes d’attention à grande échelle sans pour autant rendre intractables les modèles. Les fenêtres de contexte des modèles les plus performants deviennent immenses. Par exemple le modèle Llama 3.1 (rendu public par Meta en Juillet 2024) propose une fenêtre de contexte de 128 000 *tokens*, soit environ 96 000 mots, l’équivalent du *Hobbit* de Tolkien. Autrement dit, pour déduire la subtilité du sens d’un mot, ce modèle peut parcourir un contexte aussi long qu’un roman d’environ 300 pages.\n",
    "\n",
    "Les deux modèles qui ont marqué leur époque dans le domaine sont les modèles `BERT` développé en 2018 par *Google* (qui était déjà à l’origine de `Word2Vec`) et la première version du bien-connu `GPT` d’`OpenAI`, qui, en 2017, était le premier modèle préentraîné basé sur l’architecture *transformer*. Ces deux familles de *transformer* diffèrent dans la manière dont ils intègrent le contexte pour faire une prédiction. `GPT` est un modèle autorégressif, donc ne considère que les *tokens* avant celui dont on désire faire une prédiction. `BERT` utilise les *tokens* à gauche et à droite pour inférer le contexte. Ces deux grands modèles de langage entraînés sont entraînés par auto-renforcement, principalement sur des tâches de prédiction du prochain *token* (« The Hugging Face Course, 2022 » 2022). Depuis le succès de `ChatGPT`, les nouveaux modèles GPT (à partir de la version 3) ne sont plus *open source*. Pour les utiliser, il faut donc passer par les API d’OpenAI. Il existe néanmoins de nombreuses alternatives dont les poids sont ouverts, à défaut d’être *open source*[1], qui permettent d’utiliser ces LLM par le biais de `Python`, par le biais, notamment, de la librairie `transformers` développée par *Hugging Face*.\n",
    "\n",
    "Quand on travaille avec des corpus de taille restreinte,\n",
    "c’est généralement une mauvaise idée d’entraîner son propre modèle *from scratch*. Heureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles. Ils permettent de réaliser du *transfer learning*, c’est-à-dire de bénéficier de la performance d’un modèle qui a été entraîné sur une autre tâche ou bien sur un autre corpus.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercice 3\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  Refaire un train/test split avec 500 lignes aléatoires\n",
    "2.  Importer le modèle `all-MiniLM-L6-v2` avec le package `sentence transformers`. Encoder `X_train` et `X_test`\n",
    "3.  Faire une classification avec une méthode simple, par exemple des SVC, s’appuyant sur les *embeddings* produits à la question précédente. Comme le jeu d’entraînement est réduit, vous pouvez faire de la validation croisée.\n",
    "4.  Comprendre pourquoi les performances sont détériorées par rapport au classifieur naif de Bayes.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Réponse à la question 1:\n",
    "\n",
    "``` python\n",
    "random_rows = spooky_df.sample(500)\n",
    "y = random_rows[\"author\"]\n",
    "X = random_rows['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "Réponse à la question 2:\n",
    "\n",
    "``` python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", model_kwargs={\"torch_dtype\": \"float16\"}\n",
    ")\n",
    "\n",
    "X_train_vectors = model.encode(X_train.values)\n",
    "X_test_vectors = model.encode(X_test.values)\n",
    "```\n",
    "\n",
    "Réponse à la question 3:\n",
    "\n",
    "``` python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LinearSVC(max_iter=10000, C=0.1, dual=\"auto\")\n",
    "\n",
    "scores = cross_val_score(\n",
    "    clf, X_train_vectors, y_train,\n",
    "    cv=4, scoring='f1_micro', n_jobs=4\n",
    ")\n",
    "\n",
    "print(f\"CV scores {scores}\")\n",
    "print(f\"Mean F1 {np.mean(scores)}\")\n",
    "```\n",
    "\n",
    "**Mais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?**\n",
    "\n",
    "On peut avancer plusieurs raisons :\n",
    "\n",
    "-   le `TF-IDF` est un modèle simple, mais toujours très performant\n",
    "    (on parle de *“tough-to-beat baseline”*).\n",
    "-   la classification d’auteurs est une tâche très particulière et très ardue,\n",
    "    qui ne fait pas justice aux *embeddings*. Comme on l’a dit précédemment, ces derniers se révèlent particulièrement pertinents lorsqu’il est question de similarité sémantique entre des textes (*clustering*, etc.).\n",
    "\n",
    "Dans le cas de notre tâche de classification, il est probable que\n",
    "certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,\n",
    "ce que ne permettent pas de capter les *embeddings* qui accordent à tous les mots la même importance.\n",
    "\n",
    "Mikolov, Tomas. 2013. « Efficient estimation of word representations in vector space ». *arXiv preprint arXiv:1301.3781* 3781.\n",
    "\n",
    "« The Hugging Face Course, 2022 ». 2022. <https://huggingface.co/course>.\n",
    "\n",
    "Vaswani, A. 2017. « Attention is all you need ». *Advances in Neural Information Processing Systems*.\n",
    "\n",
    "[1] Some organizations, like Meta for Llama, make available the post-training weights of their model on the *Hugging Face* platform, allowing reuse of these models if the license permits. Nevertheless, these are not *open source* models since the code used to train the models and constitute the learning corpora, derived from massive data collection by *webscraping*, and any additional annotations to make specialized versions, are not shared.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}