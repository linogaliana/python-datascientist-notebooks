{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4cfc38-3a00-434f-9004-287f21c6a9d4",
   "metadata": {},
   "source": [
    "# Introduction à l’apprentissage non supervisé avec le clustering\n",
    "\n",
    "Lino Galiana  \n",
    "2025-12-26\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples présents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/modelisation/5_clustering.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«5_clustering»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«modelisation%205_clustering»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«5_clustering»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«modelisation%205_clustering»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/modelisation/5_clustering.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "L’ensemble de la partie *machine learning* utilise le même jeu de données, présenté dans l’[introduction de cette partie](index.qmd) : les données de vote aux élections présidentielles américaines croisées à des variables sociodémographiques. Le code est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/main/content/modelisation/get_data.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48572908",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas openpyxl plotnine plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/main/content/modelisation/get_data.py'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('getdata.py', 'wb').write(r.content)\n",
    "\n",
    "import getdata\n",
    "votes = getdata.create_votes_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd596c-5f06-4855-9b34-de31081ed70a",
   "metadata": {},
   "source": [
    "Il peut également être utile d’installer `plotnine`\n",
    "pour réaliser des graphiques simplement :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotnine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab3eb4-36ac-41fc-8c57-e640ea259526",
   "metadata": {},
   "source": [
    "# 1. Introduction sur le *clustering*\n",
    "\n",
    "Jusqu’à présent, nous avons fait de l’apprentissage supervisé puisque nous\n",
    "connaissions la vraie valeur de la variable à expliquer/prédire (`y`). Ce n’est plus le cas avec\n",
    "l’apprentissage non supervisé.\n",
    "\n",
    "Le *clustering* est un champ d’application de l’apprentissage non-supervisé.\n",
    "Il s’agit d’exploiter l’information disponible en regroupant des observations\n",
    "qui se ressemblent à partir de leurs caractéristiques (*features*) communes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f499ab9-db8f-471b-bb1e-ef15d6a3c5d9",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "\n",
    "Rappel: l’arbre de décision des méthodes `Scikit`\n",
    "\n",
    "</summary>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a4/Scikit-learn_machine_learning_decision_tree.png)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aece984-a971-450a-8996-e0ccd75ab0d4",
   "metadata": {},
   "source": [
    "L’objectif est de créer des groupes d’observations (*clusters*) pour lesquels :\n",
    "\n",
    "-   Au sein de chaque cluster, les observations sont homogènes (variance intra-cluster minimale) ;\n",
    "-   Les clusters ont des profils hétérogènes, c’est-à-dire qu’ils se distinguent les uns des autres (variance inter-cluster maximale).\n",
    "\n",
    "En *Machine Learning*, les méthodes de clustering sont très utilisées pour\n",
    "faire de la recommandation. En faisant, par exemple, des classes homogènes de\n",
    "consommateurs, il est plus facile d’identifier et cibler des comportements\n",
    "propres à chaque classe de consommateurs.\n",
    "\n",
    "Ces méthodes ont également un intérêt en économie et sciences sociales parce qu’elles permettent\n",
    "de regrouper des observations sans *a priori* et ainsi interpréter une variable\n",
    "d’intérêt à l’aune de ces résultats. Cette [publication sur la ségrégation spatiale utilisant des données de téléphonie mobile](https://www.insee.fr/fr/statistiques/4925200)\n",
    "utilise par exemple cette approche.\n",
    "Dans certaines bases de données, on peut se retrouver avec quelques exemples labellisés mais la plupart sont\n",
    "non labellisés. Les labels ont par exemple été faits manuellement par des experts.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Note\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Les méthodes de *clustering* peuvent aussi intervenir en amont d’un problème de classification (dans des\n",
    "problèmes d’apprentissage semi-supervisé).\n",
    "Le manuel *Hands-on machine learning with scikit-learn, Keras et TensorFlow* (Géron 2022) présente dans le\n",
    "chapitre dédié à l’apprentissage non supervisé quelques exemples.\n",
    "\n",
    "Par exemple, supposons que dans la [base MNIST des chiffres manuscrits](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST), les chiffres ne soient pas labellisés\n",
    "et que l’on se demande quelle est la meilleure stratégie pour labelliser cette base.\n",
    "On pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.\n",
    "Les auteurs du livre montrent qu’il existe toutefois une meilleure stratégie.\n",
    "Il vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une\n",
    "image représentative par groupe, et labelliser ces images représentatives au lieu de labelliser au hasard.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Les méthodes de *clustering* sont nombreuses.\n",
    "Nous allons nous pencher sur la plus intuitive : les *k-means*.\n",
    "\n",
    "# 2. Les k-means\n",
    "\n",
    "## 2.1 Principe\n",
    "\n",
    "L’objectif des *k-means* est de partitionner l’espace des observations en trouvant des points (*centroids*) jouant le rôle de centres de gravité pour lesquels les observations proches peuvent être regroupées dans une classe homogène.\n",
    "L’algorithme *k-means* fonctionne par itération, en initialisant les centroïdes puis en les mettant à jour à chaque\n",
    "itération, jusqu’à ce que les centroïdes se stabilisent. Quelques exemples de *clusters* issus de la méthode *k-means* :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Tip\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "L’objectif des *k-means* est de trouver une partition des données $S=\\{S_1,...,S_K\\}$ telle que\n",
    "$$\n",
    "\\arg\\min_{S} \\sum_{i=1}^K \\sum_{x \\in S_i} ||x - \\mu_i||^2\n",
    "$$\n",
    "avec $\\mu_i$ la moyenne des $x_i$ dans l’ensemble de points $S_i$.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Dans ce chapitre nous allons principalement\n",
    "utiliser `Scikit`. Voici néanmoins une proposition\n",
    "d’imports de packages, pour gagner du temps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef388517-2a6d-4b3c-9687-3f91c9026377",
   "metadata": {},
   "source": [
    "You will need the following variables in the next exercise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d28994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement de la base restreinte.\n",
    "xvars = [\n",
    "  'Unemployment_rate_2019', 'Median_Household_Income_2021',\n",
    "  'Percent of adults with less than a high school diploma, 2018-22',\n",
    "  \"Percent of adults with a bachelor's degree or higher, 2018-22\"\n",
    "]\n",
    "\n",
    "votes = votes.dropna(subset = xvars + [\"per_gop\"])\n",
    "\n",
    "df2 = votes.loc[:, xvars + [\"per_gop\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cd741-5991-4669-98f6-9e80130c4e77",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercice 1 : Principe des k-means\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  Faire un *k-means* avec $k=4$.\n",
    "2.  Créer une variable `label` dans `votes` stockant le résultat de la typologie.\n",
    "3.  Afficher cette typologie sur une carte.\n",
    "4.  Choisir les variables `Median_Household_Income_2021` et `Unemployment_rate_2019` et représenter le nuage de points en colorant différemment en fonction du label obtenu. Quel est le problème ?\n",
    "5.  Refaire les questions 2 à 5 en standardisant les variables en amont.\n",
    "6.  Représenter la distribution du vote pour chaque *cluster*.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "La carte obtenue à la question 4, qui permet de\n",
    "représenter spatialement nos groupes, est\n",
    "la suivante :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5097c-3812-4c04-ae8a-2c8076a08b6e",
   "metadata": {},
   "source": [
    "Le nuage de points de la question 5, permettant de représenter\n",
    "la relation entre `Median_Household_Income_2021`\n",
    "et `Unemployment_rate_2019`, aura l’aspect suivant :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15971f-3feb-449a-bb13-c41c38e6f79f",
   "metadata": {},
   "source": [
    "La classification apparaît un peu trop nettement dans cette figure.\n",
    "Cela suggère que la variable de revenu (`Median_Household_Income_2021`)\n",
    "explique un peu trop bien le partitionnement produit par notre\n",
    "modèle pour que ce soit normal. C’est probablement le fait\n",
    "de la variance forte du revenu par rapport aux autres variables.\n",
    "Dans ce type de situation, comme cela a été évoqué, il est\n",
    "recommandé de standardiser les variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbede14-a05d-4c2a-aef3-6c417288e31c",
   "metadata": {},
   "source": [
    "On obtient ainsi la carte suivante à la question 5 :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d23719-eb32-4785-9336-4e5ff6ca9c5a",
   "metadata": {},
   "source": [
    "Et le nuage de points de la question 5 présente un aspect moins\n",
    "déterministe, ce qui est préférable :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a1958-4c82-410b-8c78-290625fea7c6",
   "metadata": {},
   "source": [
    "Enfin, en ce qui concerne la question 6, on obtient cet\n",
    "histogramme des votes pour chaque cluster :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e2e61-8978-4540-bc09-1da096ee836f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Tip\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Il faut noter plusieurs points sur l’algorithme implémenté par défaut par `scikit-learn`, que l’on peut lire dans\n",
    "la documentation :\n",
    "\n",
    "-   l’algorithme implémenté par défaut est *kmeans++* (cf. paramètre `init`). Cela signifie que\n",
    "    l’initialisation des centroïdes est faite de manière intelligente pour que les centroïdes initiaux soient choisis\n",
    "    afin de ne pas être trop proches.\n",
    "-   l’algorithme va être démarré avec `n_init` centroïdes différents et le modèle va choisir la meilleure initialisation\n",
    "    en fonction de l’*inertie* du modèle, par défaut égale à 10.\n",
    "\n",
    "Le modèle renvoie les `cluster_centers_`, les labels `labels_`, l’inertie `inertia_` et le nombre d’itérations\n",
    "`n_iter_`.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "## 2.2 Choisir le nombre de clusters\n",
    "\n",
    "Jusqu’à présent nous avons pris comme donné le nombre de cluster, comme s’il y avait une raison légitime de penser qu’il fallait 4 plutôt que 7 profils de votes.\n",
    "\n",
    "Comme tout (hyper)paramètre dans une approche de *machine learning* on peut vouloir jouer sur la valeur et prendre, en l’absence de théorie permettant de trancher, la moins mauvaise valeur empirique.\n",
    "\n",
    "Il y a un arbitrage à faire entre biais et variance : un trop grand nombre de clusters implique une variance intra-cluster très faible ce qui est typique du sur-apprentissage, même s’il n’est jamais possible de déterminer le vrai type d’une observation puisqu’on est en apprentissage non supervisé.\n",
    "\n",
    "Sans connaissance a priori du nombre de clusters, on peut recourir à deux familles de méthodes :\n",
    "\n",
    "-   **La méthode du coude** (*elbow method*) : On prend le point d’inflexion de la courbe de performance du modèle. Cela représente le moment où ajouter un cluster supplémentaire, qui se traduit par une complexité croissante du modèle, n’apporte que des gains modérés dans la modélisation des données.\n",
    "\n",
    "-   **Le score de silhouette** : On mesure la similarité entre un point et les autres points du cluster par rapport aux autres clusters et choisit le modèle qui permet de mieux distinguer les modèles (voir <a href=\"#tip-silhouette\" class=\"quarto-xref\">Tip 2.1</a>).  \n",
    "\n",
    "> **Astuce 2.1**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Le score de silhouette\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> > Silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object iswell matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters\n",
    "> >\n",
    "> > Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    ">\n",
    "> Le score de silhouette est donc une mesure de l’arbitrage entre cohésion des clusters (dans quelle mesure les observations au sein du clusters sont homogènes) et séparation de ceux-ci (dans quelle mesure les clusters se dissocient-ils les uns des autres).\n",
    ">\n",
    "> Pour chaque observation $i$, la silhouette du point est\n",
    ">\n",
    "> $$\n",
    "> s(i) = \\frac{b(i)-a(i)}{\\max(a(i),b(i))}\n",
    "> $$\n",
    ">\n",
    "> avec $a(i)$ la distance moyenne entre $i$ et les autres points de son propre cluster (mesure de la cohésion) et $b(i)$ la plus petite distance moyenne entre $i$ et les points d’un autre cluster (mesure de la séparation).\n",
    ">\n",
    "> La valeur (s(i)) est comprise entre **-1** et **1** :\n",
    ">\n",
    "> -   **$s(i) \\approx 1$** : $a(i) \\ll b(i)$  \n",
    ">     Le point est bien assigné à son cluster : il est proche des points de son cluster et loin des autres.\n",
    ">\n",
    "> -   **$s(i) \\approx 0$** : $a(i) \\approx b(i)$  \n",
    ">     Le point est à la frontière entre deux clusters : la séparation est faible localement.\n",
    ">\n",
    "> -   **$s(i) < 0$** : $a(i) > b(i)$\n",
    ">     Le point est probablement mal assigné : en moyenne, il est plus proche d’un autre cluster que du sien.\n",
    ">\n",
    "> Le score de silhouette est la moyenne des scores de silhouette des points.\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercice: déterminer le nombre optimal de partition par la méthode du coude\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  Evaluer l’inertie et la distorsion en jouant sur le nombre de *clusters* (de 1 à 9).\n",
    "2.  Représenter graphiquement et interpréter\n",
    "\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ef25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = [\n",
    "  'Unemployment_rate_2019', 'Median_Household_Income_2021',\n",
    "  'Percent of adults with less than a high school diploma, 2018-22',\n",
    "  \"Percent of adults with a bachelor's degree or higher, 2018-22\"\n",
    "]\n",
    "\n",
    "df2 = votes.loc[:, xvars + [\"per_gop\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d9e3b-c617-4208-8f85-4508fd732c3d",
   "metadata": {},
   "source": [
    "L’inertie de notre modèle est la suivante:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d9129-d5b9-42ec-b04a-567803f45fcf",
   "metadata": {},
   "source": [
    "Là où la distorsion suit la courbe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cd996-4a26-47e2-b129-5d9acca142ee",
   "metadata": {},
   "source": [
    "## 2.3 Autres méthodes de clustering\n",
    "\n",
    "Il existe de nombreuses autres méthodes de clustering. Parmi les plus connues, on peut citer trois exemples en particulier :\n",
    "\n",
    "-   Le clustering ascendant hiérarchique ;\n",
    "-   DBSCAN ;\n",
    "-   Les mélanges de Gaussiennes.\n",
    "\n",
    "### 2.3.1 Clustering Ascendant Hiérarchique (CAH)\n",
    "\n",
    "Quel est le principe ?\n",
    "\n",
    "-   On commence par calculer la dissimilarité entre nos *N* individus, *i.e.* leur distance deux à deux dans l’espace de nos variables.\n",
    "-   Puis on regroupe les deux individus dont le regroupement minimise un critère d’agrégation donné, créant ainsi une classe comprenant ces deux individus.\n",
    "-   On calcule ensuite la dissimilarité entre cette classe et les *N-2* autres individus en utilisant le critère d’agrégation.\n",
    "-   Puis on regroupe les deux individus ou classes d’individus dont le regroupement minimise le critère d’agrégation.\n",
    "-   On continue ainsi jusqu’à ce que tous les individus soient regroupés.\n",
    "\n",
    "Ces regroupements successifs produisent un arbre binaire de classification (dendrogramme), dont la racine correspond à la classe regroupant l’ensemble des individus. Ce dendrogramme représente une hiérarchie de partitions. On peut alors choisir une partition en tronquant l’arbre à un niveau donné, le niveau dépendant soit des contraintes de l’utilisateur, soit de critères plus objectifs.\n",
    "\n",
    "Plus d’informations [ici](https://www.xlstat.com/fr/solutions/fonctionnalites/classification-ascendante-hierarchique-cah).\n",
    "\n",
    "### 2.3.2 DBSCAN\n",
    "\n",
    "L’[algorithme DBSCAN](https://fr.wikipedia.org/wiki/DBSCAN) est implémenté dans `sklearn.cluster`.\n",
    "Il peut être utilisé pour faire de la détection d’anomalies notamment.\n",
    "En effet, cette méthode repose sur le clustering en régions où la densité\n",
    "des observations est continue, grâce à la notion de voisinage selon une certaine distance epsilon.\n",
    "Pour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S’il y a au\n",
    "moins `min_samples` voisins, alors l’observation sera une *core instance*.\n",
    "\n",
    "Les observations qui ne sont pas des *core instances* et qui n’en ont pas dans leur voisinage selon une distance epsilon\n",
    "vont être détectées comme des anomalies.\n",
    "\n",
    "### 2.3.3 Les mélanges de gaussiennes\n",
    "\n",
    "En ce qui concerne la théorie, voir le cours [Probabilités numériques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka](https://perso.lpsm.paris/~rebafka/#enseignement).\n",
    "Se référer notamment aux notebooks pour l’algorithme EM pour mélange gaussien.\n",
    "\n",
    "Dans `sklearn`, les mélanges gaussiens sont implémentés dans `sklearn.mixture` comme `GaussianMixture`.\n",
    "Les paramètres importants sont alors le nombre de gaussiennes `n_components` et le nombre d’initialisations `n_init`.\n",
    "Il est possible de faire de la détection d’anomalies avec les mélanges de gaussiennes.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Pour aller plus loin\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Il existe de nombreuses autres méthodes de clustering :\n",
    "\n",
    "-   Local outlier factor ;\n",
    "-   Bayesian gaussian mixture models ;\n",
    "-   D’autres méthodes de clustering hiérarchique ;\n",
    "-   Etc.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "# 3. L’Analyse en composantes principales (ACP)\n",
    "\n",
    "## 3.1 Pour la visualisation de clusters\n",
    "\n",
    "La méthode la plus simple pour visualiser les *clusters*, peu importe la méthode avec laquelle ils ont été obtenus, serait de représenter chaque individu dans l’espace à *N* dimensions des variables de la table, et colorier chaque individu en fonction de son cluster.\n",
    "On pourrait alors bien différencier les variables les plus discrimantes et les différents groupes.\n",
    "Un seul problème ici : dès que *N \\> 3*, nous avons du mal à représenter le résultat de façon intelligible…\n",
    "\n",
    "C’est là qu’intervient l’**Analyse en Composantes Principales** ([ACP](https://www.xlstat.com/fr/solutions/fonctionnalites/analyse-en-composantes-principales-acp)), qui permet de projeter notre espace à haute dimension dans un espace de dimension plus petite.\n",
    "La contrainte majeure de la projection est de pouvoir conserver le maximum d’information (mesurée par la variance totale de l’ensemble de données) dans notre nombre réduit de dimensions, appelées composantes principales.\n",
    "En se limitant à 2 ou 3 dimensions, on peut ainsi se représenter visuellement les relations entre les observations avec une perte de fiabilité minimale.\n",
    "\n",
    "On peut généralement espérer que les clusters déterminés dans notre espace à N dimensions se différencient bien sur notre projection par ACP, et que la composition des composantes principales en fonction des variables initiales permette d’interpréter les clusters obtenus.\n",
    "En effet, la combinaison linéaire des colonnes donnant naissance à nos nouveaux axes a souvent un “sens” dans le monde réel :\n",
    "\n",
    "-   Soit parce qu’une petite poignée de variables représente la majorité de la composante ;\n",
    "-   Soit parce que la plupart des colonnes intervenant dans la composante sommée se combinent bien pour former une interprétation naturelle.\n",
    "\n",
    "Pour mettre en pratique les méthodes de création de clusters, de la base brute jusqu’à la visualisation par ACP, vous pouvez consulter la partie 2 du sujet 3 du funathon 2023, *Explorer les habitudes alimentaires de nos compatriotes*, sur le [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) ou sur [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).\n",
    "\n",
    "## 3.2 Pour la réduction de dimension\n",
    "\n",
    "L’ACP est également très utile dans le champ de la réduction du nombre de variables pour de nombreux types de modélisations, comme par exemple les régressions linéaires.\n",
    "Il est ainsi possible de projeter l’espace des variables explicatives dans un espace de dimension donnée plus faible, pour notamment limiter les risques d’*overfitting*.\n",
    "\n",
    "L’inconvénient de cette approche est qu’elle rend les données utilisées en entrée du modèle moins interprétables qu’avec un LASSO puisque cette dernière technique sélectionne des variables là où la PCA sélectionne des combinaisons linéaires de nos variables.\n",
    "\n",
    "## 3.3 Exemple\n",
    "\n",
    "Reprenons nos données précédentes. Avant de faire une analyse en composante principale, dont l’objectif est de synthétiser des sources de variabilité dans nos données, il est conseillé de standardiser les variables lorsque celles-ci ont des échelles différentes (ce qui est le cas dans notre cas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58585ace-1eb3-4e7b-9f11-46e247d2a1ad",
   "metadata": {},
   "source": [
    "Faisons déjà un premier test en réduisant nos données à deux composantes, c’est-à-dire à deux combinaisons linéaires de celles-ci. Il s’agit d’une méthode implémentée en `Scikit`, très pratique. Le faire à la main serait pénible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc6caabd-005a-4173-b254-1ea360784d0f",
   "metadata": {},
   "source": [
    "<details>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71362375-b8ce-43e3-b1aa-2328ed487cb8",
   "metadata": {},
   "source": [
    "<summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d82f9-cd6d-4f42-9dc5-f40484355069",
   "metadata": {},
   "source": [
    "Faire une PCA à la main (exercice éducatif mais peu utile dans la vraie vie)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6fa1fe3-01e6-486d-bee5-f1f6455ea6ba",
   "metadata": {},
   "source": [
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996488d-0898-4565-9448-57182777c8ff",
   "metadata": {},
   "source": [
    "::::\n",
    "\n",
    "Géron, Aurélien. 2022. *Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow*. \" O’Reilly Media, Inc.\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}