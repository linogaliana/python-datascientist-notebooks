{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e096210f-4b0b-4cd1-b9d4-0fd3f2985130",
   "metadata": {},
   "source": [
    "# Les nouveaux modes d’accès aux données : le format Parquet et les\n",
    "\n",
    "données sur le cloud\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-07\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples présents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "Nous avons vu dans les chapitres précédents comment récupérer, et harmoniser, des données issues de multiples sources: fichiers type CSV, API, *webscraping*, etc. Le panorama des manières possibles de consommer de la donnée serait incomplet sans évoquer un nouveau venu dans le paysage de la donnée, à savoir le format de données `Parquet`.\n",
    "\n",
    "Du fait de ses caractéristiques techniques pensées pour l’analyse de données, et de sa simplicité d’usage avec `Python`, ce format devient de plus en plus incontournable. Il s’agit d’ailleurs d’une pierre angulaire des infrastructures *cloud* qui, depuis le milieu des années 2010, tendent à devenir l’environnement usuel dans le domaine de la *data science* (pour plus de détails, voir le [cours de mise en production de Romain Avouac et moi](https://ensae-reproductibilite.github.io/website/chapters/big-data.html)).\n",
    "\n",
    "> **Objectif de ce chapitre**\n",
    ">\n",
    "> -   Comprendre les enjeux liés au stockage et au traitement de différents formats de données ;  \n",
    "> -   Distinguer le stockage sous forme de fichier et sous forme de base de données ;  \n",
    "> -   Découvrir le format `Parquet`, ses avantages par rapport aux formats plats ou propriétaires ;  \n",
    "> -   Apprendre à traiter ces données avec `Arrow` et `DuckDB` ;  \n",
    "> -   Identifier les implications du stockage dans le *cloud* et comment `Python` peut s’y adapter.\n",
    "\n",
    "Ce chapitre s’appuie sur un atelier dédié au sujet que j’ai donné dans le cadre du réseau des *data scientists* de la statistique publique (`SSPHub`)\n",
    "\n",
    "> **Replay de l’atelier sur ce sujet**\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Afficher les slides associées\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> <div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode yaml code-with-copy\"><code class=\"sourceCode yaml\"></code><button title=\"Copy to Clipboard\" class=\"code-copy-button\"><i class=\"bi\"></i></button></pre><iframe class=\"sourceCode yaml code-with-copy\" src=\"https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/parquet\"></iframe></div>\n",
    ">\n",
    ">\n",
    "> _[Cliquer ici](https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/parquet){target=\"_blank\"}\n",
    "> pour les afficher en plein écran._\n",
    ">\n",
    "> </details>\n",
    ">\n",
    ">\n",
    "> <details>\n",
    ">\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Regarder le replay de la session live du 09 Avril 2025:\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20250416_masterclass_parquet/GMT20250416-130715_Recording_1686x768.mp4\n",
    ">\n",
    ">\n",
    "> </details>\n",
    "\n",
    "# 1. Elements de contexte\n",
    "\n",
    "## 1.1 Principe du stockage de la donnée\n",
    "\n",
    "Avant de comprendre les apports du format `Parquet`, il est utile de revenir brièvement sur la manière dont l’information est stockée et rendue accessible à un langage de traitement comme `Python`[1].\n",
    "\n",
    "Deux approches principales coexistent : le **stockage sous forme de fichiers** et celui sous forme de **bases de données relationnelles**. La distinction entre ces deux paradigmes repose sur la façon dont l’accès aux données est organisé.\n",
    "\n",
    "## 1.2 Le stockage sous forme de fichiers\n",
    "\n",
    "### 1.2.1 Les fichiers plats\n",
    "\n",
    "Dans un fichier plat, les données sont organisées de manière linéaire, souvent séparées par un caractère (virgule, point-virgule, tabulation). Exemple avec un fichier `.csv` :\n",
    "\n",
    "``` raw\n",
    "nom ; profession \n",
    "Astérix ; \n",
    "Obélix ; Tailleur de menhir ;\n",
    "Assurancetourix ; Barde\n",
    "```\n",
    "\n",
    "`Python` peut facilement structurer cette information :\n",
    "\n",
    "[1] Pour en savoir plus sur les enjeux liés à un choix de format de données, voir @dondon2023quels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63f5fd-190b-4fe1-91a6-e37808914add",
   "metadata": {},
   "source": [
    "1.  `StringIO` permet de traiter la chaîne de caractère comme le contenu d’un fichier.\n",
    "\n",
    "À propos des fichiers de ce type, on parle de **fichiers plats** car les enregistrements relatifs à une observation sont stockés ensemble, sans hiérarchie.\n",
    "\n",
    "### 1.2.2 Les fichiers hiérarchiques\n",
    "\n",
    "D’autres formats, comme `JSON`, structurent les données de manière hiérarchique :\n",
    "\n",
    "``` json\n",
    "[\n",
    "  {\n",
    "    \"nom\": \"Astérix\"\n",
    "  },\n",
    "  {\n",
    "    \"nom\": \"Obélix\",\n",
    "    \"profession\": \"Tailleur de menhir\"\n",
    "  },\n",
    "  {\n",
    "    \"nom\": \"Assurancetourix\",\n",
    "    \"profession\": \"Barde\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Cette fois, quand on n’a pas d’information, on ne se retrouve pas avec nos deux séparateurs accolés (cf. la ligne *“Astérix”*) mais l’information\n",
    "n’est tout simplement pas collectée.\n",
    "\n",
    "> **Caution**\n",
    ">\n",
    "> La différence entre un fichier `.csv` et un fichier `JSON` ne réside pas seulement dans le format : elle implique une autre logique de stockage.\n",
    ">\n",
    "> Le format `JSON`, non tabulaire, est plus souple : il permet de mettre à jour la structure des données sans recompiler ou modifier les anciennes lignes. Cela facilite la collecte évolutive dans des contextes comme les API.\n",
    ">\n",
    "> Par exemple, un site web qui collecte de nouvelles données n’aura pas à mettre à jour l’ensemble de ses enregistrements antérieurs pour stocker la nouvelle donnée (par exemple pour indiquer que pour tel ou tel client cette donnée n’a pas été collectée) mais pourra la stocker dans un nouvel item.\n",
    ">\n",
    "> Ce sera à l’outil de requête (`Python` ou un autre outil)\n",
    "> de créer une relation entre les enregistrements stockés à des endroits différents.\n",
    ">\n",
    "> C’est ce principe qui sous-tend de nombreuses bases `NoSQL` (comme `ElasticSearch`), centrales dans l’univers du *big data*.\n",
    "\n",
    "### 1.2.3 Données réparties sur plusieurs fichiers\n",
    "\n",
    "Il est fréquent qu’une observation soit répartie entre plusieurs fichiers de formats différents. Par exemple, en géomatique, les contours géographiques peuvent être stockés de différentes manières pour accompagner les données qu’elles contextualisent:\n",
    "\n",
    "-   Soit tout est empilé dans un unique fichier qui contient à la fois les contours géographiques et les valeurs attributaires. Cette logique est celle suivie, par exemple, par le `GeoJSON` ;\n",
    "-   Soit plusieurs fichiers se répartissent l’ensemble des données et, pour lire la donnée dans son ensemble (contours géographiques, données des différentes zones géographiques, système de projection, etc.), il faudra donc associer ceux-ci pour avoir un tableau de données complet. C’est l’approche suivie par le format `Shapefile`.\n",
    "\n",
    "Lorsque la donnée est éclatée dans plusieurs fichiers, c’est alors à l’outil de traitement (ex. `Python`) d’effectuer la jonction logique.\n",
    "\n",
    "### 1.2.4 Le rôle du *file system*\n",
    "\n",
    "Le **système de fichiers** (*file system*) permet à l’ordinateur de localiser physiquement les fichiers sur le disque. C’est un composant central dans la gestion de fichiers : il assure leur nommage, leur hiérarchie et leur accès.\n",
    "\n",
    "## 1.3 Le stockage sous forme de bases de données\n",
    "\n",
    "La logique des bases de données est différente. Elle est plus systémique. Une **base de données relationnelle** est gérée par un **Système de Gestion de Base de Données** (SGBD) qui permet :\n",
    "\n",
    "-   de stocker des ensembles cohérents de données,\n",
    "-   d’en permettre la mise à jour (ajout, suppression, modification),\n",
    "-   d’en contrôler l’accès (droits utilisateurs, types de requêtes, etc.).\n",
    "\n",
    "Les données sont organisées en **tables** reliées par des **relations**, souvent selon un **schéma en étoile** :\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\" alt=\"Source: Documentation Databricks\" />\n",
    "<figcaption aria-hidden=\"true\">Source: <a href=\"https://www.databricks.com/fr/glossary/star-schema\">Documentation Databricks</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Le logiciel associé à la base de données fera ensuite le lien entre ces tables à partir de requêtes `SQL`. L’un des logiciels les plus efficaces dans ce domaine est [`PostgreSQL`](https://www.postgresql.org/).\n",
    "\n",
    "`Python` est tout à fait utilisable pour passer une requête SQL à un gestionnaire de base de données. Historiquement, les packages [`sqlalchemy`](https://www.sqlalchemy.org/) et [`psycopg2`](https://www.psycopg.org/docs/) ont été très utilisés pour envoyer des requêtes à une base de données `PostgreSQL` (lire celle-ci, la mettre à jour, etc.). Aujourd’hui, [`DuckDB`](https://duckdb.org/), sur lequel nous reviendrons lorsque nous parlerons du format `Parquet`, est un choix pratique pour passer des requêtes SQL à un SGBD `PostgreSQL`.\n",
    "\n",
    "### Pourquoi les fichiers ont le vent en poupe\n",
    "\n",
    "Le succès croissant des fichiers dans l’écosystème de la data science s’explique par plusieurs facteurs techniques et pratiques qui les rendent particulièrement adaptés aux usages analytiques modernes.\n",
    "\n",
    "En premier lieu, les fichiers sont beaucoup plus légers à manipuler que les bases de données. Ils ne nécessitent pas l’installation ou le maintien d’un logiciel de gestion spécialisé : un simple *file system*, déjà présent sur tout système d’exploitation, suffit à y accéder.\n",
    "\n",
    "Pour lire un fichier dans `Python`, il suffit d’utiliser une librairie comme `Pandas`. À l’inverse, interagir avec une base de données implique souvent :\n",
    "\n",
    "-   l’installation et la configuration d’un SGBD (comme `PostgreSQL`, `MySQL`, etc.) ;\n",
    "-   la gestion d’une connexion réseau ;\n",
    "-   le recours à des bibliothèques comme `sqlalchemy` ou `psycopg2`.\n",
    "\n",
    "Cette différence de complexité rend l’approche fichier beaucoup plus souple et rapide pour les tâches exploratoires ou ponctuelles.\n",
    "\n",
    "Cette légèreté a une contrepartie : les fichiers ne permettent pas une gestion fine des droits d’accès. Il est difficile, par exemple, d’empêcher un utilisateur de modifier ou supprimer la donnée à moins de dupliquer le fichier et de travailler sur une copie. C’est l’une des limites de l’approche fichier dans les environnements multi-utilisateurs mais auxquelles les solutions *cloud*, notamment la technologie `S3` sur laquelle nous reviendrons, apportent des réponses.\n",
    "\n",
    "La principale raison pour laquelle les fichiers sont souvent privilégiés par rapport aux SGBD réside dans la nature des opérations effectuées. Les bases de données relationnelles prennent tout leur sens lorsque l’on doit gérer des écritures fréquentes ou des mises à jour complexes sur des ensembles de données structurés — c’est-à-dire dans une **logique applicative**, où la donnée évolue continuellement (ajout, modification, suppression).\n",
    "\n",
    "À l’inverse, dans un contexte **analytique**, on se contente généralement de lire et de manipuler temporairement des données sans modifier la source. L’objectif est d’interroger, d’agréger, de filtrer — pas de pérenniser les changements. Pour ce type d’usage, les fichiers (notamment dans des formats optimisés comme `Parquet`, comme nous allons le voir) sont parfaitement adaptés : ils offrent une lecture rapide, une portabilité élevée et n’imposent pas l’intermédiation d’un moteur de base de données.\n",
    "\n",
    "# 2. Le format `Parquet`\n",
    "\n",
    "Le format `CSV` a longtemps été plébiscité en raison de sa simplicité :\n",
    "\n",
    "-   Il est **lisible par un humain** (un simple éditeur de texte suffit pour en lire le contenu) ;\n",
    "-   Il repose sur une **structure tabulaire** simple, bien adaptée à de nombreuses situations d’analyse ;\n",
    "-   Il est **universel** et interopérable, car non dépendant d’un logiciel particulier.\n",
    "\n",
    "Mais cette simplicité a un coût. Plusieurs limites du format `CSV` ont justifié l’émergence de formats plus performants pour l’analyse de données comme `Parquet`\n",
    "\n",
    "## 2.1 Limites du format `CSV`\n",
    "\n",
    "Le CSV est un format **lourd** :\n",
    "\n",
    "-   Il n’est pas compressé, ce qui augmente sa taille disque ;\n",
    "-   Toutes les données y sont stockées de façon brute. L’optimisation du typage (entier, flottant, chaîne…) est laissée à la librairie qui l’importe (comme `Pandas`), ce qui nécessite de **scanner les données** à l’ouverture, augmentant le temps de chargement et le risque d’erreur.\n",
    "\n",
    "Le CSV est **orienté ligne** :\n",
    "\n",
    "-   Pour accéder à une colonne spécifique, il faut lire **chaque ligne** du fichier puis en extraire la colonne d’intérêt ;\n",
    "-   Ce modèle est peu performant lorsqu’on souhaite ne manipuler qu’un **sous-ensemble de colonnes** — un cas très courant en data science.\n",
    "\n",
    "Le CSV est **coûteux à modifier** :\n",
    "\n",
    "-   Ajouter une colonne ou insérer une donnée intermédiaire implique de **réécrire tout le fichier**. Par exemple, ajouter une colonne `cheveux` nécessiterait de produire une nouvelle version du fichier :\n",
    "\n",
    "    ``` raw\n",
    "    nom ; cheveux ; profession\n",
    "    Astérix ; blond ; \n",
    "    Obélix ; roux ; Tailleur de menhir\n",
    "    Assurancetourix ; blond ; Barde\n",
    "    ```\n",
    "\n",
    "> **À propos des formats propriétaires**\n",
    ">\n",
    "> La plupart des outils de data science proposent des formats de sérialisation spécifiques :\n",
    ">\n",
    "> -   `.pickle` pour `Python`,  \n",
    "> -   `.rda` ou `.RData` pour `R`,  \n",
    "> -   `.dta` pour `Stata`,  \n",
    "> -   `.sas7bdat` pour `SAS`.\n",
    ">\n",
    "> Cependant, ces formats sont **propriétaires** ou **fortement couplés à un langage**, ce qui pose des problèmes d’interopérabilité. Par exemple, `Python` ne peut pas lire nativement un `.sas7bdat`. Même s’il existe des bibliothèques dédiées, l’absence de documentation officielle rend le support incertain.\n",
    ">\n",
    "> À ce titre, malgré ses limites, le `.csv` conserve l’avantage de l’universalité. Mais le format `Parquet` combine cette portabilité avec des performances bien supérieures.\n",
    "\n",
    "## 2.2 L’émergence du format `Parquet`\n",
    "\n",
    "Pour répondre à ces limites, le format `Parquet`, développé comme [projet *open-source* Apache](https://parquet.apache.org/), propose une approche radicalement différente.\n",
    "\n",
    "Sa principale caractéristique : il est **orienté colonne**. Contrairement au CSV, les données de chaque colonne sont stockées **séparément**. Cela permet :\n",
    "\n",
    "-   de charger uniquement les colonnes utiles à une analyse ;\n",
    "-   de compresser plus efficacement les données ;\n",
    "-   d’accélérer significativement les requêtes sélectives.\n",
    "\n",
    "Voici une représentation tirée du [blog d’Upsolver](https://www.upsolver.com/blog/apache-parquet-why-use) qui illustre la différence entre stockage ligne (`row-based`) et stockage colonne (`columnar`) :\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://www.upsolver.com/wp-content/uploads/2020/05/Screen-Shot-2020-05-26-at-17.52.58.png\" alt=\"Parquet vs CSV\" />\n",
    "<figcaption aria-hidden=\"true\">Parquet vs CSV</figcaption>\n",
    "</figure>\n",
    "\n",
    "Dans notre exemple, on pourrait lire la colonne `profession` sans parcourir les noms, ce qui rend l’accès plus rapide (ignorez l’élément `pyarrow.Table`, nous\n",
    "reviendrons dessus) :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6124f-469d-4258-881b-0ab3bda4612a",
   "metadata": {},
   "source": [
    "Grâce à la structure orientée colonne, il est possible de lire uniquement une variable (comme `profession`) sans avoir à parcourir toutes les lignes du fichier.\n",
    "\n",
    "``` raw\n",
    "path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        └── gender=female\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "```\n",
    "\n",
    "À la lecture, l’ensemble est reconstruit sous forme tabulaire :\n",
    "\n",
    "``` raw\n",
    "root\n",
    "|-- name: string (nullable = true)\n",
    "|-- age: long (nullable = true)\n",
    "|-- gender: string (nullable = true)\n",
    "|-- country: string (nullable = true)\n",
    "```\n",
    "\n",
    "## 2.3 Un format taillé pour l’analyse - pas uniquement le *big data*\n",
    "\n",
    "Comme le rappelle le blog d’Upsolver :\n",
    "\n",
    "> *Complex data such as logs and event streams would need to be represented as a table with hundreds or thousands of columns, and many millions of rows. Storing this table in a row-based format such as CSV would mean:*\n",
    ">\n",
    "> -   *Queries will take longer to run since more data needs to be scanned…*\n",
    "> -   *Storage will be more costly since CSVs are not compressed as efficiently as Parquet*\n",
    "\n",
    "Mais le format `Parquet` **n’est pas réservé aux architectures *big data***. Toute personne produisant ou manipulant des jeux de données bénéficiera de ses qualités :\n",
    "\n",
    "-   fichiers plus petits,\n",
    "-   import rapide et fiable,"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9eb65b3-2001-44e6-b24d-5facc56fafac",
   "metadata": {},
   "source": [
    "<!---\n",
    "\n",
    "fin traduction pour le moment\n",
    "\n",
    "---->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f8ce5-39ac-4802-94f5-844cae663677",
   "metadata": {},
   "source": [
    "## 2.4 Lire un `Parquet` en `Python`: exemple\n",
    "\n",
    "Il existe de nombreuses librairies fonctionnant bien avec `Parquet` mais les deux plus utiles à connaître sont `PyArrow` et `DuckDB`. Nous avons déjà évoqué succinctement celles-ci lorsqu’il était question des alternatives à `Pandas` gérant mieux les données volumineuses. Ces librairies peuvent servir à effectuer les premières opérations lourdes avant de convertir les données obtenues, plus légères, en `pd.DataFrame`.\n",
    "\n",
    "La librairie [`PyArrow`](https://arrow.apache.org/docs/python/) permet de lire et écrire des fichiers `Parquet` tout en tirant parti de la structure colonne du format[1]. Elle repose sur un objet `pyarrow.Table`, qui peut, une fois les calculs lourds effectués, être converti vers un `DataFrame` `Pandas` pour bénéficier d’un écosystème plus riche en fonctionnalités.\n",
    "\n",
    "La librairie [`DuckDB`](https://duckdb.org/docs/api/python/) permet d’interroger directement des fichiers `Parquet` à l’aide du langage `SQL`, sans les charger entièrement en mémoire. Autrement dit, elle reprend la philosophie du monde de la base de données (l’utilisation de SQL) mais sur des fichiers. Le résultat des requêtes peut, là aussi, être converti en `DataFrame` `Pandas`, ce qui permet de profiter à la fois de la souplesse de `Pandas` et de la performance du moteur SQL embarqué. Fonctionnalité moins connue, cette librairie permet aussi d’effectuer des opérations SQL directement sur un `DataFrame` `Pandas`. Ceci peut être pertinent pour des situations où la syntaxe `Pandas` est peu pratique là où `SQL` est très bien fait ; par exemple, pour créer une nouvelle variable comme le résultat d’une statistique par groupe.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> L’utilisation des alias `pa` pour `pyarrow` et `pq` pour `pyarrow.parquet` est une convention largement adoptée, à l’image de celle de `pd` pour `pandas`.\n",
    "\n",
    "Pour illustrer ces fonctionnalités, prenons un jeu de données issu des données synthétiques du recensement de la population diffusés par l’Insee.\n",
    "\n",
    "[1] Il est recommandé de régulièrement consulter la documentation officielle de `pyarrow` concernant [la lecture et écriture de fichiers](https://arrow.apache.org/docs/python/parquet.html) et celle relative aux [manipulations de données](https://arrow.apache.org/cookbook/py/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Example Parquet\n",
    "url = \"https://minio.lab.sspcloud.fr/projet-formation/bonnes-pratiques/data/RPindividus/REGION=93/part-0.parquet\"\n",
    "\n",
    "# Télécharger le fichier et l'enregistrer en local\n",
    "with open(\"example.parquet\", \"wb\") as f:\n",
    "    response = requests.get(url)\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1a203-c053-400f-b329-aa03a294d6a3",
   "metadata": {},
   "source": [
    "## `Arrow`\n",
    "\n",
    "L’idéal pour bénéficier pleinement des optimisations permises par le format `Parquet` est de passer par `pyarrow.dataset`. Cela permettra de bénéficier des optimisations permises par le combo `Parquet` et `Arrow`, que toutes les manières de lire un `Parquet` avec `Arrow` ne proposent pas (cf. prochains exercices).\n",
    "\n",
    "``` python\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\n",
    "  \"example.parquet\"\n",
    ").scanner(columns = [\"AGED\", \"IPONDI\", \"DEPT\"])\n",
    "table = dataset.to_table()\n",
    "table\n",
    "```\n",
    "\n",
    "    pyarrow.Table\n",
    "    AGED: int32\n",
    "    IPONDI: double\n",
    "    DEPT: dictionary<values=string, indices=int32, ordered=0>\n",
    "    ----\n",
    "    AGED: [[9,12,40,70,52,...,29,66,72,75,77],[46,76,46,32,2,...,7,5,37,29,4],...,[67,37,45,56,75,...,64,37,47,20,18],[16,25,51,6,11,...,93,90,92,21,65]]\n",
    "    IPONDI: [[2.73018871840726,2.73018871840726,2.73018871840726,0.954760150327854,3.75907197064638,...,3.27143319621654,4.83980378599556,4.83980378599556,4.83980378599556,4.83980378599556],[3.02627578376137,3.01215358930406,3.01215358930406,2.93136309038958,2.93136309038958,...,2.96848755763453,2.96848755763453,3.25812879950072,3.25812879950072,1.12514509319438],...,[2.57931132917563,2.85579410739065,0.845993555838931,2.50296716736141,3.70786113613679,...,3.08375347880892,2.88038807573222,3.22776230929947,3.22776230929947,3.22776230929947],[3.22776230929947,3.22776230929947,3.22776230929947,3.29380242174036,3.29380242174036,...,5.00000768518755,5.00000768518755,5.00000768518755,5.00000768518755,1.00000153703751]]\n",
    "    DEPT: [  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [5,5,5,5,5,...,5,5,5,5,5],  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [5,5,5,5,5,...,5,5,5,5,5],...,  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [84,84,84,84,84,...,84,84,84,84,84],  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [84,84,84,84,84,...,84,84,84,84,84]]\n",
    "\n",
    "Pour importer et traiter ces données, on peut conserver\n",
    "les données sous le format `pyarrow.Table`\n",
    "ou transformer en `pandas.DataFrame`. La deuxième\n",
    "option est plus lente mais présente l’avantage\n",
    "de permettre ensuite d’appliquer toutes les\n",
    "manipulations offertes par l’écosystème\n",
    "`pandas` qui est généralement mieux connu que\n",
    "celui d’`Arrow`.\n",
    "\n",
    "## `DuckDB`\n",
    "\n",
    "``` python\n",
    "import duckdb\n",
    "duckdb.sql(\"\"\"\n",
    "FROM read_parquet('example.parquet')\n",
    "SELECT AGED, IPONDI, DEPT\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "    ┌───────┬───────────────────┬─────────┐\n",
    "    │ AGED  │      IPONDI       │  DEPT   │\n",
    "    │ int32 │      double       │ varchar │\n",
    "    ├───────┼───────────────────┼─────────┤\n",
    "    │     9 │  2.73018871840726 │ 06      │\n",
    "    │    12 │  2.73018871840726 │ 06      │\n",
    "    │    40 │  2.73018871840726 │ 06      │\n",
    "    │    70 │ 0.954760150327854 │ 06      │\n",
    "    │    52 │  3.75907197064638 │ 06      │\n",
    "    │    82 │  3.21622922493506 │ 06      │\n",
    "    │     6 │  3.44170061276923 │ 06      │\n",
    "    │    12 │  3.44170061276923 │ 06      │\n",
    "    │    15 │  3.44170061276923 │ 06      │\n",
    "    │    43 │  3.44170061276923 │ 06      │\n",
    "    │     · │          ·        │ ·       │\n",
    "    │     · │          ·        │ ·       │\n",
    "    │     · │          ·        │ ·       │\n",
    "    │    68 │  2.73018871840726 │ 06      │\n",
    "    │    35 │  3.46310256220757 │ 06      │\n",
    "    │     2 │  3.46310256220757 │ 06      │\n",
    "    │    37 │  3.46310256220757 │ 06      │\n",
    "    │    84 │  3.69787960424482 │ 06      │\n",
    "    │    81 │   4.7717265388427 │ 06      │\n",
    "    │    81 │   4.7717265388427 │ 06      │\n",
    "    │    51 │  3.60566450823737 │ 06      │\n",
    "    │    25 │  3.60566450823737 │ 06      │\n",
    "    │    13 │  3.60566450823737 │ 06      │\n",
    "    ├───────┴───────────────────┴─────────┤\n",
    "    │    ? rows (>9999 rows, 20 shown)    │\n",
    "    └─────────────────────────────────────┘\n",
    "\n",
    "## 2.5 Des exercices pour en apprendre plus\n",
    "\n",
    "Voici une série d’exercices issues du cours de [mise en production de projets data science](https://ensae-reproductibilite.github.io/website/chapters/big-data.html#sec-new-formats) que Romain Avouac et moi proposons à la fin du cursus d’ingénieurs de l’ENSAE.\n",
    "\n",
    "Ces exercices illustrent progressivement quelques concepts présentés ci-dessus tout en présentant les bonnes pratiques à adopter pour traiter des données volumineuses. La correction de ces exercices est disponible sur la page du cours en question.\n",
    "\n",
    "Tout au long de cette application, nous allons voir comment utiliser le format `Parquet` de la manière la plus efficiente possible. Afin de comparer les différents formats et méthodes d’utilisation, nous allons **comparer le temps d’exécution et l’usage mémoire d’une requête standard**. Commençons déjà, sur un premier exemple avec une donnée légère, pour comparer les formats `CSV` et `Parquet`.\n",
    "\n",
    "Pour cela, nous allons avoir besoin de récupérer des données au format `Parquet`. Nous proposons d’utiliser les données détaillées et anonymisées du recensement de la population française: environ 20 millions de lignes pour 80 colonnes. Le code pour récupérer celles-ci est donné ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd334b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "\n",
    "# Définir le fichier de destination\n",
    "filename_table_individu = \"data/RPindividus.parquet\"\n",
    "\n",
    "# Copier le fichier depuis le stockage distant (remplacer par une méthode adaptée si nécessaire)\n",
    "os.system(\"mc cp s3/projet-formation/bonnes-pratiques/data/RPindividus.parquet data/RPindividus.parquet\")\n",
    "\n",
    "# Charger le fichier Parquet\n",
    "table = pq.read_table(filename_table_individu)\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Filtrer les données pour REGION == \"24\"\n",
    "df_filtered = df.loc[df[\"REGION\"] == \"24\"]\n",
    "\n",
    "# Sauvegarder en CSV\n",
    "df_filtered.to_csv(\"data/RPindividus_24.csv\", index=False)\n",
    "\n",
    "# Sauvegarder en Parquet\n",
    "pq.write_table(pa.Table.from_pandas(df_filtered), \"data/RPindividus_24.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595e504-9555-4e4e-96ae-94529a97beb7",
   "metadata": {},
   "source": [
    "> **Si vous n’êtes pas sur le `SSPCloud`**\n",
    ">\n",
    "> Vous devrez remplacer la ligne\n",
    ">\n",
    "> ``` python\n",
    "> os.system(\"mc cp s3/projet-formation/bonnes-pratiques/data/RPindividus.parquet data/RPindividus.parquet\")\n",
    "> ```\n",
    ">\n",
    "> qui utilise l’outil en ligne de commande `mc` par un code téléchargeant cette donnée à partir de l’URL <https://projet-formation.minio.lab.sspcloud.fr/bonnes-pratiques/data/RPindividus.parquet>.\n",
    ">\n",
    "> Il y a de nombreuses manières de faire. Vous pouvez par exemple le faire en pur `Python` avec `requests`. Si vous avez `curl` installé, vous pouvez aussi l’utiliser. Par l’intermédiaire de `Python`, cela donnera la commande `os.system(\"curl -o data/RPindividus.parquet https://projet-formation/bonnes-pratiques/data/RPindividus.parquet\")`.\n",
    "\n",
    "Ces exercices vont utiliser des décorateurs `Python`, c’est-à-dire des fonctions qui surchargent le comportement d’une autre fonction. En l’occurrence, nous allons créer une fonction exécutant une chaîne d’opérations et la surcharger avec une autre chargée de contrôler l’usage mémoire et le temps d’exécution.\n",
    "\n",
    "> **Partie 1 : Du `CSV` au `Parquet`**\n",
    ">\n",
    "> -   Créer un notebook `benchmark_parquet.ipynb` afin de réaliser les différentes comparaisons de performance de l’application\n",
    "> -   Créons notre décorateur, en charge de *benchmarker* le code `Python`:\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Dérouler pour retrouver le code du décorateur permettant de mesurer la performance\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ::: {#904c3a4a .cell execution_count=8}\n",
    "> \\`\\`\\` {.python .cell-code}\n",
    "> import time\n",
    "> from memory_profiler import memory_usage\n",
    "> from functools import wraps\n",
    "> import warnings\n",
    ">\n",
    ">     def convert_size(size_bytes):\n",
    ">     if size_bytes == 0:\n",
    ">         return \"0B\"\n",
    ">     size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    ">     i = int(math.floor(math.log(size_bytes, 1024)))\n",
    ">     p = math.pow(1024, i)\n",
    ">     s = round(size_bytes / p, 2)\n",
    ">     return \"%s %s\" % (s, size_name[i])\n",
    ">\n",
    ">     # Decorator to measure execution time and memory usage\n",
    ">     def measure_performance(func, return_output=False):\n",
    ">         @wraps(func)\n",
    ">         def wrapper(return_output=False, *args, **kwargs):\n",
    ">             warnings.filterwarnings(\"ignore\")\n",
    ">             start_time = time.time()\n",
    ">             mem_usage = memory_usage((func, args, kwargs), interval=0.1)\n",
    ">             end_time = time.time()\n",
    ">             warnings.filterwarnings(\"always\")\n",
    ">\n",
    ">             exec_time = end_time - start_time\n",
    ">             peak_mem = max(mem_usage)  # Peak memory usage\n",
    ">             exec_time_formatted = f\"\\033[92m{exec_time:.4f} sec\\033[0m\"\n",
    ">             peak_mem_formatted = f\"\\033[92m{convert_size(1024*peak_mem)}\\033[0m\"\n",
    ">\n",
    ">             print(f\"{func.__name__} - Execution Time: {exec_time_formatted} | Peak Memory Usage: {peak_mem_formatted}\")\n",
    ">             if return_output is True:\n",
    ">                 return func(*args, **kwargs)\n",
    ">\n",
    ">         return wrapper\n",
    ">\n",
    ">     :::\n",
    ">\n",
    ">\n",
    ">     </details>\n",
    ">\n",
    ">     * Reprendre ce code pour encapsuler un code de construction d'une pyramide des âges dans une fonction `process_csv_appli1`\n",
    ">\n",
    ">     <details>\n",
    ">\n",
    ">     <summary>\n",
    ">     Dérouler pour récupérer le code pour mesurer les performances de la lecture en CSV\n",
    ">     </summary>\n",
    ">\n",
    ">\n",
    ">     ::: {#13d8ed7e .cell execution_count=9}\n",
    ">     ``` {.python .cell-code}\n",
    ">     # Apply the decorator to functions\n",
    ">     @measure_performance\n",
    ">     def process_csv_appli1(*args, **kwargs):\n",
    ">         df = pd.read_csv(\"data/RPindividus_24.csv\")\n",
    ">         return (\n",
    ">             df.loc[df[\"DEPT\"] == 36]\n",
    ">             .groupby([\"AGED\", \"DEPT\"])[\"IPONDI\"]\n",
    ">             .sum().reset_index()\n",
    ">             .rename(columns={\"IPONDI\": \"n_indiv\"})\n",
    ">         )\n",
    ">\n",
    "> :::\n",
    ">\n",
    "> </details>\n",
    ">\n",
    "> -   Exécuter `process_csv_appli1()` et `process_csv_appli1(return_output=True)`\n",
    ">\n",
    "> -   Sur le même modèle, construire une fonction `process_parquet_appli1` basée cette fois sur le fichier `data/RPindividus_24.parquet` chargé avec la fonction [read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) de `Pandas`\n",
    ">\n",
    "> -   Comparer les performances (temps d’exécution et allocation mémoire) de ces deux méthodes grâce à la fonction.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Correction complète\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ``` python\n",
    "> import math\n",
    "> import pandas as pd\n",
    "> import time\n",
    "> from memory_profiler import memory_usage\n",
    "> from functools import wraps\n",
    "> import warnings\n",
    ">\n",
    "> def convert_size(size_bytes):\n",
    ">    if size_bytes == 0:\n",
    ">        return \"0B\"\n",
    ">    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    ">    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    ">    p = math.pow(1024, i)\n",
    ">    s = round(size_bytes / p, 2)\n",
    ">    return \"%s %s\" % (s, size_name[i])\n",
    ">\n",
    "> # Decorator to measure execution time and memory usage\n",
    "> def measure_performance(func, return_output=False):\n",
    ">     @wraps(func)\n",
    ">     def wrapper(return_output=False, *args, **kwargs):\n",
    ">         warnings.filterwarnings(\"ignore\")\n",
    ">         start_time = time.time()\n",
    ">         mem_usage = memory_usage((func, args, kwargs), interval=0.1)\n",
    ">         end_time = time.time()\n",
    ">         warnings.filterwarnings(\"always\")\n",
    ">\n",
    ">         exec_time = end_time - start_time\n",
    ">         peak_mem = max(mem_usage)  # Peak memory usage\n",
    ">         exec_time_formatted = f\"\\033[92m{exec_time:.4f} sec\\033[0m\"\n",
    ">         peak_mem_formatted = f\"\\033[92m{convert_size(1024*peak_mem)}\\033[0m\"\n",
    ">\n",
    ">         print(f\"{func.__name__} - Execution Time: {exec_time_formatted} | Peak Memory Usage: {peak_mem_formatted}\")\n",
    ">         if return_output is True:\n",
    ">             return func(*args, **kwargs)\n",
    ">\n",
    ">     return wrapper\n",
    ">\n",
    "> # Apply the decorator to functions\n",
    "> @measure_performance\n",
    "> def process_csv(*args, **kwargs):\n",
    ">     df = pd.read_csv(\"data/RPindividus_24.csv\")\n",
    ">     return (\n",
    ">         df.loc[df[\"DEPT\"] == 36]\n",
    ">         .groupby([\"AGED\", \"DEPT\"])[\"IPONDI\"]\n",
    ">         .sum().reset_index()\n",
    ">         .rename(columns={\"IPONDI\": \"n_indiv\"})\n",
    ">     )\n",
    ">\n",
    "> @measure_performance\n",
    "> def process_parquet(*args, **kwargs):\n",
    ">     df = pd.read_parquet(\"data/RPindividus_24.parquet\")\n",
    ">     return (\n",
    ">         df.loc[df[\"DEPT\"] == \"36\"]\n",
    ">         .groupby([\"AGED\", \"DEPT\"])[\"IPONDI\"]\n",
    ">         .sum().reset_index()\n",
    ">         .rename(columns={\"IPONDI\": \"n_indiv\"})\n",
    ">     )\n",
    ">\n",
    "> process_csv()\n",
    "> process_parquet()\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    "\n",
    "*❓️ Quelle semble être la limite de la fonction `read_parquet` ?*\n",
    "\n",
    "On gagne déjà un temps conséquent en lecture mais on ne bénéficie pas vraiment de l’optimisation permise par `Parquet` car on transforme les données directement après la lecture en `DataFrame` `Pandas`. On n’utilise donc pas l’une des fonctionnalités principales du format `Parquet`, qui explique ses excellentes performances: le *predicate pushdown* qui consiste à optimiser notre traitement pour faire remonter, le plus tôt possible, les filtres sur les colonnes pour ne garder que celles vraiment utilisées dans le traitement.\n",
    "\n",
    "> **Partie 2 : Exploiter la *lazy evaluation* et les optimisations d’`Arrow` ou de `DuckDB`**\n",
    ">\n",
    "> La partie précédente a montré un **gain de temps considérable** du passage de `CSV` à `Parquet`. Néanmoins, l’**utilisation mémoire était encore très élevée** alors qu’on utilise de fait qu’une infime partie du fichier.\n",
    ">\n",
    "> Dans cette partie, on va voir comment utiliser la ***lazy evaluation*** et les **optimisations du plan d’exécution** effectuées par `Arrow` pour exploiter pleinement la puissance du format `Parquet`.\n",
    ">\n",
    "> -   Ouvrir le fichier `data/RPindividus_24.parquet` avec [pyarrow.dataset](https://arrow.apache.org/docs/python/dataset.html). Regarder la classe de l’objet obtenu.\n",
    "> -   Tester le code ci-dessous pour lire un échantillon de données:\n",
    ">\n",
    "> ``` python\n",
    "> (\n",
    ">     dataset.scanner()\n",
    ">     .head(5)\n",
    ">     .to_pandas()\n",
    "> )\n",
    "> ```\n",
    ">\n",
    "> Comprenez-vous la différence avec précédemment ? Observez dans la documentation la méthode `to_table` : comprenez-vous son principe ?\n",
    ">\n",
    "> -   Construire une fonction `summarize_parquet_arrow` (resp. `summarize_parquet_duckdb`) qui importe cette fois les données avec la fonction [`pyarrow.dataset`](https://arrow.apache.org/docs/python/dataset.html) (resp. avec `DuckDB`) et effectue l’agrégation voulue.\n",
    "> -   Comparer les performances (temps d’exécution et allocation mémoire) des trois méthodes (`Parquet` lu et processé avec `Pandas`, `Arrow` et `DuckDB`) grâce à notre fonction.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Correction\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ``` python\n",
    "> import duckdb\n",
    "> import pyarrow.dataset as ds\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_parquet_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus_24.parquet')\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_parquet_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus_24.parquet\", format=\"parquet\")\n",
    ">     table = dataset.to_table()\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    "> process_parquet()\n",
    "> summarize_parquet_duckdb()\n",
    "> summarize_parquet_arrow()\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    "\n",
    "Avec l’évaluation différée, on obtient donc un processus en plusieurs temps:\n",
    "\n",
    "-   `Arrow` ou `DuckDB` reçoit des instructions, les optimise, exécute les requêtes\n",
    "-   Seules les données en sortie de cette chaîne sont renvoyées à `Python`\n",
    "\n",
    "![](https://linogaliana.github.io/parquet-recensement-tutomate/img/duckdb-delegation1.png)\n",
    "\n",
    "> **Partie 3a : Et si on filtrait sur les lignes ?**\n",
    ">\n",
    "> Ajoutez une étape de filtre sur les lignes dans nos requêtes:\n",
    ">\n",
    "> -   Avec `DuckDB`, vous devez modifier la requête avec un `WHERE DEPT IN ('18', '28', '36')`\n",
    "> -   Avec `Arrow`, vous devez modifier l’étape `to_table` de cette manière: `dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))`\n",
    ">\n",
    "> ``` python\n",
    "> import pyarrow.dataset as ds\n",
    "> import pyarrow.compute as pc\n",
    "> import duckdb\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus.parquet\", format=\"parquet\")\n",
    ">     table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus_24.parquet')\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     WHERE DEPT IN ('11','31','34')\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    "> summarize_filter_parquet_arrow()\n",
    "> summarize_filter_parquet_duckdb()\n",
    "> ```\n",
    "\n",
    "*❓️ Pourquoi ne gagne-t-on pas de temps avec nos filtres sur les lignes (voire pourquoi en perdons nous?) comme c’est le cas avec les filtres sur les colonnes ?*\n",
    "\n",
    "La donnée n’est pas organisée par blocs de lignes comme elle l’est par bloc de colonne. Heureusement, il existe pour cela un moyen: le partitionnement !\n",
    "\n",
    "> **Partie 3 : Le `Parquet` partitionné**\n",
    ">\n",
    "> La *lazy evaluation* et les optimisations d’`Arrow` apportent des gain de performance considérables. Mais on peut encore faire mieux ! Lorsqu’on sait qu’on va être amené à **filter régulièrement les données selon une variable d’intérêt**, on a tout intérêt à **partitionner** le fichier `Parquet` selon cette variable.\n",
    ">\n",
    "> 1.  Parcourir la documentation de la fonction [`pyarrow.parquet.write_to_dataset`](https://arrow.apache.org/docs/python/parquet.html#writing-to-partitioned-datasets) pour comprendre comment spécifier une clé de partitionnement lors de l’écriture d’un fichier `Parquet`. Plusieurs méthodes sont possibles.\n",
    ">\n",
    "> 2.  Importer la table complète des individus du recensement depuis `\"data/RPindividus.parquet\"` avec la fonction [`pyarrow.dataset.dataset`](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.dataset.html) et l’exporter en une table partitionnée `\"data/RPindividus_partitionne.parquet\"`, partitionnée par la région (`REGION`) et le département (`DEPT`).\n",
    ">\n",
    "> 3.  Observer l’arborescence des fichiers de la table exportée pour voir comment la partition a été appliquée.\n",
    ">\n",
    "> 4.  Modifier nos fonctions d’import, filtre et agrégations via `Arrow` ou `DuckDB` pour utiliser, cette fois, le `Parquet` partitionné. Comparer à l’utilisation du fichier non partitionné.\n",
    ">\n",
    "> ``` python\n",
    "> import pyarrow.parquet as pq\n",
    "> dataset = ds.dataset(\n",
    ">     \"data/RPindividus.parquet\", format=\"parquet\"\n",
    "> ).to_table()\n",
    ">\n",
    "> pq.write_to_dataset(\n",
    ">     dataset,\n",
    ">     root_path=\"data/RPindividus_partitionne\",\n",
    ">     partition_cols=[\"REGION\", \"DEPT\"]\n",
    "> )\n",
    "> ```\n",
    ">\n",
    "> ``` python\n",
    "> import pyarrow.dataset as ds\n",
    "> import pyarrow.compute as pc\n",
    "> import duckdb\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_partitioned_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus_partitionne/\", partitioning=\"hive\")\n",
    ">     table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n",
    ">\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_complete_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus.parquet\")\n",
    ">     table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n",
    ">\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_complete_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus.parquet')\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     WHERE DEPT IN ('11','31','34')\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_partitioned_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus_partitionne/**/*.parquet', hive_partitioning = True)\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     WHERE DEPT IN ('11','31','34')\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    ">\n",
    "> summarize_filter_parquet_complete_arrow()\n",
    "> summarize_filter_parquet_partitioned_arrow()\n",
    "> summarize_filter_parquet_complete_duckdb()\n",
    "> summarize_filter_parquet_partitioned_duckdb()\n",
    "> ```\n",
    "\n",
    "*❓️ Dans le cadre d’une mise à disposition de données en `Parquet`, comment bien choisir la/les clé(s) de partitionnement ? Quelle est la limite à garder en tête ?*\n",
    "\n",
    "## 2.6 Pour aller plus loin\n",
    "\n",
    "-   La [formation aux bonnes pratiques `R` et `Git`](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/) développée par l’Insee avec des éléments très similaires à ceux présentés dans ce chapitre.\n",
    "-   Un [atelier](https://linogaliana.github.io/parquet-recensement-tutomate/) sur le format `Parquet` et l’écosystème `DuckDB` pour l’EHESS avec des exemples `R` et `Python` utilisant la même source de données que l’application.\n",
    "-   Le [guide de prise en main](https://ssphub.netlify.app/post/parquetrp/) des données du recensement au format `Parquet` avec des exemples d’utilisation de `DuckDB` en WASM (directement depuis le navigateur, sans installation `R` ou `Python`)\n",
    "\n",
    "# 3. Les données dans le *cloud*\n",
    "\n",
    "Le stockage *cloud*, dans le contexte de la science des données, reprend le principe de services comme `Dropbox` ou `Google Drive` : un utilisateur accède à des fichiers distants comme s’ils étaient sur son propre disque[1]. Autrement dit, pour un utilisateur de `Python`, **la manipulation de fichiers stockés dans le cloud peut sembler identique** à celle de fichiers locaux.\n",
    "\n",
    "Mais contrairement à un dossier de type `Mes Documents/monsuperfichier`, les fichiers ne résident pas sur l’ordinateur local. Ils sont hébergés sur un serveur distant, et chaque opération (lecture, écriture) passe par une connexion réseau.\n",
    "\n",
    "## 3.1 Pourquoi ne pas utiliser `Dropbox` ou `Drive` ?\n",
    "\n",
    "Néanmoins, `Dropbox` ou `Drive` ne sont pas faits pour du stockage de données. Pour ces dernières, il est plus pertinent d’utiliser une technologie adaptée (voir [le cours de mise en production](https://ensae-reproductibilite.github.io/website/chapters/big-data.html)). Les principaux fournisseurs de service *cloud* (AWS, GCP, Azure…) reposent sur le même principe avec un stockage orienté objet reposant sur une technologie de type `S3`.\n",
    "\n",
    "C’est pourquoi les principaux fournisseurs *cloud* (AWS, Google Cloud, Azure…) proposent des solutions spécifiques au stockage de données, souvent basées sur des systèmes orientés objet, dont le plus connu est `S3`.\n",
    "\n",
    "## 3.2 Le système `S3`\n",
    "\n",
    "Le système `S3` (*Simple Storage Service*), développé par Amazon, est devenu un standard dans le monde du stockage cloud. Il s’agit d’un système :\n",
    "\n",
    "-   **fiable** (réplication des données) ;\n",
    "-   **sécurisé** (données chiffrées, contrôle d’accès granulaire) ;\n",
    "-   **scalable** (adapté à des volumes massifs).\n",
    "\n",
    "L’unité centrale de S3 est le **bucket** : un espace de stockage (privé ou public) qui peut contenir une arborescence de fichiers.\n",
    "\n",
    "Pour accéder à un fichier dans un bucket :\n",
    "\n",
    "-   L’utilisateur doit être **autorisé** (via des identifiants ou des jetons d’accès, souvent appelés *tokens*) ;\n",
    "-   Une fois authentifié, il peut **lire, écrire ou modifier** les fichiers à l’intérieur du bucket, à la manière d’un système de fichiers distant.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Les exemples suivants seront réplicables pour les utilisateurs de la plateforme\n",
    "> SSP Cloud\n",
    ">\n",
    "> <div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples présents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "> <a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "> <a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "> <a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    ">\n",
    "> Ils peuvent également l’être pour des utilisateurs ayant un\n",
    "> accès à AWS, il suffit de changer l’URL du `endpoint`\n",
    "> présenté ci-dessous.\n",
    "\n",
    "## 3.3 Comment faire avec Python ?\n",
    "\n",
    "### 3.3.1 Les librairies principales\n",
    "\n",
    "L’interaction entre ce système distant de fichiers et une session locale de Python\n",
    "est possible grâce à des API. Les deux principales librairies sont les suivantes :\n",
    "\n",
    "-   [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), une librairie créée et maintenue par Amazon ;\n",
    "-   [s3fs](https://s3fs.readthedocs.io/en/latest/), une librairie qui permet d’interagir avec les fichiers stockés à l’instar d’un filesystem classique.\n",
    "\n",
    "Les librairies `pyarrow` et `duckdb` que nous avons déjà présentées permettent également de traiter des données stockées sur le *cloud* comme si elles\n",
    "étaient sur le serveur local. C’est extrêmement pratique\n",
    "et permet de fiabiliser la lecture ou l’écriture de fichiers\n",
    "dans une architecture *cloud*.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Sur le SSP Cloud, les jetons d’accès au stockage S3 sont injectés automatiquement dans les services lors de leur création. Ils sont ensuite valides pour une durée de 7 jours. Si l’icône du service passe du vert au rouge, cela signifie que ces jetons sont périmés, il faut donc sauvegarder son code / ses données et reprendre depuis un nouveau service.\n",
    "\n",
    "## 3.4 Cas pratique : stocker les données de son projet sur le SSP Cloud\n",
    "\n",
    "Une composante essentielle de l’évaluation des projets `Python` est la **reproductibilité**, i.e. la possibilité de retrouver les mêmes résultats à partir des mêmes données d’entrée et du même code. Dans la mesure du possible, il faut donc que votre rendu final parte des données brutes utilisées comme source dans votre projet. Si les fichiers de données source sont accessibles via une URL publique par exemple, il est idéal de les importer directement à partir de cette URL au début de votre projet (voir le [TP Pandas](../../content/manipulation/02_pandas_suite.qmd) pour un exemple d’un tel import via `Pandas`).\n",
    "\n",
    "En pratique, cela n’est pas toujours possible. Peut-être que vos données ne sont pas directement publiquement accessibles, ou bien sont disponibles sous des formats complexes qui demandent des pré-traitements avant d’être exploitables dans un format de donnée standard. Peut-être que vos données résultent d’une phase de récupération automatisée via une [API](../../content/manipulation/04c_API_TP.qmd) ou du [webscraping](../../content/manipulation/04a_webscraping_TP.qmd), auquel cas l’étape de récupération peut prendre du temps à reproduire. Par ailleurs, les sites internet évoluent fréquemment dans le temps, il est donc préférable de “figer” les données une fois l’étape de récupération effectuée. De la même façon, même s’il ne s’agit pas de données source, vous pouvez vouloir entraîner des modèles et stocker leur version entraînée, car cette étape peut également être chronophage.\n",
    "\n",
    "Dans toutes ces situations, il est nécessaire de pouvoir stocker des données (ou des modèles). **Votre dépôt `Git` n’est pas le lieu adapté pour le stockage de fichiers volumineux**. Un projet `Python` bien construit est modulaire: il sépare le stockage du code (`Git`), d’éléments de configuration (par exemple des jetons d’API qui ne doivent pas être dans le code) et du stockage des données. Cette séparation conceptuelle entre code et données permet de meilleurs projets.\n",
    "\n",
    "Là où `Git` est fait pour stocker du code, on utilise des solutions adaptées pour le stockage de fichiers. De nombreuses solutions existent pour ce faire. Sur le SSP Cloud, on propose `MinIO`, une implémentation open-source du stockage `S3` présenté plus haut. Ce court tutoriel vise à présenter une utilisation standard dans le cadre de vos projets.\n",
    "\n",
    "> **Warning**\n",
    ">\n",
    "> Quelle que soit la solution de stockage retenue pour vos données/modèles, **le code ayant servir à produire ces objets doit impérativement figurer dans votre dépôt de projet**.\n",
    "\n",
    "### 3.4.1 Partager des fichiers sur le SSP Cloud\n",
    "\n",
    "Comme expliqué plus haut, on stocke les fichiers sur `S3` dans un bucket. Sur le SSP Cloud, un bucket est créé automatiquement lors de votre création de compte, avec le même nom que votre compte SSP Cloud. L’interface [Mes Fichiers](https://datalab.sspcloud.fr/my-files) vous permet d’y accéder de manière visuelle, d’y importer des fichiers, de les télécharger, etc.\n",
    "\n",
    "Dans ce tutoriel, nous allons plutôt y accéder de manière programmatique, via du code `Python`. Le package `s3fs` permet de requêter votre bucket à la manière d’un filesystem classique. Par exemple, vous pouvez lister les fichiers disponibles sur votre *bucket* avec la commande suivante :\n",
    "\n",
    "[1] Ce comportement est souvent rendu possible via des systèmes de fichiers virtuels ou des wrappers compatibles avec `pandas`, `pyarrow`, `duckdb`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
    "fs.ls(MY_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4bb5d-a69b-4cc2-a4e0-c8d7d9dd97fc",
   "metadata": {},
   "source": [
    "Si vous n’avez jamais ajouté de fichier sur MinIO, votre *bucket* est vide, cette commande devrait donc renvoyer une liste vide. On va donc ajouter un premier dossier pour voir la différence.\n",
    "\n",
    "Par défaut, un bucket vous est personnel, c’est à dire que les données qui s’y trouvent ne peuvent être lues ou modifiées que par vous. Dans le cadre de votre projet, vous aurez envie de partager ces fichiers avec les membres de votre groupe pour développer de manière collaborative. Mais pas seulement ! Il faudra également que vos correcteurs puissent accéder à ces fichiers pour reproduire vos analyses.\n",
    "\n",
    "Il existe différentes possibilités de rendre des fichiers plus ou moins publics sur `MinIO`. La plus simple, et celle que nous vous recommandons, est de créer un dossier `diffusion` à la racine de votre bucket. Sur le SSP Cloud, tous les fichiers qui se situent dans un dossier `diffusion` sont accessibles **en lecture** à l’ensemble des utilisateurs authentifiés. Utilisez l’interface [Mes Fichiers](https://datalab.sspcloud.fr/my-files) pour créer un dossier `diffusion` à la racine de votre *bucket*. Si tout a bien fonctionné, la commande `Python` ci-dessus devrait désormais afficher le chemin `mon_nom_utilisateur_sspcloud/diffusion`.\n",
    "\n",
    "> **Le stockage *cloud* favorise le travail collaboratif !**\n",
    ">\n",
    "> Plutôt que chaque membre du projet travaille avec ses propres fichiers sur son ordinateur, ce qui implique une synchronisation fréquente entre membres du groupe et limite la reproductibilité du fait des risques d’erreur, les fichiers sont mis sur un dépôt central, que chaque membre du groupe peut ensuite requêter.\n",
    ">\n",
    "> Pour cela, il faut simplement s’accorder au sein du groupe pour utiliser le bucket d’un des membres du projet, et s’assurer que les autres membres du groupe peuvent accéder aux données, en les mettant dans le dossier `diffusion` du bucket choisi.\n",
    "\n",
    "### 3.4.2 Récupération et stockage de données\n",
    "\n",
    "Maintenant que nous savons où mettre nos données sur `MinIO`, regardons comment le faire en pratique depuis `Python`.\n",
    "\n",
    "#### Cas d’un Dataframe\n",
    "\n",
    "Reprenons un exemple issu du cours sur les [API](../../content/manipulation/04c_API_TP.qmd#illustration-avec-une-api-de-lademe-pour-obtenir-des-diagnostics-énergétiques) pour simuler une étape de récupération de données coûteuse en temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_api = \"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=%2A&sampling=neighbors\"\n",
    "response_json = requests.get(url_api).json()\n",
    "df_dpe = pd.json_normalize(response_json[\"results\"])\n",
    "\n",
    "df_dpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27a826-0579-4c20-bcdc-7a7c9640db95",
   "metadata": {},
   "source": [
    "Cette requête nous permet de récupérer un *DataFrame* `Pandas`, dont les deux premières lignes sont imprimées ci-dessus. Dans notre cas le processus est volontairement simpliste, mais on peut imaginer que de nombreuses étapes de requêtage / préparation de la données sont nécessaires pour aboutir à un dataframe exploitable dans la suite du projet, et que ce processus est coûteux en temps. On va donc stocker ces données “intermédiaires” sur `MinIO` afin de pouvoir exécuter la suite du projet sans devoir refaire tourner tout le code qui les a produites.\n",
    "\n",
    "On peut utiliser les fonctions d’export de `Pandas`, qui permettent d’exporter dans différents formats de données. Vu qu’on est dans le cloud, une étape supplémentaire est nécessaire : on ouvre une connexion vers `MinIO`, puis on exporte notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
    "FILE_PATH_OUT_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.csv\"\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n",
    "    df_dpe.to_csv(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c7c649-dede-4435-b21c-65cbb3b78575",
   "metadata": {},
   "source": [
    "On peut vérifier que notre fichier a bien été uploadé via l’interface [Mes Fichiers](https://datalab.sspcloud.fr/my-files) ou bien directement en `Python` en interrogeant le contenu du dossier `diffusion` de notre *bucket* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41347de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{MY_BUCKET}/diffusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cee83-1046-4718-9497-6e84cfda4bae",
   "metadata": {},
   "source": [
    "On pourrait tout aussi simplement exporter notre *dataset* en `Parquet`, pour limiter l’espace de stockage et maximiser les performances à la lecture. Attention : vu que `Parquet` est un format compressé, il faut préciser qu’on écrit un fichier binaire : le mode d’ouverture du fichier passé à la fonction `fs.open` passe de `w` (`write`) à `wb` (`write binary`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_OUT_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.parquet\"\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, 'wb') as file_out:\n",
    "    df_dpe.to_parquet(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9fd3e-b957-44ea-96ae-9c62d5f482ad",
   "metadata": {},
   "source": [
    "#### Cas de fichiers\n",
    "\n",
    "Dans la partie précédente, on était dans le cas “simple” d’un dataframe, ce qui nous permettait d’utiliser directement les fonctions d’export de `Pandas`. Maintenant, imaginons qu’on ait plusieurs fichiers d’entrée, pouvant chacun avoir des formats différents. Un cas typique de tels fichiers sont les fichiers `ShapeFile`, qui sont des fichiers de données géographiques, et se présentent sous forme d’une combinaison de fichiers (cf. [chapitre sur GeoPandas](../../content/manipulation/03_geopandas_intro.qmd#le-format-shapefile-.shp-et-le-geopackage-.gpkg)). Commençons par récupérer un fichier `.shp` pour voir sa structure.\n",
    "\n",
    "On récupère ci-dessous les contours du département de la Réunion [produits par l’IGN](https://geoservices.ign.fr/telechargement-api/ADMIN-EXPRESS-COG-CARTO), sous la forme d’une archive `.7z` qu’on va décompresser en local dans un dossier `departements_fr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0fb755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADMIN-EXPRESS-COG-CARTO_3-2__SHP_RGR92UTM40S_REU_2023-05-03']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import requests\n",
    "import py7zr\n",
    "\n",
    "# Import et décompression\n",
    "contours_url = \"https://data.geopf.fr/telechargement/download/ADMIN-EXPRESS-COG-CARTO/ADMIN-EXPRESS-COG-CARTO_3-2__SHP_RGR92UTM40S_REU_2023-05-03/ADMIN-EXPRESS-COG-CARTO_3-2__SHP_RGR92UTM40S_REU_2023-05-03.7z\"\n",
    "response = requests.get(contours_url, stream=True)\n",
    "\n",
    "with py7zr.SevenZipFile(io.BytesIO(response.content), mode='r') as archive:\n",
    "    archive.extractall(path=\"departements_fr\")\n",
    "\n",
    "# Vérification du dossier (local, pas sur S3)\n",
    "os.listdir(\"departements_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937894d9-dd7e-474e-b841-5982b296766f",
   "metadata": {},
   "source": [
    "Vu qu’il s’agit cette fois de fichiers locaux et non d’un *dataframe* `Pandas`, on doit utiliser le package `s3fs` pour transférer les fichiers du filesystem local au filesystem distant (`MinIO`). Grâce à la commande `put`, on peut copier en une seule commande le dossier sur `MinIO`. Attention à bien spécifier le paramètre `recursive=True`, qui permet de copier à la fois un dossier et son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.put(\"departements_fr/\", f\"{MY_BUCKET}/diffusion/departements_fr/\", recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529fb83-858c-4596-b1e0-da71327271bb",
   "metadata": {},
   "source": [
    "Vérifions que le dossier a bien été copié :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{MY_BUCKET}/diffusion/departements_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446660b-ad5a-43b3-97c7-a35c57e5d905",
   "metadata": {},
   "source": [
    "Si le dossier, comme ici, contient des fichiers à plusieurs niveaux, il faudra parcourir la liste de manière récursive pour accéder aux fichiers. Si par exemple, on s’intéresse exclusivement aux découpages des communes, on pourra par exemple utiliser un [`glob`](https://docs.python.org/fr/3/library/glob.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec09ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.glob(f'{MY_BUCKET}/diffusion/departements_fr/**/COMMUNE.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cba16-2aa0-4a68-8a22-ac82b6eddfc5",
   "metadata": {},
   "source": [
    "Si tout a bien fonctionné, la commande ci-dessus devrait renvoyer une liste contenant les chemins sur `MinIO` des différents fichiers (`.shp`, `.shx`, `.prj`, etc.) constitutifs du `ShapeFile` des communes de la Réunion.\n",
    "\n",
    "### 3.4.3 Utilisation des données\n",
    "\n",
    "En sens inverse, pour récupérer les fichiers depuis `MinIO` dans une session `Python`, les commandes sont symétriques.\n",
    "\n",
    "#### Cas d’un dataframe\n",
    "\n",
    "Attention à bien passer cette fois le paramètre `r` (`read`, pour lecture) et non plus `w` (`write`, pour écriture) à la fonction `fs.open` afin de ne pas écraser le fichier !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
    "FILE_PATH_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.csv\"\n",
    "\n",
    "# Import\n",
    "with fs.open(FILE_PATH_S3, 'r') as file_in:\n",
    "    df_dpe = pd.read_csv(file_in)\n",
    "\n",
    "# Vérification\n",
    "df_dpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb4784-47d6-49f1-a995-34b34821d9da",
   "metadata": {},
   "source": [
    "De même, si le fichier est en `Parquet` (en n’oubliant pas de passer de `r` à `rb` pour tenir compte de la compression) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
    "FILE_PATH_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.parquet\"\n",
    "\n",
    "# Import\n",
    "with fs.open(FILE_PATH_S3, 'rb') as file_in:\n",
    "    df_dpe = pd.read_parquet(file_in)\n",
    "\n",
    "# Vérification\n",
    "df_dpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211228b-0fc2-4d45-ae61-41ed0da7c569",
   "metadata": {},
   "source": [
    "#### Cas de fichiers\n",
    "\n",
    "Dans le cas de fichiers, on va devoir dans un premier temps rapatrier les fichiers de `MinIO` vers la machine local (en l’occurence, le service ouvert sur le SSP Cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des fichiers depuis MinIO vers la machine locale\n",
    "fs.get(f\"{MY_BUCKET}/diffusion/departements_fr/\", \"departements_fr/\", recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511c2c5-2717-477f-924a-ce79e1b4ae8b",
   "metadata": {},
   "source": [
    "Puis on les importe classiquement depuis `Python` avec le *package* approprié. Dans le cas des `ShapeFile`, où les différents fichiers sont en fait des parties d’un seul et même fichier, une seule commande permet de les importer après les avoir rappatriés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d56b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "localpath = Path(\"departements_fr\")\n",
    "\n",
    "\n",
    "df_dep = gpd.read_file(\n",
    "  list(\n",
    "    localpath.glob(\"**/COMMUNE.shp\")\n",
    "  )[0]\n",
    ")\n",
    "df_dep.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7275a71-26ab-46b5-b8f4-c5991806512a",
   "metadata": {},
   "source": [
    "## 3.5 Pour aller plus loin\n",
    "\n",
    "-   [La documentation sur MinIO du SSPCloud](https://docs.sspcloud.fr/content/storage.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
