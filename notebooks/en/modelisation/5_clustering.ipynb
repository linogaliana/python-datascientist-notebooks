{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2055b21-7234-416a-8905-04c5544bbf22",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Lino Galiana  \n",
    "2025-12-26\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/modelisation/5_clustering.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«5_clustering»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/modelisation%205_clustering»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«5_clustering»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/modelisation%205_clustering»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/modelisation/5_clustering.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "Machine learning materials in this course uses a unique dataset, presented in the [introduction](index.qmd). All examples are based on US county level presidential election results combined with sociodemographic variables. Source code for data ingestion is available on [`Github`](https://github.com/linogaliana/python-datascientist/blob/main/content/modelisation/get_data.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas openpyxl plotnine plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/main/content/modelisation/get_data.py'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('getdata.py', 'wb').write(r.content)\n",
    "\n",
    "import getdata\n",
    "votes = getdata.create_votes_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784a656-8d05-4887-9e6e-6fdf7e2dcb83",
   "metadata": {},
   "source": [
    "It can also be useful to install `plotnine`\n",
    "to easily create visualizations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e6dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotnine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b137a09-68bb-4e63-b0fd-92412ee40bc3",
   "metadata": {},
   "source": [
    "# 1. Introduction to *Clustering*\n",
    "\n",
    "Until now, we have engaged in supervised learning because we knew the true value of the variable to be explained/predicted (`y`). This is no longer the case with unsupervised learning.\n",
    "\n",
    "*Clustering* is a field of application within unsupervised learning.\n",
    "It involves leveraging available information by grouping observations\n",
    "that are similar based on their common characteristics (*features*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a90c3d-c46b-4410-b3c1-ec920946a067",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "\n",
    "Reminder: The Decision Tree of `Scikit` Methods\n",
    "\n",
    "</summary>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a4/Scikit-learn_machine_learning_decision_tree.png)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828d2d5-5ada-4539-98a7-38f1f3123fe4",
   "metadata": {},
   "source": [
    "The objective is to create groups of observations (*clusters*) where:\n",
    "\n",
    "-   Within each cluster, the observations are homogeneous (minimal intra-cluster variance);\n",
    "-   Clusters have heterogeneous profiles, meaning they are distinct from one another (maximal inter-cluster variance).\n",
    "\n",
    "In *Machine Learning*, clustering methods are widely used for\n",
    "recommendation systems. For example, by creating homogeneous groups of\n",
    "consumers, it becomes easier to identify and target behaviors specific to each consumer group.\n",
    "\n",
    "These methods also have applications in economics and social sciences as they enable\n",
    "grouping observations without prior assumptions, thereby interpreting a variable\n",
    "of interest in light of these results. This [publication on spatial segregation using mobile phone data](https://www.insee.fr/fr/statistiques/4925200)\n",
    "is an example of this approach.\n",
    "In some databases, there may be a few labeled examples while most are\n",
    "unlabeled. The labels might have been manually created by experts.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Note\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Clustering methods can also be used upstream of a classification problem (in\n",
    "semi-supervised learning problems).\n",
    "The book *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow* (Géron 2022) provides\n",
    "examples in the chapter dedicated to unsupervised learning.\n",
    "\n",
    "For instance, suppose that in the [MNIST dataset of handwritten digits](https://en.wikipedia.org/wiki/MNIST_database), the digits are unlabeled,\n",
    "and we want to determine the best strategy for labeling this dataset.\n",
    "One could randomly look at handwritten digit images in the dataset and label them.\n",
    "However, the book’s authors demonstrate a better strategy.\n",
    "It is better to apply a clustering algorithm beforehand to group the images together and have a\n",
    "representative image per group, then label these representative images instead of labeling randomly.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "There are numerous clustering methods.\n",
    "We will focus on the most intuitive one: *k-means*.\n",
    "\n",
    "# 2. k-means\n",
    "\n",
    "## 2.1 Principle\n",
    "\n",
    "The objective of *k-means* is to partition the observation space by finding points (*centroids*) that act as centers of gravity around which nearby observations can be grouped into homogeneous classes.\n",
    "The *k-means* algorithm works iteratively, initializing the centroids and then updating them at each iteration until the centroids stabilize. Here are some examples of *clusters* resulting from the *k-means* method:\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Tip\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "The objective of *k-means* is to find a partition of the data $S=\\{S_1,...,S_K\\}$ such that\n",
    "$$\n",
    "\\arg\\min_{S} \\sum_{i=1}^K \\sum_{x \\in S_i} ||x - \\mu_i||^2\n",
    "$$\n",
    "where $\\mu_i$ is the mean of $x_i$ in the set of points $S_i$.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "In this chapter, we will primarily use `Scikit`. However, here is a suggested import of packages to save time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c428a-c9b7-4b1c-9172-928a4f4a12ad",
   "metadata": {},
   "source": [
    "Dans le prochain exercice, nous allons utiliser les variables suivantes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd66e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement de la base restreinte.\n",
    "xvars = [\n",
    "  'Unemployment_rate_2019', 'Median_Household_Income_2021',\n",
    "  'Percent of adults with less than a high school diploma, 2018-22',\n",
    "  \"Percent of adults with a bachelor's degree or higher, 2018-22\"\n",
    "]\n",
    "\n",
    "votes = votes.dropna(subset = xvars + [\"per_gop\"])\n",
    "\n",
    "df2 = votes.loc[:, xvars + [\"per_gop\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5fe2a-04bb-4864-8995-b98cb610bfd1",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercise 1: Principle of k-means\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  Perform a *k-means* with $k=4$.\n",
    "2.  Create a `label` variable in `votes` to store the typology results.\n",
    "3.  Display this typology on a map.\n",
    "4.  Choose the variables `Median_Household_Income_2021` and `Unemployment_rate_2019` and represent the scatter plot, coloring it differently based on the obtained label. What is the problem?\n",
    "5.  Repeat questions 2 to 5, standardizing the variables beforehand.\n",
    "6.  Represent the distribution of the vote for each *cluster*.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "The map obtained in question 4, which allows us to\n",
    "spatially represent our groups, is\n",
    "as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676db1f9-0fee-4e9b-9b3b-1d502038f565",
   "metadata": {},
   "source": [
    "The scatter plot from question 5, representing\n",
    "the relationship between `Median_Household_Income_2021`\n",
    "and `Unemployment_rate_2019`, will have the following appearance:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d5044-bf20-4d2b-9872-f47d6b63ca0a",
   "metadata": {},
   "source": [
    "The classification appears too distinct in this figure.\n",
    "This suggests that the income variable (`Median_Household_Income_2021`)\n",
    "explains the partitioning produced by our model too well to be normal.\n",
    "This is likely due to the high variance of income compared to other variables.\n",
    "In such situations, as mentioned earlier, it is recommended to standardize the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25d57d-d6aa-45f4-94f6-39bb9939d25e",
   "metadata": {},
   "source": [
    "Thus, the following map is obtained in question 5:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196cab5-466f-4cc5-9140-58880c65618a",
   "metadata": {},
   "source": [
    "And the scatter plot from question 5 has a less deterministic appearance, which is preferable:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0da6a-1473-41fa-b52d-25376f8ea4ce",
   "metadata": {},
   "source": [
    "Finally, regarding question 6, the following histogram of votes for each cluster is obtained:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5755f5-6ca2-4ecc-9646-24bfa9f655b7",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Tip\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "Several points about the algorithm implemented by default in `scikit-learn` should be noted, as outlined in\n",
    "the documentation:\n",
    "\n",
    "-   The default algorithm is *kmeans++* (see the `init` parameter). This means that\n",
    "    the initialization of centroids is done intelligently so that the initial centroids are chosen\n",
    "    to avoid being too close to each other.\n",
    "-   The algorithm will start with `n_init` different centroids, and the model will select the best initialization\n",
    "    based on the model’s *inertia*, with a default value of 10.\n",
    "\n",
    "The model outputs the `cluster_centers_`, the labels `labels_`, the inertia `inertia_`, and the number of iterations\n",
    "`n_iter_`.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "## 2.2 Choosing the number of clusters\n",
    "\n",
    "Up to now, we have taken the number of clusters as given, as if there were a legitimate reason to think that we need 4 rather than 7 voting profiles.\n",
    "\n",
    "Like any (hyper)parameter in a machine learning approach, we may want to vary its value and, in the absence of a theory to decide, pick the least bad empirical choice.\n",
    "\n",
    "There is a trade-off between bias and variance: too large a number of clusters implies very low within-cluster variance, which is typical of overfitting, even though it is never possible to determine the true type of an observation since we are in unsupervised learning.\n",
    "\n",
    "Without prior knowledge of the number of clusters, we can rely on two families of methods:\n",
    "\n",
    "-   **Elbow method**: We take the inflection point of the model performance curve. This corresponds to the moment when adding an additional cluster, which increases model complexity, brings only modest gains in modeling the data.\n",
    "\n",
    "-   **Silhouette score**: We measure the similarity between a point and the other points in its cluster relative to other clusters, and choose the model that best separates clusters (see <a href=\"#tip-silhouette-en\" class=\"quarto-xref\">Tip 2.1</a>).  \n",
    "\n",
    "> **Tip 2.1**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Tip\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> > Silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object iswell matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters\n",
    "> >\n",
    "> > Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    ">\n",
    "> The silhouette score is therefore a measure of the trade-off between **cluster cohesion** (to what extent observations within a cluster are homogeneous) and **cluster separation** (to what extent clusters are distinct from one another).\n",
    ">\n",
    "> For each observation $i$, the silhouette of the point is:\n",
    ">\n",
    "> $$\n",
    "> s(i) = \\frac{b(i)-a(i)}{\\max(a(i),b(i))}\n",
    "> $$\n",
    ">\n",
    "> where $a(i)$ is the average distance between $i$ and the other points in its own cluster (a measure of cohesion) and $b(i)$ is the smallest average distance between $i$ and the points of another cluster (a measure of separation).\n",
    ">\n",
    "> The value (s(i)) lies between **-1** and **1**:\n",
    ">\n",
    "> -   **$s(i) \\approx 1$** : $a(i) \\ll b(i)$  \n",
    ">     The point is well assigned to its cluster: it is close to the points in its cluster and far from the others.\n",
    ">\n",
    "> -   **$s(i) \\approx 0$** : $a(i) \\approx b(i)$  \n",
    ">     The point lies on the boundary between two clusters: separation is weak locally.\n",
    ">\n",
    "> -   **$s(i) < 0$** : $a(i) > b(i)$\n",
    ">     The point is probably misassigned: on average, it is closer to another cluster than to its own.\n",
    ">\n",
    "> The silhouette score is the average of the points’ silhouette values.\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Exercise: determine the optimal number of clusters using the elbow method\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "1.  Evaluate inertia and distortion by varying the number of *clusters* (from 1 to 9).\n",
    "2.  Plot the results and interpret them.\n",
    "\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = [\n",
    "  'Unemployment_rate_2019', 'Median_Household_Income_2021',\n",
    "  'Percent of adults with less than a high school diploma, 2018-22',\n",
    "  \"Percent of adults with a bachelor's degree or higher, 2018-22\"\n",
    "]\n",
    "\n",
    "df2 = votes.loc[:, xvars + [\"per_gop\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eda880-76e9-4cef-8229-c99cdc5cf582",
   "metadata": {},
   "source": [
    "## 2.3 Other Clustering Methods\n",
    "\n",
    "There are many other clustering methods. Among the most well-known, here are three notable examples:\n",
    "\n",
    "-   Hierarchical Agglomerative Clustering (HAC);\n",
    "-   DBSCAN;\n",
    "-   Gaussian Mixture Models.\n",
    "\n",
    "### 2.3.1 Hierarchical Agglomerative Clustering (HAC)\n",
    "\n",
    "What is the principle?\n",
    "\n",
    "-   Start by calculating the dissimilarity between our *N* individuals, i.e., their pairwise distances in the variable space.\n",
    "-   Then, group the two individuals whose grouping minimizes a given aggregation criterion, thus creating a class containing these two individuals.\n",
    "-   Next, calculate the dissimilarity between this class and the *N-2* other individuals using the aggregation criterion.\n",
    "-   Then, group the two individuals or classes of individuals whose grouping minimizes the aggregation criterion.\n",
    "-   Continue until all individuals are grouped.\n",
    "\n",
    "These successive groupings produce a binary classification tree (dendrogram), where the root corresponds to the class grouping all individuals. This dendrogram represents a hierarchy of partitions. A partition can be chosen by truncating the tree at a certain level, based on either user constraints or more objective criteria.\n",
    "\n",
    "More information [here](https://www.xlstat.com/en/solutions/features/hierarchical-clustering-hac).\n",
    "\n",
    "### 2.3.2 DBSCAN\n",
    "\n",
    "The [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN) is implemented in `sklearn.cluster`.\n",
    "It can be used notably for anomaly detection.\n",
    "This method is based on clustering in regions of continuous observation density using the concept of neighborhood within a certain epsilon distance.\n",
    "For each observation, it is checked whether there are neighbors within its epsilon-distance neighborhood. If there are at\n",
    "least `min_samples` neighbors, the observation will be a *core instance*.\n",
    "\n",
    "Observations that are not *core instances* and have no *core instances* in their epsilon-distance neighborhood\n",
    "will be detected as anomalies.\n",
    "\n",
    "### 2.3.3 Gaussian Mixture Models\n",
    "\n",
    "For theoretical insights, see the course [Probabilistic and Computational Statistics, M1 Jussieu, V.Lemaire and T.Rebafka](https://perso.lpsm.paris/~rebafka/#enseignement).\n",
    "Refer especially to the notebooks for the EM algorithm for Gaussian mixture models.\n",
    "\n",
    "In `sklearn`, Gaussian mixture models are implemented in `sklearn.mixture` as `GaussianMixture`.\n",
    "Key parameters include the number of Gaussians `n_components` and the number of initializations `n_init`.\n",
    "Gaussian mixture models can also be used for anomaly detection.\n",
    "\n",
    "\n",
    "<div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "<div class=\"callout-header d-flex align-content-center\">\n",
    "<div class=\"callout-icon-container\">\n",
    "<i class=\"callout-icon\"></i>\n",
    "</div>\n",
    "<div class=\"callout-title-container flex-fill\">\n",
    "Going Further\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"callout-body-container callout-body\">\n",
    "\n",
    "There are many other clustering methods:\n",
    "\n",
    "-   Local Outlier Factor;\n",
    "-   Bayesian Gaussian Mixture Models;\n",
    "-   Other Hierarchical Clustering Methods;\n",
    "-   Etc.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "# 3. Principal Component Analysis (PCA)\n",
    "\n",
    "## 3.1 For Cluster Visualization\n",
    "\n",
    "The simplest method to visualize *clusters*, regardless of how they were obtained, would be to represent each individual in the *N*-dimensional space of the table’s variables, coloring each individual based on their cluster.\n",
    "This would clearly differentiate the most discriminating variables and the various groups.\n",
    "One issue here: as soon as *N \\> 3*, it becomes difficult to represent the results intelligibly…\n",
    "\n",
    "This is where **Principal Component Analysis** ([PCA](https://www.xlstat.com/en/solutions/features/principal-component-analysis-pca)) comes into play, allowing us to project our high-dimensional space into a smaller-dimensional space.\n",
    "The major constraint of this projection is to retain the maximum amount of information (measured by the total variance of the dataset) within our reduced number of dimensions, called principal components.\n",
    "By limiting to 2 or 3 dimensions, we can visually represent relationships between observations with minimal loss of reliability.\n",
    "\n",
    "We can generally expect that clusters determined in our N-dimensional space will differentiate well in our PCA projection and that the composition of the principal components based on the initial variables will help interpret the obtained clusters.\n",
    "Indeed, the linear combination of columns creating our new axes often has “meaning” in the real world:\n",
    "\n",
    "-   Either because a small handful of variables represent the majority of the component;\n",
    "-   Or because most columns contributing to the summed component combine well to form a natural interpretation.\n",
    "\n",
    "To practice cluster creation methods, from raw data to PCA visualization, refer to part 2 of subject 3 in the 2023 funathon, *Explore the eating habits of our compatriots*, on [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) or on [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).\n",
    "\n",
    "## 3.2 For Dimensionality Reduction\n",
    "\n",
    "PCA is also very useful in reducing the number of variables for many types of modeling, such as linear regression.\n",
    "It is possible to project the space of explanatory variables into a lower-dimensional space, specifically to limit the risks of *overfitting*.\n",
    "\n",
    "The drawback of this approach is that it makes the data used as input for the model less interpretable compared to LASSO, as the latter selects variables, while PCA selects linear combinations of our variables.\n",
    "\n",
    "## 3.3 Example\n",
    "\n",
    "Let us revisit our previous data. Before performing a principal component analysis, whose objective is to synthesize sources of variability in our data, it is advisable to standardize the variables when they have different scales (which is the case here).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d8ae0-6775-48f4-be24-369c779887a2",
   "metadata": {},
   "source": [
    "Let us start with a preliminary test by reducing our data to two components, that is, two linear combinations of the data. This is a very practical method implemented in `Scikit`. Doing it manually would be cumbersome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835af24-a944-454c-b2fa-17f080d33fd0",
   "metadata": {},
   "source": [
    "Géron, Aurélien. 2022. *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*. \" O’Reilly Media, Inc.\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}