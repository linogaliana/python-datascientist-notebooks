{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d2a7cb-e3f8-438c-9dd7-53fc0ac469d8",
   "metadata": {},
   "source": [
    "# New ways of accessing data: Parquet format and data in the cloud\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-07\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "In the previous chapters, we explored how to retrieve and harmonize data from various source: CSV files, APIs, web scraping, and more. However, no overview of data access methods would be complete without mentioning a relatively recent addition to the data ecosystem: the `Parquet` data format.\n",
    "\n",
    "Thanks to its technical advantages - specifically designed for analytical workloads - and its seamless integration with Python, `Parquet` is becoming increasingly essential. It has, in fact, become a key component of modern cloud infrastructures, which have emerged since the mid-2010s as the standard environment for data science workflows.[1]\n",
    "\n",
    "> **Objectives**\n",
    ">\n",
    "> -   Understand the challenges involved in storing and processing different types of data formats  \n",
    "> -   Distinguish between file-based storage and database systems  \n",
    "> -   Discover `Parquet` format and its advantages over flat files and proprietary formats  \n",
    "> -   Learn how to work with `Parquet` data using `Arrow` and `DuckDB`  \n",
    "> -   Explore the implications of cloud-based storage and how `Python` can adapt to modern data infrastructures\n",
    "\n",
    "# 1. Contextual Elements\n",
    "\n",
    "## 1.1 Principles of Data Storage\n",
    "\n",
    "Before exploring the advantages of the `Parquet` format, it is helpful to briefly review how data is stored and made accessible to a processing language like `Python`[2].\n",
    "\n",
    "Two main approaches coexist: **file-based storage** and **relational database storage**. The key distinction between these paradigms lies in how access to data is structured and managed.\n",
    "\n",
    "## 1.2 File-Based Storage\n",
    "\n",
    "### 1.2.1 Flat Files\n",
    "\n",
    "In a flat file, data is organized in a linear fashion, with values typically separated by a delimiter such as a comma, semicolon, or tab. Here’s a simple example using a `.csv` file:\n",
    "\n",
    "``` raw\n",
    "nom ; profession \n",
    "Astérix ; \n",
    "Obélix ; Tailleur de menhir ;\n",
    "Assurancetourix ; Barde\n",
    "```\n",
    "\n",
    "`Python` can easily structure this information:\n",
    "\n",
    "[1] For more details, see the [production deployment course by Romain Avouac and myself](https://ensae-reproductibilite.github.io/website/chapters/big-data.html). Apologies to non-French-speaking readers—this resource is currently only available in French.\n",
    "\n",
    "[2] For a deeper discussion of data format selection challenges, see @dondon2023quels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c495af8-9c2b-4b15-a550-0d154b3c55f8",
   "metadata": {},
   "source": [
    "1.  `StringIO` permet de traiter la chaîne de caractère comme le contenu d’un fichier.\n",
    "\n",
    "These are referred to as **flat files** because all the records related to a given observation are stored sequentially, without any hierarchical structure.\n",
    "\n",
    "### 1.2.2 Hierarchical files\n",
    "\n",
    "Other formats, such as `JSON`, structure data in a hierarchical way:\n",
    "\n",
    "``` json\n",
    "[\n",
    "  {\n",
    "    \"nom\": \"Astérix\"\n",
    "  },\n",
    "  {\n",
    "    \"nom\": \"Obélix\",\n",
    "    \"profession\": \"Tailleur de menhir\"\n",
    "  },\n",
    "  {\n",
    "    \"nom\": \"Assurancetourix\",\n",
    "    \"profession\": \"Barde\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "In such cases, when information is missing - as in the line for *“Astérix”* - you don’t see two delimiters side by side. Instead, the missing data is simply left out.\n",
    "\n",
    "> **Caution**\n",
    ">\n",
    "> The difference between a `.csv` file and a `JSON` file lies not only in their format but also in their underlying logic of data storage.\n",
    ">\n",
    "> `JSON` is a non-tabular format that offers greater flexibility: it allows the structure of the data to evolve over time without requiring previous entries to be modified or recompiled. This makes it particularly well-suited for dynamic data collection environments, such as APIs.\n",
    ">\n",
    "> For instance, a website that begins collecting a new piece of information doesn’t need to retroactively update all prior records. It can simply add the new field to the relevant entries—omitting it from others where the information wasn’t collected.\n",
    ">\n",
    "> It is then up to the query tool—such as `Python` or another platform—to reconcile and link data across these differing structures.\n",
    ">\n",
    "> This flexible approach underpins many `NoSQL` databases (like `ElasticSearch`), which play a central role in the *big data* ecosystem.\n",
    "\n",
    "### 1.2.3 Data split across multiple files\n",
    "\n",
    "It is common for a single observation to be distributed across multiple files in different formats. In geomatics, for example, geographic boundaries are often stored separately from the data that gives them context:\n",
    "\n",
    "-   In some cases, everything is bundled into a single file that contains both the geographic shapes and the associated attribute values. This is the approach used by formats like `GeoJSON`;\n",
    "-   In other cases, the data is split across several files, and reading the full information - geographic shapes, data for each zone, projection system, etc. - requires combining them. This is the case with the `Shapefile` format.\n",
    "\n",
    "When data is distributed across multiple files, this is up to the processing tool (e.g., `Python`) to perform the necessary joins and link the information together.\n",
    "\n",
    "### 1.2.4 The role of the *file system*\n",
    "\n",
    "The **file system** enables the computer to locate files physically on the disk. It plays a central role in file management, handling file naming, folder hierarchy, and access permissions.\n",
    "\n",
    "## 1.3 Storing Data in a Database\n",
    "\n",
    "The logic behind databases differs fundamentally and is more systematic in nature. A **relational database** is managed by a **Database Management System (DBMS)**, which provides capabilities for:\n",
    "\n",
    "-   storing coherent datasets;\n",
    "-   performing updates (insertions, deletions, modifications);\n",
    "-   controlling access (user permissions, query types, etc.).\n",
    "\n",
    "Data is organized into **tables** connected by **relationships**, often structured according to a **star schema**:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\" alt=\"Source: Databricks Documentation\" />\n",
    "<figcaption aria-hidden=\"true\">Source: <a href=\"https://www.databricks.com/fr/glossary/star-schema\">Databricks Documentation</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "The software managing the database links these tables together using `SQL` queries. One of the most powerful and widely used systems for this purpose is [`PostgreSQL`](https://www.postgresql.org/).\n",
    "\n",
    "`Python` can interact with databases by issuing SQL queries. Historically, packages such as [`sqlalchemy`](https://www.sqlalchemy.org/) and [`psycopg2`](https://www.psycopg.org/docs/) have been standard tools for communicating with `PostgreSQL` databases, enabling both reading and updating operations. More recently, [`DuckDB`](https://duckdb.org/) has emerged as a lightweight and user-friendly alternative for querying relational data, and it will be discussed again in the context of the `Parquet` format.\n",
    "\n",
    "### Why File-Based Storage is gaining popularity\n",
    "\n",
    "The increasing popularity of file-based storage in the data science ecosystem is due to a number of technical and practical advantages that make it well-suited for modern analytical workflows.\n",
    "\n",
    "Files are generally more lightweight and easier to manage than databases. They do not require the installation or maintenance of specialized software; a basic file system, available on every operating system, is sufficient for accessing them.\n",
    "\n",
    "Reading a file in `Python` simply involves using a library such as `Pandas`. In contrast, interacting with a database typically requires:\n",
    "\n",
    "-   installing and configuring a DBMS (e.g., `PostgreSQL`, `MySQL`);\n",
    "-   managing network connections;\n",
    "-   relying on libraries such as `sqlalchemy` or `psycopg2`.\n",
    "\n",
    "This additional complexity makes file-based workflows more flexible and faster for exploratory tasks.\n",
    "\n",
    "However, this simplicity comes with limitations. File-based systems generally lack fine-grained access control. For instance, preventing a user from modifying or deleting a file is difficult without duplicating it and working from a copy. This represents a constraint in multi-user environments, although cloud-based storage solutions — particularly `S3` technology, which will be addressed later — offer effective remedies.\n",
    "\n",
    "The primary reason why files are often preferred over DBMSs lies in the nature of the operations being performed. Relational databases are particularly well-suited for contexts involving frequent updates or complex operations on structured data — a typical **application logic**, in which data is continuously evolving (through insertions, updates, or deletions).\n",
    "\n",
    "By contrast, in **analytical** contexts, the focus is on reading and temporarily manipulating data without altering the original source. The objective is to query, aggregate, and filter — not to persist changes. In such scenarios, files (especially when stored in optimized formats like `Parquet`) are ideal: they offer fast read times, high portability, and eliminate the overhead associated with running a full database engine.\n",
    "\n",
    "# 2. The `Parquet` format\n",
    "\n",
    "The `CSV` format has long been popular for its simplicity:\n",
    "\n",
    "-   It is **human-readable** (any text editor can open it);\n",
    "-   It relies on a **simple tabular structure**, well-suited for many analytical situations;\n",
    "-   It is **universal** and interoperable, as it is not tied to any specific software.\n",
    "\n",
    "However, this simplicity comes at a cost. Several limitations of the `CSV` format have led to the emergence of more efficient formats for data analysis, such as `Parquet`.\n",
    "\n",
    "## 2.1 Limitations of the `CSV` format\n",
    "\n",
    "CSV is a **heavy** format:\n",
    "\n",
    "-   It is not compressed, increasing its disk size;\n",
    "-   All data is stored as raw text. Data type optimization (integer, float, string, etc.) is deferred to the importing library (like `Pandas`), which must **scan the data** at load time—slowing down performance and increasing error risk.\n",
    "\n",
    "CSV is **row-oriented**:\n",
    "\n",
    "-   To access a specific column, every **row must be read**, and the relevant column extracted;\n",
    "-   This model performs poorly when only a **subset of columns** is needed—a common case in data science.\n",
    "\n",
    "CSV is **expensive to modify**:\n",
    "\n",
    "-   Adding a column or inserting intermediate data requires **rewriting the entire file**. For example, adding a `hair` column would mean generating a new version of the file:\n",
    "\n",
    "    ``` raw\n",
    "    name ; hair ; profession\n",
    "    Asterix ; blond ; \n",
    "    Obelix ; redhead ; Menhir sculptor\n",
    "    Assurancetourix ; blond ; Bard\n",
    "    ```\n",
    "\n",
    "> **About proprietary formats**\n",
    ">\n",
    "> Most data science tools offer their own serialization formats:\n",
    ">\n",
    "> -   `.pickle` for `Python`,  \n",
    "> -   `.rda` or `.RData` for `R`,  \n",
    "> -   `.dta` for `Stata`,  \n",
    "> -   `.sas7bdat` for `SAS`.\n",
    ">\n",
    "> However, these formats are **proprietary** or **tightly coupled to one language**, which raises interoperability issues. For example, `Python` cannot natively read `.sas7bdat`. Even when third-party libraries exist, the lack of official documentation makes support unreliable.\n",
    ">\n",
    "> In this regard, despite its limitations, `.csv` remains popular for its universality. But the `Parquet` format offers both this portability and significantly better performance.\n",
    "\n",
    "## 2.2 The Rise of the `Parquet` Format\n",
    "\n",
    "To address these limitations, the `Parquet` format—developed as an [Apache open-source project](https://parquet.apache.org/) offers a radically different approach.\n",
    "\n",
    "Its key characteristic: it is **column-oriented**. Unlike CSVs, data for each column is stored **separately**. This allows:\n",
    "\n",
    "-   loading only the columns relevant for analysis;\n",
    "-   more efficient data compression;\n",
    "-   significantly faster selective queries.\n",
    "\n",
    "Here’s a diagram from the [Upsolver blog](https://www.upsolver.com/blog/apache-parquet-why-use) illustrating the difference between row-based and columnar storage:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://www.upsolver.com/wp-content/uploads/2020/05/Screen-Shot-2020-05-26-at-17.52.58.png\" alt=\"Parquet vs CSV\" />\n",
    "<figcaption aria-hidden=\"true\">Parquet vs CSV</figcaption>\n",
    "</figure>\n",
    "\n",
    "In our example, you could read the `profession` column without parsing names, making access faster (ignore the `pyarrow.Table` element—we’ll return to it later):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e30221-f231-4bde-8d51-eaf56e23c1fd",
   "metadata": {},
   "source": [
    "Thanks to the column-oriented structure, it is possible to read only a single variable (such as `profession`) without having to scan every row in the file.\n",
    "\n",
    "``` raw\n",
    "path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        └── gender=female\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "```\n",
    "\n",
    "When read, the entire dataset is reconstructed into a tabular format:\n",
    "\n",
    "``` raw\n",
    "root\n",
    "|-- name: string (nullable = true)\n",
    "|-- age: long (nullable = true)\n",
    "|-- gender: string (nullable = true)\n",
    "|-- country: string (nullable = true)\n",
    "```\n",
    "\n",
    "## 2.3 A format designed for analysis — not just big data\n",
    "\n",
    "As emphasized in the Upsolver blog:\n",
    "\n",
    "> *Complex data such as logs and event streams would need to be represented as a table with hundreds or thousands of columns, and many millions of rows. Storing this table in a row-based format such as CSV would mean:*\n",
    ">\n",
    "> -   *Queries will take longer to run since more data needs to be scanned…*\n",
    "> -   *Storage will be more costly since CSVs are not compressed as efficiently as Parquet*\n",
    "\n",
    "However, the `Parquet` format **is not limited to *big data* architectures**. Its advantages are accessible to anyone who produces or works with datasets, regardless of scale:\n",
    "\n",
    "-   significantly reduced file sizes;\n",
    "-   fast, reliable, and memory-efficient data import."
   ]
  },
  {
   "cell_type": "raw",
   "id": "20df67d6-a95e-49ca-8213-81002ef8f307",
   "metadata": {},
   "source": [
    "<!---\n",
    "\n",
    "fin traduction pour le moment\n",
    "\n",
    "---->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccf31f-94af-4a56-9bc8-1303ce0fde68",
   "metadata": {},
   "source": [
    "The translation of this chapter is still in progress. The remaining sections will be available soon.\n",
    "\n",
    "## 2.4 Reading a `Parquet` file in `Python`: example\n",
    "\n",
    "There are many libraries that integrate well with the `Parquet` format, but the two most important to know are `PyArrow` and `DuckDB`. These libraries were previously mentioned as alternatives to `Pandas` for handling larger-than-memory datasets. They are often used to perform heavy initial operations before converting the results into a lightweight `pd.DataFrame` for further analysis.\n",
    "\n",
    "The [`PyArrow`](https://arrow.apache.org/docs/python/) library enables efficient reading and writing of `Parquet` files by taking advantage of their columnar structure[1]. It operates on a `pyarrow.Table` object, which—after processing—can be converted to a `Pandas` `DataFrame` to leverage the broader capabilities of the `Pandas` ecosystem.\n",
    "\n",
    "The [`DuckDB`](https://duckdb.org/docs/api/python/) library allows you to query `Parquet` files directly using `SQL`, without loading the entire file into memory. In essence, it brings the database philosophy (structured queries via SQL) to file-based storage. The results of such queries can also be converted into a `Pandas` `DataFrame`, combining the convenience of `Pandas` with the efficiency of an embedded SQL engine.\n",
    "\n",
    "A lesser-known but valuable feature of `DuckDB` is its ability to perform SQL queries directly on a `Pandas` `DataFrame`. This can be especially useful when `Pandas` syntax becomes verbose or cumbersome—for example, when computing a new column based on grouped statistics, where SQL expressions can be more concise and readable.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> Using `pa` for `pyarrow` and `pq` for `pyarrow.parquet` is a widely adopted convention, much like using `pd` for `pandas`.\n",
    "\n",
    "To demonstrate these features, we will use a dataset derived from the synthetic census data published by Insee, the French national statistics agency.\n",
    "\n",
    "[1] It is recommended to regularly\n",
    "consult official `pyarrow` documentation on [reading and writing files](https://arrow.apache.org/docs/python/parquet.html) and [data manipulation](https://arrow.apache.org/cookbook/py/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd25484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Example Parquet\n",
    "url = \"https://minio.lab.sspcloud.fr/projet-formation/bonnes-pratiques/data/RPindividus/REGION=93/part-0.parquet\"\n",
    "\n",
    "# Télécharger le fichier et l'enregistrer en local\n",
    "with open(\"example.parquet\", \"wb\") as f:\n",
    "    response = requests.get(url)\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c8a72-0ef4-4303-9794-0657eadcda22",
   "metadata": {},
   "source": [
    "## `Arrow`\n",
    "\n",
    "To fully benefit from the optimizations provided by the `Parquet` format, it is recommended to use `pyarrow.dataset`. This approach makes it possible to take full advantage of the performance gains offered by the combination of `Parquet` and `Arrow`, which are not always accessible when reading `Parquet` files using other methods available in the `Arrow` ecosystem (as will be explored in upcoming exercises).\n",
    "\n",
    "``` python\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\n",
    "  \"example.parquet\"\n",
    ").scanner(columns = [\"AGED\", \"IPONDI\", \"DEPT\"])\n",
    "table = dataset.to_table()\n",
    "table\n",
    "```\n",
    "\n",
    "    pyarrow.Table\n",
    "    AGED: int32\n",
    "    IPONDI: double\n",
    "    DEPT: dictionary<values=string, indices=int32, ordered=0>\n",
    "    ----\n",
    "    AGED: [[9,12,40,70,52,...,29,66,72,75,77],[46,76,46,32,2,...,7,5,37,29,4],...,[67,37,45,56,75,...,64,37,47,20,18],[16,25,51,6,11,...,93,90,92,21,65]]\n",
    "    IPONDI: [[2.73018871840726,2.73018871840726,2.73018871840726,0.954760150327854,3.75907197064638,...,3.27143319621654,4.83980378599556,4.83980378599556,4.83980378599556,4.83980378599556],[3.02627578376137,3.01215358930406,3.01215358930406,2.93136309038958,2.93136309038958,...,2.96848755763453,2.96848755763453,3.25812879950072,3.25812879950072,1.12514509319438],...,[2.57931132917563,2.85579410739065,0.845993555838931,2.50296716736141,3.70786113613679,...,3.08375347880892,2.88038807573222,3.22776230929947,3.22776230929947,3.22776230929947],[3.22776230929947,3.22776230929947,3.22776230929947,3.29380242174036,3.29380242174036,...,5.00000768518755,5.00000768518755,5.00000768518755,5.00000768518755,1.00000153703751]]\n",
    "    DEPT: [  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [5,5,5,5,5,...,5,5,5,5,5],  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [5,5,5,5,5,...,5,5,5,5,5],...,  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [84,84,84,84,84,...,84,84,84,84,84],  -- dictionary:\n",
    "    [\"01\",\"02\",\"03\",\"04\",\"05\",...,\"95\",\"971\",\"972\",\"973\",\"974\"]  -- indices:\n",
    "    [84,84,84,84,84,...,84,84,84,84,84]]\n",
    "\n",
    "To import and process these data, one can either keep the data in `pyarrow.Table` format or convert it into a `pandas.DataFrame`. The second option is slower but has the advantage of enabling all the manipulations offered by the `pandas` ecosystem, which is generally more familiar than that of `Arrow`.\n",
    "\n",
    "## `DuckDB`\n",
    "\n",
    "``` python\n",
    "import duckdb\n",
    "duckdb.sql(\"\"\"\n",
    "FROM read_parquet('example.parquet')\n",
    "SELECT AGED, IPONDI, DEPT\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "    ┌───────┬───────────────────┬─────────┐\n",
    "    │ AGED  │      IPONDI       │  DEPT   │\n",
    "    │ int32 │      double       │ varchar │\n",
    "    ├───────┼───────────────────┼─────────┤\n",
    "    │     9 │  2.73018871840726 │ 06      │\n",
    "    │    12 │  2.73018871840726 │ 06      │\n",
    "    │    40 │  2.73018871840726 │ 06      │\n",
    "    │    70 │ 0.954760150327854 │ 06      │\n",
    "    │    52 │  3.75907197064638 │ 06      │\n",
    "    │    82 │  3.21622922493506 │ 06      │\n",
    "    │     6 │  3.44170061276923 │ 06      │\n",
    "    │    12 │  3.44170061276923 │ 06      │\n",
    "    │    15 │  3.44170061276923 │ 06      │\n",
    "    │    43 │  3.44170061276923 │ 06      │\n",
    "    │     · │          ·        │ ·       │\n",
    "    │     · │          ·        │ ·       │\n",
    "    │     · │          ·        │ ·       │\n",
    "    │    68 │  2.73018871840726 │ 06      │\n",
    "    │    35 │  3.46310256220757 │ 06      │\n",
    "    │     2 │  3.46310256220757 │ 06      │\n",
    "    │    37 │  3.46310256220757 │ 06      │\n",
    "    │    84 │  3.69787960424482 │ 06      │\n",
    "    │    81 │   4.7717265388427 │ 06      │\n",
    "    │    81 │   4.7717265388427 │ 06      │\n",
    "    │    51 │  3.60566450823737 │ 06      │\n",
    "    │    25 │  3.60566450823737 │ 06      │\n",
    "    │    13 │  3.60566450823737 │ 06      │\n",
    "    ├───────┴───────────────────┴─────────┤\n",
    "    │    ? rows (>9999 rows, 20 shown)    │\n",
    "    └─────────────────────────────────────┘\n",
    "\n",
    "## 2.5 Exercises to Learn More\n",
    "\n",
    "The following is a series of exercises adapted from the [production deployment of data science projects](https://ensae-reproductibilite.github.io/website/chapters/big-data.html#sec-new-formats) course, which Romain Avouac and I teach in the final year of the engineering program at ENSAE.\n",
    "\n",
    "These exercises progressively illustrate some of the key concepts discussed above, while also emphasizing best practices for working with large-scale data. Solutions to all exercises are available on the corresponding course page.\n",
    "\n",
    "In this practical section, we will explore how to use the `Parquet` format as efficiently as possible. To compare different data formats and access strategies, we will **measure and compare the execution time and memory usage of a standard query**. We will begin with a lightweight example that compares the performance of reading data in `CSV` versus `Parquet` format.\n",
    "\n",
    "To do so, we will first retrieve a dataset in `Parquet` format. We suggest using the detailed and anonymized French population census data, which contains approximately 20 million rows and 80 columns. The code to download and access this dataset is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf96270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "\n",
    "# Définir le fichier de destination\n",
    "filename_table_individu = \"data/RPindividus.parquet\"\n",
    "\n",
    "# Copier le fichier depuis le stockage distant (remplacer par une méthode adaptée si nécessaire)\n",
    "os.system(\"mc cp s3/projet-formation/bonnes-pratiques/data/RPindividus.parquet data/RPindividus.parquet\")\n",
    "\n",
    "# Charger le fichier Parquet\n",
    "table = pq.read_table(filename_table_individu)\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Filtrer les données pour REGION == \"24\"\n",
    "df_filtered = df.loc[df[\"REGION\"] == \"24\"]\n",
    "\n",
    "# Sauvegarder en CSV\n",
    "df_filtered.to_csv(\"data/RPindividus_24.csv\", index=False)\n",
    "\n",
    "# Sauvegarder en Parquet\n",
    "pq.write_table(pa.Table.from_pandas(df_filtered), \"data/RPindividus_24.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42365bd-4f8c-4958-9c97-024cc8d2f93f",
   "metadata": {},
   "source": [
    "> **If you are not on `SSPCloud`**\n",
    ">\n",
    "> You will need to replace the line\n",
    ">\n",
    "> ``` python\n",
    "> os.system(\"mc cp s3/projet-formation/bonnes-pratiques/data/RPindividus.parquet data/RPindividus.parquet\")\n",
    "> ```\n",
    ">\n",
    "> which uses the `mc` command-line tool, with code that downloads this data from the URL <https://projet-formation.minio.lab.sspcloud.fr/bonnes-pratiques/data/RPindividus.parquet>.\n",
    ">\n",
    "> There are many ways to do this. For instance, you can use plain `Python` with `requests`. If you have `curl` installed, you can use it as well. Via `Python`, this would translate to the command: `os.system(\"curl -o data/RPindividus.parquet https://projet-formation/bonnes-pratiques/data/RPindividus.parquet\")`.\n",
    "\n",
    "These exercises will make use of `Python` decorators - functions that modify or extend the behavior of other functions. In our case, we will define a function that performs a series of operations, and then apply a decorator to it that tracks both memory usage and execution time.\n",
    "\n",
    "> **Part 1: From `CSV` to `Parquet`**\n",
    ">\n",
    "> -   Create a notebook named `benchmark_parquet.ipynb` to perform various performance comparisons throughout the application.  \n",
    "> -   Define a custom decorator that will be used to benchmark the `Python` code by measuring execution time and memory usage.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Click to expand and view the code for the performance-measuring decorator.\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ::: {#94505178 .cell execution_count=11}\n",
    "> \\`\\`\\` {.python .cell-code}\n",
    "> import time\n",
    "> from memory_profiler import memory_usage\n",
    "> from functools import wraps\n",
    "> import warnings\n",
    ">\n",
    ">     def convert_size(size_bytes):\n",
    ">     if size_bytes == 0:\n",
    ">         return \"0B\"\n",
    ">     size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    ">     i = int(math.floor(math.log(size_bytes, 1024)))\n",
    ">     p = math.pow(1024, i)\n",
    ">     s = round(size_bytes / p, 2)\n",
    ">     return \"%s %s\" % (s, size_name[i])\n",
    ">\n",
    ">     # Decorator to measure execution time and memory usage\n",
    ">     def measure_performance(func, return_output=False):\n",
    ">         @wraps(func)\n",
    ">         def wrapper(return_output=False, *args, **kwargs):\n",
    ">             warnings.filterwarnings(\"ignore\")\n",
    ">             start_time = time.time()\n",
    ">             mem_usage = memory_usage((func, args, kwargs), interval=0.1)\n",
    ">             end_time = time.time()\n",
    ">             warnings.filterwarnings(\"always\")\n",
    ">\n",
    ">             exec_time = end_time - start_time\n",
    ">             peak_mem = max(mem_usage)  # Peak memory usage\n",
    ">             exec_time_formatted = f\"\\033[92m{exec_time:.4f} sec\\033[0m\"\n",
    ">             peak_mem_formatted = f\"\\033[92m{convert_size(1024*peak_mem)}\\033[0m\"\n",
    ">\n",
    ">             print(f\"{func.__name__} - Execution Time: {exec_time_formatted} | Peak Memory Usage: {peak_mem_formatted}\")\n",
    ">             if return_output is True:\n",
    ">                 return func(*args, **kwargs)\n",
    ">\n",
    ">         return wrapper\n",
    ">\n",
    ">     :::\n",
    ">\n",
    ">\n",
    ">     </details>\n",
    ">\n",
    ">     * Reuse this code to wrap the logic for constructing the age pyramid into a function named `process_csv_appli1`.\n",
    ">\n",
    ">     <details>\n",
    ">\n",
    ">     <summary>\n",
    ">     Click to expand and view the code used to measure the performance of reading CSV files.\n",
    ">     </summary>\n",
    ">\n",
    ">\n",
    ">     ::: {#ac6a76b4 .cell execution_count=12}\n",
    ">     ``` {.python .cell-code}\n",
    ">     # Apply the decorator to functions\n",
    ">     @measure_performance\n",
    ">     def process_csv_appli1(*args, **kwargs):\n",
    ">         df = pd.read_csv(\"data/RPindividus_24.csv\")\n",
    ">         return (\n",
    ">             df.loc[df[\"DEPT\"] == 36]\n",
    ">             .groupby([\"AGED\", \"DEPT\"])[\"IPONDI\"]\n",
    ">             .sum().reset_index()\n",
    ">             .rename(columns={\"IPONDI\": \"n_indiv\"})\n",
    ">         )\n",
    ">\n",
    "> :::\n",
    ">\n",
    "> </details>\n",
    ">\n",
    "> -   Run `process_csv_appli1()` and `process_csv_appli1(return_output=True)` to observe performance and optionally return the processed data.\n",
    ">\n",
    "> -   Using the same approach, define a new function named `process_parquet_appli1`, this time based on the `data/RPindividus_24.parquet` file, and load it using `Pandas`’ [`read_parquet`](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) function.\n",
    ">\n",
    "> -   Compare the performance (execution time and memory usage) of the two methods using the benchmarking decorator.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Full correction\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ``` python\n",
    "> import math\n",
    "> import pandas as pd\n",
    "> import time\n",
    "> from memory_profiler import memory_usage\n",
    "> from functools import wraps\n",
    "> import warnings\n",
    ">\n",
    "> def convert_size(size_bytes):\n",
    ">    if size_bytes == 0:\n",
    ">        return \"0B\"\n",
    ">    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    ">    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    ">    p = math.pow(1024, i)\n",
    ">    s = round(size_bytes / p, 2)\n",
    ">    return \"%s %s\" % (s, size_name[i])\n",
    ">\n",
    "> # Decorator to measure execution time and memory usage\n",
    "> def measure_performance(func, return_output=False):\n",
    ">     @wraps(func)\n",
    ">     def wrapper(return_output=False, *args, **kwargs):\n",
    ">         warnings.filterwarnings(\"ignore\")\n",
    ">         start_time = time.time()\n",
    ">         mem_usage = memory_usage((func, args, kwargs), interval=0.1)\n",
    ">         end_time = time.time()\n",
    ">         warnings.filterwarnings(\"always\")\n",
    ">\n",
    ">         exec_time = end_time - start_time\n",
    ">         peak_mem = max(mem_usage)  # Peak memory usage\n",
    ">         exec_time_formatted = f\"\\033[92m{exec_time:.4f} sec\\033[0m\"\n",
    ">         peak_mem_formatted = f\"\\033[92m{convert_size(1024*peak_mem)}\\033[0m\"\n",
    ">\n",
    ">         print(f\"{func.__name__} - Execution Time: {exec_time_formatted} | Peak Memory Usage: {peak_mem_formatted}\")\n",
    ">         if return_output is True:\n",
    ">             return func(*args, **kwargs)\n",
    ">\n",
    ">     return wrapper\n",
    ">\n",
    "> # Apply the decorator to functions\n",
    "> @measure_performance\n",
    "> def process_csv(*args, **kwargs):\n",
    ">     df = pd.read_csv(\"data/RPindividus_24.csv\")\n",
    ">     return (\n",
    ">         df.loc[df[\"DEPT\"] == 36]\n",
    ">         .groupby([\"AGED\", \"DEPT\"])[\"IPONDI\"]\n",
    ">         .sum().reset_index()\n",
    ">         .rename(columns={\"IPONDI\": \"n_indiv\"})\n",
    ">     )\n",
    ">\n",
    "> @measure_performance\n",
    "> def process_parquet(*args, **kwargs):\n",
    ">     df = pd.read_parquet(\"data/RPindividus_24.parquet\")\n",
    ">     return (\n",
    ">         df.loc[df[\"DEPT\"] == \"36\"]\n",
    ">         .groupby([\"AGED\", \"DEPT\"])[\"IPONDI\"]\n",
    ">         .sum().reset_index()\n",
    ">         .rename(columns={\"IPONDI\": \"n_indiv\"})\n",
    ">     )\n",
    ">\n",
    "> process_csv()\n",
    "> process_parquet()\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    "\n",
    "*❓️ What seems to be the limitation of the `read_parquet` function?*\n",
    "\n",
    "Although we already observe a significant speed improvement during file reading, we are not fully leveraging the optimizations provided by the `Parquet` format. This is because the data is immediately loaded into a `Pandas` `DataFrame`, where transformations are applied afterward.\n",
    "\n",
    "As a result, we miss out on one of `Parquet`’s core performance features: **predicate pushdown**. This optimization allows filters to be applied as early as possible—at the file scan level—so that only the relevant columns and rows are read into memory. By bypassing this mechanism, we lose much of what makes `Parquet` so efficient in analytical workflows.\n",
    "\n",
    "> **Part 2: Leveraging *Lazy Evaluation* and the Optimizations of `Arrow` or `DuckDB`**\n",
    ">\n",
    "> In the previous section, we observed a **significant improvement in read times** when switching from `CSV` to `Parquet`. However, **memory usage remained high**, even though only a small portion of the data was actually used.\n",
    ">\n",
    "> In this section, we will explore how to take advantage of ***lazy evaluation*** and **execution plan optimizations** offered by `Arrow` to fully unlock the performance benefits of the `Parquet` format.\n",
    ">\n",
    "> -   Open the file `data/RPindividus_24.parquet` using [`pyarrow.dataset`](https://arrow.apache.org/docs/python/dataset.html). Check the class of the resulting object.\n",
    "> -   Run the code below to read a sample of the data:\n",
    ">\n",
    "> ``` python\n",
    "> (\n",
    ">     dataset.scanner()\n",
    ">     .head(5)\n",
    ">     .to_pandas()\n",
    "> )\n",
    "> ```\n",
    ">\n",
    "> Can you identify the difference compared to the previous approach? Consult the documentation for the `to_table` method—do you understand what it does and why it matters?\n",
    ">\n",
    "> -   Create a function `summarize_parquet_arrow` (and a corresponding `summarize_parquet_duckdb`) that imports the data using [`pyarrow.dataset`](https://arrow.apache.org/docs/python/dataset.html) (or `DuckDB`) and performs the required aggregation.\n",
    ">\n",
    "> -   Use the benchmarking decorator to compare the performance (execution time and memory usage) of the three approaches: reading and processing `Parquet` data using `Pandas`, `PyArrow`, and `DuckDB`.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Correction\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ``` python\n",
    "> import duckdb\n",
    "> import pyarrow.dataset as ds\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_parquet_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus_24.parquet')\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_parquet_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus_24.parquet\", format=\"parquet\")\n",
    ">     table = dataset.to_table()\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    "> process_parquet()\n",
    "> summarize_parquet_duckdb()\n",
    "> summarize_parquet_arrow()\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    "\n",
    "With lazy evaluation, the process unfolds in several stages:\n",
    "\n",
    "-   `Arrow` or `DuckDB` receives a set of instructions, builds an execution plan, optimizes it, and then executes the query;\n",
    "-   Only the final result of this pipeline is returned to `Python`, rather than the entire dataset.\n",
    "\n",
    "![](https://linogaliana.github.io/parquet-recensement-tutomate/img/duckdb-delegation1.png)\n",
    "\n",
    "> **Part 3a: What Happens When We Filter Rows?**\n",
    ">\n",
    "> Let us now add a row-level filtering step to our queries:\n",
    ">\n",
    "> -   With `DuckDB`, modify the SQL query to include a `WHERE` clause:  \n",
    ">     `WHERE DEPT IN ('18', '28', '36')`\n",
    ">\n",
    "> -   With `Arrow`, update the `to_table` call as follows:  \n",
    ">     `dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))`\n",
    ">\n",
    "> ``` python\n",
    "> import pyarrow.dataset as ds\n",
    "> import pyarrow.compute as pc\n",
    "> import duckdb\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus.parquet\", format=\"parquet\")\n",
    ">     table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus_24.parquet')\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     WHERE DEPT IN ('11','31','34')\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    "> summarize_filter_parquet_arrow()\n",
    "> summarize_filter_parquet_duckdb()\n",
    "> ```\n",
    "\n",
    "*❓️ Why do row filters not improve performance (and sometimes even slow things down), unlike column filters?*\n",
    "\n",
    "This is because data is not stored in row blocks the way it is in column blocks. As a result, filtering rows does not allow the system to skip over large sections of the file as efficiently.\n",
    "\n",
    "Fortunately, there is a solution: **partitioning**.\n",
    "\n",
    "> **Part 3: Partitioned `Parquet`**\n",
    ">\n",
    "> *Lazy evaluation* and the optimizations available through `Arrow` already provide significant performance improvements. But we can go even further. When you know in advance that your queries will frequently **filter data based on a specific variable**, it is highly advantageous to **partition** the `Parquet` file using that variable.\n",
    ">\n",
    "> 1.  Review the documentation for [`pyarrow.parquet.write_to_dataset`](https://arrow.apache.org/docs/python/parquet.html#writing-to-partitioned-datasets) to understand how to define a partitioning key when writing a `Parquet` file. Several approaches are available.\n",
    ">\n",
    "> 2.  Import the full individuals table from the census using `pyarrow.dataset.dataset(\"data/RPindividus.parquet\")`, and export it as a partitioned dataset to `\"data/RPindividus_partitionne.parquet\"`, using both `REGION` and `DEPT` as partitioning keys.\n",
    ">\n",
    "> 3.  Explore the resulting directory structure to examine how partitioning was applied—each partition key should create a subfolder representing a unique value.\n",
    ">\n",
    "> 4.  Update your data loading, filtering, and aggregation functions (using either `Arrow` or `DuckDB`) to operate on the partitioned `Parquet` file. Then compare the performance with the non-partitioned version.\n",
    ">\n",
    "> ``` python\n",
    "> import pyarrow.parquet as pq\n",
    "> dataset = ds.dataset(\n",
    ">     \"data/RPindividus.parquet\", format=\"parquet\"\n",
    "> ).to_table()\n",
    ">\n",
    "> pq.write_to_dataset(\n",
    ">     dataset,\n",
    ">     root_path=\"data/RPindividus_partitionne\",\n",
    ">     partition_cols=[\"REGION\", \"DEPT\"]\n",
    "> )\n",
    "> ```\n",
    ">\n",
    "> ``` python\n",
    "> import pyarrow.dataset as ds\n",
    "> import pyarrow.compute as pc\n",
    "> import duckdb\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_partitioned_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus_partitionne/\", partitioning=\"hive\")\n",
    ">     table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n",
    ">\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_complete_arrow(*args, **kwargs):\n",
    ">\n",
    ">     dataset = ds.dataset(\"data/RPindividus.parquet\")\n",
    ">     table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n",
    ">\n",
    ">     grouped_table = (\n",
    ">         table\n",
    ">         .group_by([\"AGED\", \"DEPT\"])\n",
    ">         .aggregate([(\"IPONDI\", \"sum\")])\n",
    ">         .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n",
    ">         .to_pandas()\n",
    ">     )\n",
    ">\n",
    ">     return (\n",
    ">         grouped_table\n",
    ">     )\n",
    ">\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_complete_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus.parquet')\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     WHERE DEPT IN ('11','31','34')\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    ">\n",
    "> @measure_performance\n",
    "> def summarize_filter_parquet_partitioned_duckdb(*args, **kwargs):\n",
    ">     con = duckdb.connect(\":memory:\")\n",
    ">     query = \"\"\"\n",
    ">     FROM read_parquet('data/RPindividus_partitionne/**/*.parquet', hive_partitioning = True)\n",
    ">     SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n",
    ">     WHERE DEPT IN ('11','31','34')\n",
    ">     GROUP BY AGED, DEPT\n",
    ">     \"\"\"\n",
    ">\n",
    ">     return (con.sql(query).to_df())\n",
    ">\n",
    ">\n",
    "> summarize_filter_parquet_complete_arrow()\n",
    "> summarize_filter_parquet_partitioned_arrow()\n",
    "> summarize_filter_parquet_complete_duckdb()\n",
    "> summarize_filter_parquet_partitioned_duckdb()\n",
    "> ```\n",
    "\n",
    "*❓️ When delivering data in `Parquet` format, how should you choose the partitioning key(s)? What limitations should you keep in mind?*\n",
    "\n",
    "# 3. Data in the *Cloud*\n",
    "\n",
    "Cloud storage, in the context of data science, follows the same principle as services like `Dropbox` or `Google Drive`: users can access remote files as if they were stored locally on their machines[1]. In other words, for a `Python` user, **working with cloud-stored files can feel exactly the same** as working with local files.\n",
    "\n",
    "However, unlike a local path such as `My Documents/mysuperfile`, the files are not physically stored on the user’s computer. They are hosted on a remote server, and every operation—reading or writing—relies on a network connection.\n",
    "\n",
    "## 3.1 Why Not Use `Dropbox` or `Google Drive`?\n",
    "\n",
    "Despite their similarities, services like `Dropbox` or `Google Drive` are not intended for large-scale data storage and processing. For data-intensive use cases, it is strongly recommended to rely on dedicated storage technologies (see [the production deployment course](https://ensae-reproductibilite.github.io/website/chapters/big-data.html)).\n",
    "\n",
    "All major cloud providers—AWS, Google Cloud Platform (GCP), Microsoft Azure—rely on a common principle: **object storage**, often implemented through `S3`-compatible systems.\n",
    "\n",
    "This is why leading cloud platforms offer specialized storage services, typically based on object storage, with `S3` (Simple Storage Service) being the most widely used and recognized standard.\n",
    "\n",
    "## 3.2 The `S3` System\n",
    "\n",
    "`S3` (Simple Storage Service), developed by Amazon, has become the de facto standard for cloud-based storage. It is:\n",
    "\n",
    "-   **Reliable**: Data is replicated across multiple servers or zones;\n",
    "-   **Secure**: It supports encryption and fine-grained access control;\n",
    "-   **Scalable**: It is designed to handle massive volumes of data without performance degradation.\n",
    "\n",
    "### 3.2.1 The Concept of a *Bucket*\n",
    "\n",
    "The fundamental unit in `S3` is the **bucket**—a storage container (either private or public) that can contain a virtual file system of folders and files.\n",
    "\n",
    "To access a file stored in a bucket:\n",
    "\n",
    "-   The user must be **authorized**, typically via credentials or secure access tokens;\n",
    "-   Once authenticated, the user can **read, write, or modify** the contents of the bucket, similar to interacting with a remote file system.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> The following examples are fully reproducible for  \n",
    "> SSP Cloud platform users.\n",
    ">\n",
    "> <div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "> <a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "> <a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«05_parquet_s3»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/manipulation%2005_parquet_s3%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "> <a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/manipulation/05_parquet_s3.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    ">\n",
    "> They can also be used by AWS users—simply update the `endpoint` URL as shown below.\n",
    "\n",
    "## 3.3 How to do it with Python?\n",
    "\n",
    "### 3.3.1 Key libraries\n",
    "\n",
    "Interaction between a remote file system and a local Python session  \n",
    "is made possible through APIs. The two main libraries for this purpose are:\n",
    "\n",
    "-   [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), a library developed and maintained by Amazon;\n",
    "-   [`s3fs`](https://s3fs.readthedocs.io/en/latest/), a library that enables interaction with stored files as if they were part of a traditional local filesystem.\n",
    "\n",
    "The `pyarrow` and `duckdb` libraries we previously introduced also support working with cloud-stored data as if it were located on the local machine. This functionality is extremely convenient and helps ensure reliable reading and writing of files in cloud-based environments.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> On SSP Cloud, access tokens for S3 storage are automatically injected into services when they are launched. These tokens remain valid for 7 days.  \n",
    "> If the service icon changes from green to red, it indicates that the tokens have expired — you should save your code and data, then restart the session by launching a new service.\n",
    "\n",
    "## 3.4 Practical Case: Storing Your Project’s Data on SSP Cloud\n",
    "\n",
    "A key criterion for evaluating `Python` projects is **reproducibility**—the ability to obtain the same results using the same input data and code. Whenever possible, your final submission should begin with the raw data used as input for your project. If the source files are publicly available via a URL, the ideal approach is to import them directly at the start of your project (see the [Pandas lab](../../content/manipulation/02_pandas_suite.qmd) for an example of such an import using `Pandas`).\n",
    "\n",
    "In practice, this is not always feasible. Your data may not be publicly accessible, or it might come in complex formats that require preprocessing before it can be used in standard workflows. In other cases, your dataset may be the result of an automated retrieval process—such as through an [API](../../content/manipulation/04c_API_TP.qmd) or [web scraping](../../content/manipulation/04a_webscraping_TP.qmd)—which can be time-consuming to reproduce. Moreover, since websites frequently change over time, it is often better to “freeze” the data once collected. Similarly, even if you are not storing raw data, you may want to preserve trained models, as the training process can also be resource-intensive.\n",
    "\n",
    "In all of these situations, you need a reliable way to store data or models. **Your `Git` repository is not the appropriate place to store large files.** A well-structured `Python` project follows a modular design: it separates code (versioned with `Git`), configuration elements (such as API tokens, which should never be hardcoded), and data storage. This conceptual separation leads to cleaner, more maintainable projects.\n",
    "\n",
    "While `Git` is designed for source code management, file storage requires dedicated solutions. Many such tools exist. On SSP Cloud, the recommended option is `MinIO`, an open-source implementation of the `S3` storage protocol introduced earlier. This brief tutorial will walk you through a standard workflow for using `MinIO` in your project.\n",
    "\n",
    "> **Warning**\n",
    ">\n",
    "> No matter which storage solution you use for your data or models, \\*\\*you must include the code that generates these objects in your project repository\n",
    "\n",
    "### 3.4.1 Sharing files on SSP Cloud\n",
    "\n",
    "As mentioned earlier, files are stored on `S3` in a bucket. On SSP Cloud, a bucket is automatically created when your account is set up, and it has the same name as your SSP Cloud username. The [My Files](https://datalab.sspcloud.fr/my-files) interface allows you to access it visually, upload files, download them, and more.\n",
    "\n",
    "In this tutorial, however, we’ll access it programmatically using `Python` code. The `s3fs` package lets you interact with your bucket just like a regular file system. For example, you can list the files available in your *bucket* using the following command:\n",
    "\n",
    "[1] This functionality is often enabled through virtual file systems or filesystem wrappers compatible with libraries such as `pandas`, `pyarrow`, `duckdb`, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
    "fs.ls(MY_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7092062f-e053-4581-9c01-48be95d7f5a1",
   "metadata": {},
   "source": [
    "If you’ve never added any files to MinIO, your *bucket* will be empty, so the command above should return an empty list. Let’s add a first folder to see the difference.\n",
    "\n",
    "By default, a bucket is personal, meaning the data inside can only be read or modified by you. For your project, you’ll want to share these files with your group members to collaborate efficiently. But that’s not all! Your instructors will also need to access these files to reproduce your analyses.\n",
    "\n",
    "There are several ways to make files more or less public on `MinIO`. The simplest—and the one we recommend—is to create a folder named `diffusion` at the root of your bucket. On SSP Cloud, all files inside a `diffusion` folder are **readable** by all authenticated users. Use the [My Files](https://datalab.sspcloud.fr/my-files) interface to create a `diffusion` folder at the root of your *bucket*. If everything worked correctly, the Python command above should now display the path `your_sspcloud_username/diffusion`.\n",
    "\n",
    "> **Cloud storage encourages collaborative work!**\n",
    ">\n",
    "> Instead of each project member working with their own files on their personal machine—which would require frequent syncing and makes reproducibility more error-prone—files are placed in a central repository that every group member can access.\n",
    ">\n",
    "> To do this, the group should agree to use one member’s bucket, and ensure that other members can access the data by placing it in the `diffusion` folder of the selected bucket.\n",
    "\n",
    "### 3.4.2 Retrieving and storing data\n",
    "\n",
    "Now that we know where to place our data on `MinIO`, let’s look at how to do it in practice using `Python`.\n",
    "\n",
    "#### Case of a DataFrame\n",
    "\n",
    "Let’s revisit an example from the course on [APIs](../../content/manipulation/04c_API_TP.qmd#illustration-avec-une-api-de-lademe-pour-obtenir-des-diagnostics-énergétiques) to simulate a time-consuming data retrieval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f74344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_api = \"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=%2A&sampling=neighbors\"\n",
    "response_json = requests.get(url_api).json()\n",
    "df_dpe = pd.json_normalize(response_json[\"results\"])\n",
    "\n",
    "df_dpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1d77e-746e-4aaf-9e9d-cb56035d7bd2",
   "metadata": {},
   "source": [
    "This request returns a `Pandas` *DataFrame*, and the first two rows are printed above. In our example, the process is deliberately simple, but in practice, you might have many steps of querying and preparing the data before obtaining a usable DataFrame for the rest of the project. This process might be time-consuming, so we’ll store these “intermediate” data on `MinIO` to avoid rerunning all the code that generated them each time.\n",
    "\n",
    "We can use `Pandas` export functions, which allow saving data in various formats. Since we’re working in the cloud, one additional step is needed: we open a connection to `MinIO`, then export our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
    "FILE_PATH_OUT_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.csv\"\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n",
    "    df_dpe.to_csv(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a0163-28ae-4ce6-b8ac-2d3abb7a236f",
   "metadata": {},
   "source": [
    "You can verify that your file has been successfully uploaded either via the [My Files](https://datalab.sspcloud.fr/my-files) interface, or directly in `Python` by checking the contents of the `diffusion` folder in your *bucket*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f88d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{MY_BUCKET}/diffusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757500a3-77c9-4168-978a-50182b47a165",
   "metadata": {},
   "source": [
    "We could just as easily export our dataset in `Parquet` format to reduce storage space and improve read performance. Note: since `Parquet` is a compressed format, you must specify that you’re writing a binary file — the file opening mode passed to `fs.open` should be changed from `w` (write) to `wb` (write binary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_OUT_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.parquet\"\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, 'wb') as file_out:\n",
    "    df_dpe.to_parquet(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958bac8-595d-4b5c-8644-31cc93ca4068",
   "metadata": {},
   "source": [
    "#### File-based use case\n",
    "\n",
    "In the previous section, we dealt with the “simple” case of a DataFrame, which allowed us to use `Pandas`’ built-in export functions. Now, let’s imagine we have multiple input files, each potentially in a different format. A typical example of such files are `ShapeFile`s, which are geographic data files and are composed of several related files (see [GeoPandas chapter](../../content/manipulation/03_geopandas_intro.qmd#le-format-shapefile-.shp-et-le-geopackage-.gpkg)). Let’s start by downloading a `.shp` file to inspect its structure.\n",
    "\n",
    "Below are the outlines of the department of Réunion [produced by the IGN](https://geoservices.ign.fr/telechargement-api/ADMIN-EXPRESS-COG-CARTO), in the form of a `..7z` archive that we will unzip locally into a folder called `departements_fr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADMIN-EXPRESS-COG-CARTO_3-2__SHP_RGR92UTM40S_REU_2023-05-03']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import requests\n",
    "import py7zr\n",
    "\n",
    "# Import et décompression\n",
    "contours_url = \"https://data.geopf.fr/telechargement/download/ADMIN-EXPRESS-COG-CARTO/ADMIN-EXPRESS-COG-CARTO_3-2__SHP_RGR92UTM40S_REU_2023-05-03/ADMIN-EXPRESS-COG-CARTO_3-2__SHP_RGR92UTM40S_REU_2023-05-03.7z\"\n",
    "response = requests.get(contours_url, stream=True)\n",
    "\n",
    "with py7zr.SevenZipFile(io.BytesIO(response.content), mode='r') as archive:\n",
    "    archive.extractall(path=\"departements_fr\")\n",
    "\n",
    "# Vérification du dossier (local, pas sur S3)\n",
    "os.listdir(\"departements_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8764b-beb6-4a2a-93e4-67a5e35db6aa",
   "metadata": {},
   "source": [
    "Since we’re now dealing with local files rather than a `Pandas` *DataFrame*, we need to use the `s3fs` package to transfer files from the local filesystem to the remote filesystem (`MinIO`). Using the `put` command, we can copy an entire folder to `MinIO` in a single step. Be sure to set the `recursive=True` parameter so that both the folder and its contents are copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.put(\"departements_fr/\", f\"{MY_BUCKET}/diffusion/departements_fr/\", recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c9750-93c2-4f08-8f96-59d53124b091",
   "metadata": {},
   "source": [
    "Let’s check that the folder was successfully copied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef39e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{MY_BUCKET}/diffusion/departements_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a953064-1434-45bd-905d-48fe2bbcc951",
   "metadata": {},
   "source": [
    "If the folder, as in this case, contains files at multiple levels, you will need to browse the list recursively to access the files. If, for example, you are only interested in municipal divisions, you can use a [`glob`](https://docs.python.org/fr/3/library/glob.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.glob(f'{MY_BUCKET}/diffusion/departements_fr/**/COMMUNE.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7eb054-86b9-4d5b-aefa-5dec6877b4cf",
   "metadata": {},
   "source": [
    "If everything worked correctly, the command above should return a list of file paths on `MinIO` for the various components of the Réunion cities `ShapeFile` (`.shp`, `.shx`, `.prj`, etc.).\n",
    "\n",
    "### 3.4.3 Using the data\n",
    "\n",
    "In the reverse direction, to retrieve files from `MinIO` in a `Python` session, the commands are symmetrical.\n",
    "\n",
    "#### Case of a dataframe\n",
    "\n",
    "Make sure to pass the `r` parameter (`read`, for reading) instead of `w` (`write`, for writing) to the `fs.open` function to avoid overwriting the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ba213",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET = \"your_sspcloud_username\"\n",
    "FILE_PATH_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.csv\"\n",
    "\n",
    "# Import\n",
    "with fs.open(FILE_PATH_S3, 'r') as file_in:\n",
    "    df_dpe = pd.read_csv(file_in)\n",
    "\n",
    "# Check\n",
    "df_dpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce56612-0891-43d5-979e-db427d896371",
   "metadata": {},
   "source": [
    "Similarly, if the file is in `Parquet` format (don’t forget to use `rb` instead of `r` due to compression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET = \"your_sspcloud_username\"\n",
    "FILE_PATH_S3 = f\"{MY_BUCKET}/diffusion/df_dpe.parquet\"\n",
    "\n",
    "# Import\n",
    "with fs.open(FILE_PATH_S3, 'rb') as file_in:\n",
    "    df_dpe = pd.read_parquet(file_in)\n",
    "\n",
    "# Check\n",
    "df_dpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea135e-7a9a-4199-a1c6-0d3724383f4f",
   "metadata": {},
   "source": [
    "#### Case of files\n",
    "\n",
    "For file collections, you’ll first need to download the files from `MinIO` to your local machine (i.e., the current SSP Cloud session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ee3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve files from MinIO to the local machine\n",
    "fs.get(f\"{MY_BUCKET}/diffusion/departements_fr/\", \"departements_fr/\", recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5f312-7c42-4ffb-a1a5-6f3e7ea943f6",
   "metadata": {},
   "source": [
    "Then, you can import them in the usual way using the appropriate `Python` package. For `ShapeFile`s, where multiple files make up a single dataset, one command is sufficient after retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "localpath = Path(\"departements_fr\")\n",
    "\n",
    "\n",
    "df_dep = gpd.read_file(\n",
    "  list(\n",
    "    localpath.glob(\"**/COMMUNE.shp\")\n",
    "  )[0]\n",
    ")\n",
    "df_dep.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79636141-452f-4137-b2e4-a77155decaf7",
   "metadata": {},
   "source": [
    "## 3.5 To go further\n",
    "\n",
    "-   [SSPCloud documentation on MinIO](https://docs.sspcloud.fr/content/storage.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
