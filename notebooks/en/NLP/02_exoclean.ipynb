{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4939722e-3835-4bb3-b727-af61c39585bd",
   "metadata": {},
   "source": [
    "# Frequentist analysis using the bag-of-words approach: forces and\n",
    "\n",
    "limitations\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-07\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/NLP/02_exoclean.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«02_exoclean»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/NLP%2002_exoclean%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«02_exoclean»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/NLP%2002_exoclean%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/NLP/02_exoclean.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "To move forward in this chapter, we need to perform some preliminary installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e86b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a13a1-2db3-41e3-8140-7de2e128ddf2",
   "metadata": {},
   "source": [
    "It is also useful to define the following function, taken from our previous chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    # Tokenize, remove stop words and punctuation, and lemmatize\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24996107-0a0a-46d8-ab91-13d449a83b33",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Previously, we saw the importance of cleaning data to filter down the volume of information present in unstructured data. The goal of this chapter is to deepen our understanding of the frequency-based approach applied to text data. We will explore how this frequentist analysis helps summarize the information contained within a text corpus. We’ll also look at how to refine the *bag of words* approach by taking into account the order or proximity of terms within a sentence.\n",
    "\n",
    "## 1.1 Data\n",
    "\n",
    "We will reuse the Anglo-Saxon dataset from the previous chapter, which includes\n",
    "texts from gothic authors [Edgar Allan Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) (*EAP*), [HP Lovecraft](https://en.wikipedia.org/wiki/H._P._Lovecraft) (*HPL*), and [Mary Wollstonecraft Shelley](https://en.wikipedia.org/wiki/Mary_Shelley) (*MWS*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
    "#1. Import des données\n",
    "horror = pd.read_csv(url,encoding='latin-1')\n",
    "#2. Majuscules aux noms des colonnes\n",
    "horror.columns = horror.columns.str.capitalize()\n",
    "#3. Retirer le prefixe id\n",
    "horror['ID'] = horror['Id'].str.replace(\"id\",\"\")\n",
    "horror = horror.set_index('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bad05f-df0a-4add-bf41-3bcfa45640a9",
   "metadata": {},
   "source": [
    "``` python\n",
    "```\n",
    "\n",
    "# 2. The TF-IDF Measure (*term frequency - inverse document frequency*)\n",
    "\n",
    "## 2.1 The Document-Term Matrix\n",
    "\n",
    "As mentioned earlier, we construct a synthetic representation of our corpus as a bag of words, where words are sampled more or less frequently depending on their appearance frequency. This is, of course, a simplified representation of reality: word sequences are not just random independent words.\n",
    "\n",
    "However, before addressing those limitations, we should complete the bag-of-words approach. The most characteristic representation of this paradigm is the document-term matrix, mainly used to compare corpora. It involves creating a matrix where each document is represented by the presence or absence of terms in our corpus. The idea is to count how often words (terms, in columns) appear in each sentence or phrase (documents, in rows). This matrix then becomes a numerical representation of the text data.\n",
    "\n",
    "Consider a corpus made up of the following three sentences:\n",
    "\n",
    "-   The practice of knitting and crocheting\n",
    "-   Passing on the passion for stamps\n",
    "-   Living off one’s passion”\n",
    "\n",
    "The corresponding document-term matrix is:\n",
    "\n",
    "| Sentence | and | crocheting | for | knitting | living | of | one’s | on | passion | passing | practice | stamps | the |\n",
    "|------------------|:--:|:----:|:--:|:----:|:---:|:-:|:--:|:-:|:---:|:----:|:----:|:---:|:--:|\n",
    "| The practice of knitting and crocheting | 1 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 |\n",
    "| Passing on the passion for stamps | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 1 |\n",
    "| Living off one’s passion | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "Each sentence in the corpus is associated with a numeric vector. For instance,\n",
    "the sentence *“La pratique du tricot et du crochet”*, which is meaningless to a machine on its own, becomes a numeric vector it can interpret: `[1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0]`. This numeric vector is a *sparse* representation of language, since each document (row) will only contain a small portion of the total vocabulary (all columns). Words that do not appear in a document are represented as zeros, hence a *sparse* vector. As we’ll see later, this numeric representation is very different from modern *embedding* approaches, which are based on dense representations.\n",
    "\n",
    "## 2.2 Use for Information Retrieval\n",
    "\n",
    "Different documents can then be compared based on these measures. This is one of the methods used by search engines, although the most advanced ones rely on far more sophisticated approaches. The [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) metric (*term frequency–inverse document frequency*)\n",
    "allows for calculating a relevance score between a search term and a document using two components:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Let $t$ be a specific term (e.g., a word), $d$ a specific document, and $D$ the entire set of documents in the corpus.\n",
    "\n",
    "-   The `tf` component computes a function that increases with the frequency of the search term in the document under consideration;\n",
    "\n",
    "-   The `idf` component computes a function that decreases with the frequency of the term across the entire document set (or corpus).\n",
    "\n",
    "-   The first part (*term frequency*, TF) is the frequency of occurrence of term $t$ in document $d$. There are normalization strategies available to avoid biasing the score in favor of longer documents.\n",
    "\n",
    "$$\n",
    "\\text{tf}(t, d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}\n",
    "$$\n",
    "\n",
    "where $f_{t,d}$ is the raw count of how many times term $t$ appears in document $d$, and the denominator is the total number of terms in document $d$.\n",
    "\n",
    "-   The second part (*inverse document frequency*, IDF) measures the rarity—or conversely, the commonness—of a term across the corpus. If $N$ is the total number of documents in the corpus $D$, this part of the metric is given by\n",
    "\n",
    "$$\n",
    "\\text{idf}(t, D) = \\log \\left( \\frac{N}{|\\{d \\in D : t \\in d\\}|} \\right)\n",
    "$$\n",
    "\n",
    "The denominator $( |\\{d \\in D : t \\in d\\}| )$ corresponds to the number of documents in which the term $t$ appears. The rarer the word, the more its presence in a document is given additional weight.\n",
    "\n",
    "Many search engines use this logic to find the most relevant documents in response to a search query. One notable example is [`ElasticSearch`](https://www.elastic.co/elasticsearch), the software used to implement powerful search engines. To rank the most relevant documents for a given search term, it uses the [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) distance metric, which is a more advanced version of the TF-IDF measure.\n",
    "\n",
    "## 2.3 Example\n",
    "\n",
    "Let’s illustrate this with a small corpus. The following code implements a TF-IDF metric. It slightly deviates from the standard definition to avoid division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Documents d'exemple\n",
    "documents = [\n",
    "    \"Le corbeau et le renard\",\n",
    "    \"Rusé comme un renard\",\n",
    "    \"Le chat est orange comme un renard\"\n",
    "]\n",
    "\n",
    "# Tokenisation\n",
    "def preprocess(doc):\n",
    "    return doc.lower().split()\n",
    "\n",
    "tokenized_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Term frequency (TF)\n",
    "def term_frequency(term, tokenized_doc):\n",
    "    term_count = tokenized_doc.count(term)\n",
    "    return term_count / len(tokenized_doc)\n",
    "\n",
    "# Inverse document frequency (DF)\n",
    "def document_frequency(term, tokenized_docs):\n",
    "    return sum(1 for doc in tokenized_docs if term in doc)\n",
    "\n",
    "# Calculate inverse document frequency (IDF)\n",
    "def inverse_document_frequency(word, corpus):\n",
    "    # Normalisation avec + 1 pour éviter la division par zéro\n",
    "    count_of_documents = len(corpus) + 1\n",
    "    count_of_documents_with_word = sum([1 for doc in corpus if word in doc]) + 1\n",
    "    idf = np.log10(count_of_documents/count_of_documents_with_word) + 1\n",
    "    return idf\n",
    "\n",
    "# Calculate TF-IDF scores in each document\n",
    "def tf_idf_term(term):\n",
    "  tf_idf_scores = pd.DataFrame(\n",
    "    [\n",
    "      [\n",
    "      term_frequency(term, doc),\n",
    "      inverse_document_frequency(term, tokenized_docs)\n",
    "      ] for doc in tokenized_docs\n",
    "    ],\n",
    "    columns = [\"TF\", \"IDF\"]\n",
    "  )\n",
    "  tf_idf_scores[\"TF-IDF\"] = tf_idf_scores[\"TF\"] * tf_idf_scores[\"IDF\"]\n",
    "  return tf_idf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ddbe0-6d69-4ea8-a0d8-3b56fca289bb",
   "metadata": {},
   "source": [
    "Let’s begin by computing the TF-IDF score of the word “cat” for each document. Naturally,\n",
    "it is the third document—the only one where the word appears—that has the highest score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96cce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_term(\"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615de8a-e68e-4373-a0be-1e623023f3d9",
   "metadata": {},
   "source": [
    "What about the term “renard” (fox in French) which appears in all the documents (making the $\\text{idf}$ component equal to 1)? In this case, the document where the word appears most frequently—in this example, the second document—has the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf65a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_term(\"renard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5529eb-e2ca-40b4-ae59-30429ad1ad49",
   "metadata": {},
   "source": [
    "## 2.4 Application\n",
    "\n",
    "The previous example didn’t scale very well. Fortunately, `Scikit` provides an implementation of TF-IDF vector search, which we can explore in a new exercise.\n",
    "\n",
    "> **Exercise 1: TF-IDF Frequency Calculation**\n",
    ">\n",
    "> 1.  Use the TF-IDF vectorizer from `scikit-learn` to transform your corpus into a `document x terms` matrix. Use the `stop_words` option to avoid inflating the matrix size. Name the model `tfidf` and the resulting dataset `tfs`.\n",
    "> 2.  After constructing the document x terms matrix with the code below, find the rows where terms matching `abandon` are non-zero.\n",
    "> 3.  Identify the 50 excerpts where the TF-IDF score for the word *“fear”* is highest and their associated authors. Determine the distribution of authors among these 50 documents.\n",
    "> 4.  Inspect the top 10 scores where TF-IDF for *“fear”* is highest.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Hint for question 2\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ``` python\n",
    "> feature_names = tfidf.get_feature_names_out()\n",
    "> corpus_index = [n for n in list(tfidf.vocabulary_.keys())]\n",
    "> horror_dense = pd.DataFrame(tfs.todense(), columns=feature_names)\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    "\n",
    "The vectorizer obtained at the end of question 1 is\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65946b8b-9401-4065-972e-976ace396691",
   "metadata": {},
   "source": [
    "The lines where the term *“abandon”* appears\n",
    "are as follows (question 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f1ab6-fb8e-438d-ac7c-8be1c7aa6ce1",
   "metadata": {},
   "source": [
    "The document-term matrix associated with these is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d91527-b20f-40a8-aac6-bf15db310de3",
   "metadata": {},
   "source": [
    "Here we notice the drawback of not applying stemming. Variations of *“abandon”* are spread across many columns. *“abandoned”* is treated as different from *“abandon”* just as it is from *“fear”*. This is one of the limitations of the *bag of words* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496e4ce-544d-4088-ad81-1b393bd2826c",
   "metadata": {},
   "source": [
    "The 10 highest scores are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa17d56-a396-4dc1-a1f0-1e5a845e9f5e",
   "metadata": {},
   "source": [
    "We observe that the highest scores correspond either to short excerpts where the word appears once, or to longer excerpts where the word *“fear”* appears multiple times.\n",
    "\n",
    "# 3. An Initial Enhancement of the Bag-of-Words Approach: *n-grams*\n",
    "\n",
    "We previously identified two main limitations of the bag-of-words approach: its disregard for context and its sparse representation of language, which sometimes leads to weak similarity matches between texts. However, within the bag-of-words paradigm, it is possible to account for the sequence of tokens using *n-grams*.\n",
    "\n",
    "To recap, in the traditional *bag of words* approach, word order doesn’t matter.\n",
    "A text is treated as a collection of words drawn independently, with varying frequencies based on their occurrence probabilities. Drawing a specific word doesn’t affect the likelihood of subsequent words.\n",
    "\n",
    "A way to introduce relationships between sequences of *tokens* is through *n-grams*.\n",
    "This method considers not only word frequencies but also which words follow others. It’s particularly useful for disambiguating homonyms. The computation of *n-grams* [1] is the simplest method for incorporating context.\n",
    "\n",
    "To carry out this type of analysis, we need to download an additional corpus:\n",
    "\n",
    "[1] We use the term *bigrams* for two-word co-occurrences, *trigrams* for three-word ones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /home/runner/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In', 'the', 'beginning', 'God', 'created', 'the', ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('genesis')\n",
    "nltk.corpus.genesis.words('english-web.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f41aa2-ed94-46c6-8667-ef8e57bd94f5",
   "metadata": {},
   "source": [
    "`NLTK` provides methods for incorporating context. To do this, we compute n-grams—that is, sequences of n consecutive word co-occurrences. Generally, we limit ourselves to bigrams or at most trigrams:\n",
    "\n",
    "-   Classification models, sentiment analysis, document comparison, etc., that rely on n-grams with large n quickly face sparse data issues, reducing their predictive power;\n",
    "-   Performance drops quickly as n increases, and data storage costs increase substantially (roughly n times larger than the original dataset).\n",
    "\n",
    "Let’s quickly examine the context in which the word `fear` appears\n",
    "in the works of Edgar Allan Poe (EAP). To do this, we first transform the EAP corpus into `NLTK` tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fa609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the']\n",
      "<Text: This process, however, afforded me no means of...>"
     ]
    }
   ],
   "source": [
    "eap_clean = horror.loc[horror[\"Author\"] == \"EAP\"]\n",
    "eap_clean = ' '.join(eap_clean['Text'])\n",
    "tokens = eap_clean.split()\n",
    "print(tokens[:10])\n",
    "text = nltk.Text(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db41d75-f9d9-482d-81c2-64f60ea785d9",
   "metadata": {},
   "source": [
    "You will need the functions `BigramCollocationFinder.from_words` and `BigramAssocMeasures.likelihood_ratio`:\n",
    "\n",
    "> **Exercise 2: n-grams and the Context of the Word “fear”**\n",
    ">\n",
    "> 1.  Use the `concordance` method to display the context in which the word `fear` appears.\n",
    "> 2.  Select and display the top collocations, for instance using the likelihood ratio criterion.\n",
    ">\n",
    "> When two words are strongly associated, it may be due to their rarity. Therefore, it’s often necessary to apply filters—for example, ignore bigrams that occur fewer than 5 times in the corpus.\n",
    ">\n",
    "> 1.  Repeat the previous task using the `BigramCollocationFinder` model, followed by the `apply_freq_filter` method to retain only bigrams appearing at least 5 times. Then, instead of the likelihood ratio, test the method `nltk.collocations.BigramAssocMeasures().jaccard`.\n",
    ">\n",
    "> 2.  Focus only on *collocations* involving the word *fear*.\n",
    "\n",
    "Using the `concordance` method (question 1),\n",
    "the list should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3affe4-34cf-4952-9136-d65a4bfaed5d",
   "metadata": {},
   "source": [
    "Although it is easy to see the words that appear before and after, this list is rather hard to interpret because it combines a lot of information.\n",
    "\n",
    "`Collocation` involves identifying bigrams that\n",
    "frequently occur together. Among all observed word pairs,\n",
    "the idea is to select the “best” ones based on a statistical model.\n",
    "Using this method (question 2), we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c9c05-7d57-40bc-ae52-1ef93bfaae7f",
   "metadata": {},
   "source": [
    "If we model the best collocations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017375ae-3834-4140-bd88-9c23196c0200",
   "metadata": {},
   "source": [
    "This list is a bit more meaningful,\n",
    "including character names, places, and frequently used expressions\n",
    "(like *Chess Player* for example).\n",
    "\n",
    "As for the *collocations* of the word *fear*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddd4ae-4a7f-432d-9117-47055f5020b5",
   "metadata": {},
   "source": [
    "If we perform the same analysis for the term *love*, we logically find subjects that are commonly associated with the verb:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652a562-2d14-4307-a997-7d2dc077538a",
   "metadata": {},
   "source": [
    "# 4. Some Applications\n",
    "\n",
    "We just discussed an initial application of the *bag of words* approach: grouping texts based on shared terms. However, this is not the only use case. We will now explore two additional applications that lead us toward language modeling: named entity recognition and classification.\n",
    "\n",
    "## 4.1 Named Entity Recognition\n",
    "\n",
    "[Named Entity Recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) is an information extraction technique used to identify the type of certain terms in a text, such as locations, people, quantities, etc.\n",
    "\n",
    "To illustrate this,\n",
    "let’s return to *The Count of Monte Cristo* and examine a short excerpt from the work to see how named entity recognition operates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa9a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" mes yeux. --vous avez donc vu l'empereur aussi? --il est entré chez le maréchal pendant que j'y étais. --et vous lui avez parlé? --c'est-à-dire que c'est lui qui m'a parlé, monsieur, dit dantès en souriant. --et que vous a-t-il dit? --il m'a fait des questions sur le bâtiment, sur l'époque de son départ pour marseille, sur la route qu'il avait suivie et sur la cargaison qu'il portait. je crois que s'il eût été vide, et que j'en eusse été le maître, son intention eût été de l'acheter; mais je lu\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/17989/17989-0.txt\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'  # Assure le bon décodage\n",
    "raw = response.text\n",
    "\n",
    "dumas = (\n",
    "  raw\n",
    "  .split(\"*** START OF THE PROJECT GUTENBERG EBOOK 17989 ***\")[1]\n",
    "  .split(\"*** END OF THE PROJECT GUTENBERG EBOOK 17989 ***\")[0]\n",
    ")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # mettre les mots en minuscule\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "dumas = clean_text(dumas)\n",
    "\n",
    "dumas[10000:10500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(dumas[15000:17000])\n",
    "# displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd38d7-a29f-4503-91c7-3adb2712a0f6",
   "metadata": {},
   "source": [
    "The named entity recognition provided\n",
    "by default in general-purpose libraries is often underwhelming; it is\n",
    "frequently necessary to supplement the default rules\n",
    "with *ad hoc* rules specific to each corpus.\n",
    "\n",
    "In practice, named entity recognition was recently\n",
    "used by Etalab to [pseudonymize administrative documents](https://guides.etalab.gouv.fr/pseudonymisation/#sommaire). This involves identifying certain sensitive information (such as civil status, address, etc.) through entity recognition and replacing it with pseudonyms.\n",
    "\n",
    "## 4.2 Text Data Classification: The `Fasttext` Algorithm\n",
    "\n",
    "`Fasttext` is a single-layer neural network developed by Meta in 2016 for text classification and language modeling. As we will see, this model serves as a bridge to more refined forms of language modeling, although `Fasttext` remains far simpler than large language models (LLMs). One of the main use cases of `Fasttext` is supervised text classification: determining a text’s category. For example, identifying whether a song’s lyrics belong to the rap or rock genre. This is a supervised model because it learns to recognize *features*—in this case, pieces of text—that lead to good prediction performance on both training and test sets.\n",
    "\n",
    "The concept of a *feature* might seem odd for text data, which is inherently unstructured. For structured data, as discussed in the [modeling section](../../content/course/modelisation/index.qmd), the approach was straightforward: features were observed variables, and the classification algorithm identified the best combination to predict the label. With text data, we must build features from the text itself—turning unstructured data into structured form. This is where the concepts we’ve covered so far come into play.\n",
    "\n",
    "`FastText` uses a *“bag of n-grams”* approach. It considers that features are derived not only from words in the corpus but also from multiple levels of n-grams. The general architecture of `FastText` looks like this:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/InseeFrLab/formation-mlops/main/slides/img/diag-fasttext.png\" alt=\"Diagram of FastText architecture\" />\n",
    "<figcaption aria-hidden=\"true\">Diagram of <code>FastText</code> architecture</figcaption>\n",
    "</figure>\n",
    "\n",
    "What interests us here is the left side of the diagram—*“feature extraction”*—since the *embedding* part relates to concepts we will cover in upcoming chapters. In the figure’s example, the text *“Business engineering and services”* is tokenized into words as we’ve seen earlier. But `Fasttext` also creates multiple levels of n-grams. For instance, it generates word bigrams: *“Business engineering”*, *“engineering and”*, *“and services”*; and also character four-grams like *“busi”*, *“usin”*, and *“sine”*. Then, `Fasttext` transforms all these items into numeric vectors. Unlike the term frequency representations we’ve seen, these vectors are not based on corpus frequency (as in document-term matrices) but are word embeddings. We’ll explore this concept in future chapters.\n",
    "\n",
    "`Fasttext` is widely used in public statistics, as many textual data sources need to be classified into aggregated nomenclatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16961592-923e-445b-adf7-bea63895e177",
   "metadata": {},
   "source": [
    "To see an interactive demonstration of such a model, visit the [corresponding site page](../../content/NLP/02_exoclean.qmd) linked to this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
